+ CUDA_VISIBLE_DEVICES=0
+ ./scripts/lora_plus_affine_cal.sh
+ CUDA_VISIBLE_DEVICES=0
+ accelerator=cpu
+ max_ls=40
+ learning_rate=1e-2
+ max_epochs=30
+ model2checkpoint=(['tinyllama']='/mnt/extra/lestienne/lit-checkpoints/TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T')
+ declare -A model2checkpoint
+ DATASETS='sst2 agnews dbpedia 20newsgroups banking77'
+ methods='dp_calibration temp_scaling bias_only'
+ dataset2numclasses=(['sst2']='2' ['agnews']='4' ['dbpedia']='14' ['20newsgroups']='20' ['banking77']='77')
+ declare -A dataset2numclasses
+ dataset2samples=(['sst2']='8 32 512' ['agnews']='4 16 256' ['dbpedia']='2 8 128' ['20newsgroups']='2 8 128' ['banking77']='1 4 64')
+ declare -A dataset2samples
+ dataset2seed=(['sst2_8']='639 923 932 6391 9322' ['sst2_32']='1564 1738 1783 15641 17832' ['sst2_512']='111 121 767 890 999' ['agnews_4']='295 926 962 2951 9622' ['agnews_16']='738 564 783 5641 7832' ['agnews_256']='493 821 812 4931 8212' ['dbpedia_2']='435 927 972 4351 9722' ['dbpedia_8']='338 364 383 3641 3832' ['dbpedia_128']='129 131 543 878 909' ['20newsgroups_2']='435 927 972 4351 9722' ['20newsgroups_8']='338 364 383 3641 3832' ['20newsgroups_128']='129 131 543 878 909' ['banking77_1']='322 444 848 858 868' ['banking77_4']='295 926 962 2951 9622' ['banking77_64']='131 888 893 912 933')
+ declare -A dataset2seed
+ for model in ${!model2checkpoint[@]}
+ checkpoint=/mnt/extra/lestienne/lit-checkpoints/TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T
+ for dataset in $DATASETS
+ for size in ${dataset2samples[$dataset]}
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=16
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration/sst2/size=8/rs=639
+ '[' dp_calibration == dp_calibration ']'
+ method_old=affine_scalar
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration/sst2/size=8/rs=639/train_logits.csv ']'
+ for method in $methods
+ total_train_samples=16
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling/sst2/size=8/rs=639
+ '[' temp_scaling == dp_calibration ']'
+ '[' temp_scaling == temp_scaling ']'
+ method_old=temp_scaling
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling/sst2/size=8/rs=639/train_logits.csv ']'
+ for method in $methods
+ total_train_samples=16
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only/sst2/size=8/rs=639
+ '[' bias_only == dp_calibration ']'
+ '[' bias_only == temp_scaling ']'
+ '[' bias_only == bias_only ']'
+ method_old=bias_only
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only/sst2/size=8/rs=639/train_logits.csv ']'
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=16
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration/sst2/size=8/rs=923
+ '[' dp_calibration == dp_calibration ']'
+ method_old=affine_scalar
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration/sst2/size=8/rs=923/train_logits.csv ']'
+ for method in $methods
+ total_train_samples=16
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling/sst2/size=8/rs=923
+ '[' temp_scaling == dp_calibration ']'
+ '[' temp_scaling == temp_scaling ']'
+ method_old=temp_scaling
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling/sst2/size=8/rs=923/train_logits.csv ']'
+ for method in $methods
+ total_train_samples=16
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only/sst2/size=8/rs=923
+ '[' bias_only == dp_calibration ']'
+ '[' bias_only == temp_scaling ']'
+ '[' bias_only == bias_only ']'
+ method_old=bias_only
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only/sst2/size=8/rs=923/train_logits.csv ']'
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=16
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration/sst2/size=8/rs=932
+ '[' dp_calibration == dp_calibration ']'
+ method_old=affine_scalar
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration/sst2/size=8/rs=932/train_logits.csv ']'
+ for method in $methods
+ total_train_samples=16
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling/sst2/size=8/rs=932
+ '[' temp_scaling == dp_calibration ']'
+ '[' temp_scaling == temp_scaling ']'
+ method_old=temp_scaling
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling/sst2/size=8/rs=932/train_logits.csv ']'
+ for method in $methods
+ total_train_samples=16
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only/sst2/size=8/rs=932
+ '[' bias_only == dp_calibration ']'
+ '[' bias_only == temp_scaling ']'
+ '[' bias_only == bias_only ']'
+ method_old=bias_only
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only/sst2/size=8/rs=932/train_logits.csv ']'
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=16
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration/sst2/size=8/rs=6391
+ '[' dp_calibration == dp_calibration ']'
+ method_old=affine_scalar
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration/sst2/size=8/rs=6391/train_logits.csv ']'
+ for method in $methods
+ total_train_samples=16
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling/sst2/size=8/rs=6391
+ '[' temp_scaling == dp_calibration ']'
+ '[' temp_scaling == temp_scaling ']'
+ method_old=temp_scaling
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling/sst2/size=8/rs=6391/train_logits.csv ']'
+ for method in $methods
+ total_train_samples=16
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only/sst2/size=8/rs=6391
+ '[' bias_only == dp_calibration ']'
+ '[' bias_only == temp_scaling ']'
+ '[' bias_only == bias_only ']'
+ method_old=bias_only
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only/sst2/size=8/rs=6391/train_logits.csv ']'
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=16
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration/sst2/size=8/rs=9322
+ '[' dp_calibration == dp_calibration ']'
+ method_old=affine_scalar
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration/sst2/size=8/rs=9322/train_logits.csv ']'
+ for method in $methods
+ total_train_samples=16
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling/sst2/size=8/rs=9322
+ '[' temp_scaling == dp_calibration ']'
+ '[' temp_scaling == temp_scaling ']'
+ method_old=temp_scaling
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling/sst2/size=8/rs=9322/train_logits.csv ']'
+ for method in $methods
+ total_train_samples=16
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only/sst2/size=8/rs=9322
+ '[' bias_only == dp_calibration ']'
+ '[' bias_only == temp_scaling ']'
+ '[' bias_only == bias_only ']'
+ method_old=bias_only
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only/sst2/size=8/rs=9322/train_logits.csv ']'
+ for size in ${dataset2samples[$dataset]}
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=64
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration/sst2/size=32/rs=1564
+ '[' dp_calibration == dp_calibration ']'
+ method_old=affine_scalar
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration/sst2/size=32/rs=1564/train_logits.csv ']'
+ for method in $methods
+ total_train_samples=64
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling/sst2/size=32/rs=1564
+ '[' temp_scaling == dp_calibration ']'
+ '[' temp_scaling == temp_scaling ']'
+ method_old=temp_scaling
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling/sst2/size=32/rs=1564/train_logits.csv ']'
+ for method in $methods
+ total_train_samples=64
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only/sst2/size=32/rs=1564
+ '[' bias_only == dp_calibration ']'
+ '[' bias_only == temp_scaling ']'
+ '[' bias_only == bias_only ']'
+ method_old=bias_only
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only/sst2/size=32/rs=1564/train_logits.csv ']'
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=64
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration/sst2/size=32/rs=1738
+ '[' dp_calibration == dp_calibration ']'
+ method_old=affine_scalar
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration/sst2/size=32/rs=1738/train_logits.csv ']'
+ for method in $methods
+ total_train_samples=64
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling/sst2/size=32/rs=1738
+ '[' temp_scaling == dp_calibration ']'
+ '[' temp_scaling == temp_scaling ']'
+ method_old=temp_scaling
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling/sst2/size=32/rs=1738/train_logits.csv ']'
+ for method in $methods
+ total_train_samples=64
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only/sst2/size=32/rs=1738
+ '[' bias_only == dp_calibration ']'
+ '[' bias_only == temp_scaling ']'
+ '[' bias_only == bias_only ']'
+ method_old=bias_only
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only/sst2/size=32/rs=1738/train_logits.csv ']'
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=64
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration/sst2/size=32/rs=1783
+ '[' dp_calibration == dp_calibration ']'
+ method_old=affine_scalar
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration/sst2/size=32/rs=1783/train_logits.csv ']'
+ for method in $methods
+ total_train_samples=64
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling/sst2/size=32/rs=1783
+ '[' temp_scaling == dp_calibration ']'
+ '[' temp_scaling == temp_scaling ']'
+ method_old=temp_scaling
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling/sst2/size=32/rs=1783/train_logits.csv ']'
+ for method in $methods
+ total_train_samples=64
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only/sst2/size=32/rs=1783
+ '[' bias_only == dp_calibration ']'
+ '[' bias_only == temp_scaling ']'
+ '[' bias_only == bias_only ']'
+ method_old=bias_only
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only/sst2/size=32/rs=1783/train_logits.csv ']'
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=64
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration/sst2/size=32/rs=15641
+ '[' dp_calibration == dp_calibration ']'
+ method_old=affine_scalar
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration/sst2/size=32/rs=15641/train_logits.csv ']'
+ for method in $methods
+ total_train_samples=64
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling/sst2/size=32/rs=15641
+ '[' temp_scaling == dp_calibration ']'
+ '[' temp_scaling == temp_scaling ']'
+ method_old=temp_scaling
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling/sst2/size=32/rs=15641/train_logits.csv ']'
+ for method in $methods
+ total_train_samples=64
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only/sst2/size=32/rs=15641
+ '[' bias_only == dp_calibration ']'
+ '[' bias_only == temp_scaling ']'
+ '[' bias_only == bias_only ']'
+ method_old=bias_only
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only/sst2/size=32/rs=15641/train_logits.csv ']'
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=64
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration/sst2/size=32/rs=17832
+ '[' dp_calibration == dp_calibration ']'
+ method_old=affine_scalar
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration/sst2/size=32/rs=17832/train_logits.csv ']'
+ for method in $methods
+ total_train_samples=64
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling/sst2/size=32/rs=17832
+ '[' temp_scaling == dp_calibration ']'
+ '[' temp_scaling == temp_scaling ']'
+ method_old=temp_scaling
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling/sst2/size=32/rs=17832/train_logits.csv ']'
+ for method in $methods
+ total_train_samples=64
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only/sst2/size=32/rs=17832
+ '[' bias_only == dp_calibration ']'
+ '[' bias_only == temp_scaling ']'
+ '[' bias_only == bias_only ']'
+ method_old=bias_only
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only/sst2/size=32/rs=17832/train_logits.csv ']'
+ for size in ${dataset2samples[$dataset]}
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=1024
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration/sst2/size=512/rs=111
+ '[' dp_calibration == dp_calibration ']'
+ method_old=affine_scalar
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration/sst2/size=512/rs=111/train_logits.csv ']'
+ for method in $methods
+ total_train_samples=1024
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling/sst2/size=512/rs=111
+ '[' temp_scaling == dp_calibration ']'
+ '[' temp_scaling == temp_scaling ']'
+ method_old=temp_scaling
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling/sst2/size=512/rs=111/train_logits.csv ']'
+ for method in $methods
+ total_train_samples=1024
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only/sst2/size=512/rs=111
+ '[' bias_only == dp_calibration ']'
+ '[' bias_only == temp_scaling ']'
+ '[' bias_only == bias_only ']'
+ method_old=bias_only
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only/sst2/size=512/rs=111/train_logits.csv ']'
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=1024
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration/sst2/size=512/rs=121
+ '[' dp_calibration == dp_calibration ']'
+ method_old=affine_scalar
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration/sst2/size=512/rs=121/train_logits.csv ']'
+ for method in $methods
+ total_train_samples=1024
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling/sst2/size=512/rs=121
+ '[' temp_scaling == dp_calibration ']'
+ '[' temp_scaling == temp_scaling ']'
+ method_old=temp_scaling
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling/sst2/size=512/rs=121/train_logits.csv ']'
+ for method in $methods
+ total_train_samples=1024
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only/sst2/size=512/rs=121
+ '[' bias_only == dp_calibration ']'
+ '[' bias_only == temp_scaling ']'
+ '[' bias_only == bias_only ']'
+ method_old=bias_only
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only/sst2/size=512/rs=121/train_logits.csv ']'
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=1024
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration/sst2/size=512/rs=767
+ '[' dp_calibration == dp_calibration ']'
+ method_old=affine_scalar
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration/sst2/size=512/rs=767/train_logits.csv ']'
+ for method in $methods
+ total_train_samples=1024
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling/sst2/size=512/rs=767
+ '[' temp_scaling == dp_calibration ']'
+ '[' temp_scaling == temp_scaling ']'
+ method_old=temp_scaling
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling/sst2/size=512/rs=767/train_logits.csv ']'
+ for method in $methods
+ total_train_samples=1024
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only/sst2/size=512/rs=767
+ '[' bias_only == dp_calibration ']'
+ '[' bias_only == temp_scaling ']'
+ '[' bias_only == bias_only ']'
+ method_old=bias_only
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only/sst2/size=512/rs=767/train_logits.csv ']'
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=1024
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration/sst2/size=512/rs=890
+ '[' dp_calibration == dp_calibration ']'
+ method_old=affine_scalar
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration/sst2/size=512/rs=890/train_logits.csv ']'
+ for method in $methods
+ total_train_samples=1024
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling/sst2/size=512/rs=890
+ '[' temp_scaling == dp_calibration ']'
+ '[' temp_scaling == temp_scaling ']'
+ method_old=temp_scaling
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling/sst2/size=512/rs=890/train_logits.csv ']'
+ for method in $methods
+ total_train_samples=1024
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only/sst2/size=512/rs=890
+ '[' bias_only == dp_calibration ']'
+ '[' bias_only == temp_scaling ']'
+ '[' bias_only == bias_only ']'
+ method_old=bias_only
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only/sst2/size=512/rs=890/train_logits.csv ']'
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=1024
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration/sst2/size=512/rs=999
+ '[' dp_calibration == dp_calibration ']'
+ method_old=affine_scalar
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration/sst2/size=512/rs=999/train_logits.csv ']'
+ for method in $methods
+ total_train_samples=1024
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling/sst2/size=512/rs=999
+ '[' temp_scaling == dp_calibration ']'
+ '[' temp_scaling == temp_scaling ']'
+ method_old=temp_scaling
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling/sst2/size=512/rs=999/train_logits.csv ']'
+ for method in $methods
+ total_train_samples=1024
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only/sst2/size=512/rs=999
+ '[' bias_only == dp_calibration ']'
+ '[' bias_only == temp_scaling ']'
+ '[' bias_only == bias_only ']'
+ method_old=bias_only
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only/sst2/size=512/rs=999/train_logits.csv ']'
+ for dataset in $DATASETS
+ for size in ${dataset2samples[$dataset]}
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=16
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=4/rs=295
+ '[' dp_calibration == dp_calibration ']'
+ method_old=affine_scalar
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=4/rs=295/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=4/rs=295 outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=4/rs=295/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=4/rs=295/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/agnews/size=4/rs=295/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/agnews/size=4/rs=295/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/agnews/size=4/rs=295/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/agnews/size=4/rs=295/test_label.csv --val_prop 0.3 --random_state 295 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=4/rs=295 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=4/rs=295/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=4/rs=295/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=4/rs=295/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 5      | train
--------------------------------------------------------
5         Trainable params
0         Non-trainable params
5         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=16
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=4/rs=295
+ '[' temp_scaling == dp_calibration ']'
+ '[' temp_scaling == temp_scaling ']'
+ method_old=temp_scaling
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=4/rs=295/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=4/rs=295 outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=4/rs=295/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=4/rs=295/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/agnews/size=4/rs=295/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/agnews/size=4/rs=295/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/agnews/size=4/rs=295/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/agnews/size=4/rs=295/test_label.csv --val_prop 0.3 --random_state 295 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=4/rs=295 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=4/rs=295/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=4/rs=295/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=4/rs=295/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 5      | train
--------------------------------------------------------
1         Trainable params
4         Non-trainable params
5         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=16
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=4/rs=295
+ '[' bias_only == dp_calibration ']'
+ '[' bias_only == temp_scaling ']'
+ '[' bias_only == bias_only ']'
+ method_old=bias_only
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=4/rs=295/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=4/rs=295 outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=4/rs=295/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=4/rs=295/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/agnews/size=4/rs=295/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/agnews/size=4/rs=295/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/agnews/size=4/rs=295/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/agnews/size=4/rs=295/test_label.csv --val_prop 0.3 --random_state 295 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=4/rs=295 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=4/rs=295/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=4/rs=295/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=4/rs=295/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 5      | train
--------------------------------------------------------
4         Trainable params
1         Non-trainable params
5         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=16
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=4/rs=926
+ '[' dp_calibration == dp_calibration ']'
+ method_old=affine_scalar
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=4/rs=926/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=4/rs=926 outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=4/rs=926/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=4/rs=926/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/agnews/size=4/rs=926/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/agnews/size=4/rs=926/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/agnews/size=4/rs=926/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/agnews/size=4/rs=926/test_label.csv --val_prop 0.3 --random_state 926 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=4/rs=926 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=4/rs=926/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=4/rs=926/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=4/rs=926/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 5      | train
--------------------------------------------------------
5         Trainable params
0         Non-trainable params
5         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=16
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=4/rs=926
+ '[' temp_scaling == dp_calibration ']'
+ '[' temp_scaling == temp_scaling ']'
+ method_old=temp_scaling
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=4/rs=926/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=4/rs=926 outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=4/rs=926/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=4/rs=926/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/agnews/size=4/rs=926/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/agnews/size=4/rs=926/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/agnews/size=4/rs=926/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/agnews/size=4/rs=926/test_label.csv --val_prop 0.3 --random_state 926 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=4/rs=926 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=4/rs=926/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=4/rs=926/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=4/rs=926/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 5      | train
--------------------------------------------------------
1         Trainable params
4         Non-trainable params
5         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=16
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=4/rs=926
+ '[' bias_only == dp_calibration ']'
+ '[' bias_only == temp_scaling ']'
+ '[' bias_only == bias_only ']'
+ method_old=bias_only
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=4/rs=926/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=4/rs=926 outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=4/rs=926/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=4/rs=926/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/agnews/size=4/rs=926/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/agnews/size=4/rs=926/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/agnews/size=4/rs=926/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/agnews/size=4/rs=926/test_label.csv --val_prop 0.3 --random_state 926 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=4/rs=926 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=4/rs=926/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=4/rs=926/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=4/rs=926/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 5      | train
--------------------------------------------------------
4         Trainable params
1         Non-trainable params
5         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=16
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=4/rs=962
+ '[' dp_calibration == dp_calibration ']'
+ method_old=affine_scalar
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=4/rs=962/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=4/rs=962 outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=4/rs=962/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=4/rs=962/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/agnews/size=4/rs=962/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/agnews/size=4/rs=962/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/agnews/size=4/rs=962/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/agnews/size=4/rs=962/test_label.csv --val_prop 0.3 --random_state 962 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=4/rs=962 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=4/rs=962/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=4/rs=962/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=4/rs=962/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 5      | train
--------------------------------------------------------
5         Trainable params
0         Non-trainable params
5         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=16
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=4/rs=962
+ '[' temp_scaling == dp_calibration ']'
+ '[' temp_scaling == temp_scaling ']'
+ method_old=temp_scaling
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=4/rs=962/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=4/rs=962 outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=4/rs=962/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=4/rs=962/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/agnews/size=4/rs=962/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/agnews/size=4/rs=962/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/agnews/size=4/rs=962/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/agnews/size=4/rs=962/test_label.csv --val_prop 0.3 --random_state 962 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=4/rs=962 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=4/rs=962/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=4/rs=962/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=4/rs=962/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 5      | train
--------------------------------------------------------
1         Trainable params
4         Non-trainable params
5         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=16
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=4/rs=962
+ '[' bias_only == dp_calibration ']'
+ '[' bias_only == temp_scaling ']'
+ '[' bias_only == bias_only ']'
+ method_old=bias_only
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=4/rs=962/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=4/rs=962 outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=4/rs=962/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=4/rs=962/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/agnews/size=4/rs=962/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/agnews/size=4/rs=962/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/agnews/size=4/rs=962/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/agnews/size=4/rs=962/test_label.csv --val_prop 0.3 --random_state 962 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=4/rs=962 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=4/rs=962/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=4/rs=962/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=4/rs=962/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 5      | train
--------------------------------------------------------
4         Trainable params
1         Non-trainable params
5         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=16
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=4/rs=2951
+ '[' dp_calibration == dp_calibration ']'
+ method_old=affine_scalar
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=4/rs=2951/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=4/rs=2951 outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=4/rs=2951/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=4/rs=2951/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/agnews/size=4/rs=2951/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/agnews/size=4/rs=2951/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/agnews/size=4/rs=2951/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/agnews/size=4/rs=2951/test_label.csv --val_prop 0.3 --random_state 2951 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=4/rs=2951 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=4/rs=2951/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=4/rs=2951/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=4/rs=2951/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 5      | train
--------------------------------------------------------
5         Trainable params
0         Non-trainable params
5         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=16
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=4/rs=2951
+ '[' temp_scaling == dp_calibration ']'
+ '[' temp_scaling == temp_scaling ']'
+ method_old=temp_scaling
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=4/rs=2951/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=4/rs=2951 outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=4/rs=2951/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=4/rs=2951/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/agnews/size=4/rs=2951/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/agnews/size=4/rs=2951/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/agnews/size=4/rs=2951/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/agnews/size=4/rs=2951/test_label.csv --val_prop 0.3 --random_state 2951 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=4/rs=2951 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=4/rs=2951/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=4/rs=2951/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=4/rs=2951/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 5      | train
--------------------------------------------------------
1         Trainable params
4         Non-trainable params
5         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=16
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=4/rs=2951
+ '[' bias_only == dp_calibration ']'
+ '[' bias_only == temp_scaling ']'
+ '[' bias_only == bias_only ']'
+ method_old=bias_only
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=4/rs=2951/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=4/rs=2951 outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=4/rs=2951/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=4/rs=2951/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/agnews/size=4/rs=2951/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/agnews/size=4/rs=2951/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/agnews/size=4/rs=2951/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/agnews/size=4/rs=2951/test_label.csv --val_prop 0.3 --random_state 2951 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=4/rs=2951 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=4/rs=2951/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=4/rs=2951/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=4/rs=2951/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 5      | train
--------------------------------------------------------
4         Trainable params
1         Non-trainable params
5         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=16
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=4/rs=9622
+ '[' dp_calibration == dp_calibration ']'
+ method_old=affine_scalar
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=4/rs=9622/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=4/rs=9622 outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=4/rs=9622/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=4/rs=9622/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/agnews/size=4/rs=9622/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/agnews/size=4/rs=9622/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/agnews/size=4/rs=9622/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/agnews/size=4/rs=9622/test_label.csv --val_prop 0.3 --random_state 9622 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=4/rs=9622 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=4/rs=9622/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=4/rs=9622/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=4/rs=9622/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 5      | train
--------------------------------------------------------
5         Trainable params
0         Non-trainable params
5         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=16
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=4/rs=9622
+ '[' temp_scaling == dp_calibration ']'
+ '[' temp_scaling == temp_scaling ']'
+ method_old=temp_scaling
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=4/rs=9622/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=4/rs=9622 outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=4/rs=9622/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=4/rs=9622/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/agnews/size=4/rs=9622/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/agnews/size=4/rs=9622/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/agnews/size=4/rs=9622/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/agnews/size=4/rs=9622/test_label.csv --val_prop 0.3 --random_state 9622 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=4/rs=9622 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=4/rs=9622/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=4/rs=9622/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=4/rs=9622/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 5      | train
--------------------------------------------------------
1         Trainable params
4         Non-trainable params
5         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=16
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=4/rs=9622
+ '[' bias_only == dp_calibration ']'
+ '[' bias_only == temp_scaling ']'
+ '[' bias_only == bias_only ']'
+ method_old=bias_only
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=4/rs=9622/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=4/rs=9622 outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=4/rs=9622/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=4/rs=9622/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/agnews/size=4/rs=9622/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/agnews/size=4/rs=9622/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/agnews/size=4/rs=9622/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/agnews/size=4/rs=9622/test_label.csv --val_prop 0.3 --random_state 9622 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=4/rs=9622 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=4/rs=9622/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=4/rs=9622/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=4/rs=9622/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 5      | train
--------------------------------------------------------
4         Trainable params
1         Non-trainable params
5         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for size in ${dataset2samples[$dataset]}
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=64
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=16/rs=738
+ '[' dp_calibration == dp_calibration ']'
+ method_old=affine_scalar
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=16/rs=738/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=16/rs=738 outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=16/rs=738/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=16/rs=738/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/agnews/size=16/rs=738/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/agnews/size=16/rs=738/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/agnews/size=16/rs=738/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/agnews/size=16/rs=738/test_label.csv --val_prop 0.3 --random_state 738 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=16/rs=738 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=16/rs=738/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=16/rs=738/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=16/rs=738/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 5      | train
--------------------------------------------------------
5         Trainable params
0         Non-trainable params
5         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=64
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=16/rs=738
+ '[' temp_scaling == dp_calibration ']'
+ '[' temp_scaling == temp_scaling ']'
+ method_old=temp_scaling
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=16/rs=738/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=16/rs=738 outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=16/rs=738/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=16/rs=738/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/agnews/size=16/rs=738/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/agnews/size=16/rs=738/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/agnews/size=16/rs=738/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/agnews/size=16/rs=738/test_label.csv --val_prop 0.3 --random_state 738 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=16/rs=738 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=16/rs=738/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=16/rs=738/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=16/rs=738/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 5      | train
--------------------------------------------------------
1         Trainable params
4         Non-trainable params
5         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=64
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=16/rs=738
+ '[' bias_only == dp_calibration ']'
+ '[' bias_only == temp_scaling ']'
+ '[' bias_only == bias_only ']'
+ method_old=bias_only
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=16/rs=738/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=16/rs=738 outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=16/rs=738/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=16/rs=738/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/agnews/size=16/rs=738/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/agnews/size=16/rs=738/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/agnews/size=16/rs=738/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/agnews/size=16/rs=738/test_label.csv --val_prop 0.3 --random_state 738 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=16/rs=738 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=16/rs=738/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=16/rs=738/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=16/rs=738/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 5      | train
--------------------------------------------------------
4         Trainable params
1         Non-trainable params
5         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=64
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=16/rs=564
+ '[' dp_calibration == dp_calibration ']'
+ method_old=affine_scalar
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=16/rs=564/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=16/rs=564 outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=16/rs=564/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=16/rs=564/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/agnews/size=16/rs=564/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/agnews/size=16/rs=564/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/agnews/size=16/rs=564/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/agnews/size=16/rs=564/test_label.csv --val_prop 0.3 --random_state 564 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=16/rs=564 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=16/rs=564/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=16/rs=564/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=16/rs=564/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 5      | train
--------------------------------------------------------
5         Trainable params
0         Non-trainable params
5         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=64
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=16/rs=564
+ '[' temp_scaling == dp_calibration ']'
+ '[' temp_scaling == temp_scaling ']'
+ method_old=temp_scaling
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=16/rs=564/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=16/rs=564 outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=16/rs=564/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=16/rs=564/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/agnews/size=16/rs=564/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/agnews/size=16/rs=564/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/agnews/size=16/rs=564/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/agnews/size=16/rs=564/test_label.csv --val_prop 0.3 --random_state 564 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=16/rs=564 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=16/rs=564/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=16/rs=564/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=16/rs=564/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 5      | train
--------------------------------------------------------
1         Trainable params
4         Non-trainable params
5         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=64
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=16/rs=564
+ '[' bias_only == dp_calibration ']'
+ '[' bias_only == temp_scaling ']'
+ '[' bias_only == bias_only ']'
+ method_old=bias_only
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=16/rs=564/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=16/rs=564 outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=16/rs=564/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=16/rs=564/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/agnews/size=16/rs=564/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/agnews/size=16/rs=564/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/agnews/size=16/rs=564/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/agnews/size=16/rs=564/test_label.csv --val_prop 0.3 --random_state 564 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=16/rs=564 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=16/rs=564/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=16/rs=564/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=16/rs=564/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 5      | train
--------------------------------------------------------
4         Trainable params
1         Non-trainable params
5         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=64
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=16/rs=783
+ '[' dp_calibration == dp_calibration ']'
+ method_old=affine_scalar
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=16/rs=783/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=16/rs=783 outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=16/rs=783/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=16/rs=783/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/agnews/size=16/rs=783/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/agnews/size=16/rs=783/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/agnews/size=16/rs=783/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/agnews/size=16/rs=783/test_label.csv --val_prop 0.3 --random_state 783 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=16/rs=783 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=16/rs=783/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=16/rs=783/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=16/rs=783/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 5      | train
--------------------------------------------------------
5         Trainable params
0         Non-trainable params
5         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=64
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=16/rs=783
+ '[' temp_scaling == dp_calibration ']'
+ '[' temp_scaling == temp_scaling ']'
+ method_old=temp_scaling
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=16/rs=783/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=16/rs=783 outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=16/rs=783/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=16/rs=783/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/agnews/size=16/rs=783/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/agnews/size=16/rs=783/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/agnews/size=16/rs=783/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/agnews/size=16/rs=783/test_label.csv --val_prop 0.3 --random_state 783 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=16/rs=783 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=16/rs=783/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=16/rs=783/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=16/rs=783/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 5      | train
--------------------------------------------------------
1         Trainable params
4         Non-trainable params
5         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=64
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=16/rs=783
+ '[' bias_only == dp_calibration ']'
+ '[' bias_only == temp_scaling ']'
+ '[' bias_only == bias_only ']'
+ method_old=bias_only
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=16/rs=783/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=16/rs=783 outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=16/rs=783/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=16/rs=783/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/agnews/size=16/rs=783/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/agnews/size=16/rs=783/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/agnews/size=16/rs=783/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/agnews/size=16/rs=783/test_label.csv --val_prop 0.3 --random_state 783 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=16/rs=783 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=16/rs=783/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=16/rs=783/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=16/rs=783/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 5      | train
--------------------------------------------------------
4         Trainable params
1         Non-trainable params
5         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=64
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=16/rs=5641
+ '[' dp_calibration == dp_calibration ']'
+ method_old=affine_scalar
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=16/rs=5641/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=16/rs=5641 outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=16/rs=5641/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=16/rs=5641/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/agnews/size=16/rs=5641/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/agnews/size=16/rs=5641/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/agnews/size=16/rs=5641/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/agnews/size=16/rs=5641/test_label.csv --val_prop 0.3 --random_state 5641 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=16/rs=5641 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=16/rs=5641/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=16/rs=5641/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=16/rs=5641/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 5      | train
--------------------------------------------------------
5         Trainable params
0         Non-trainable params
5         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=64
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=16/rs=5641
+ '[' temp_scaling == dp_calibration ']'
+ '[' temp_scaling == temp_scaling ']'
+ method_old=temp_scaling
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=16/rs=5641/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=16/rs=5641 outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=16/rs=5641/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=16/rs=5641/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/agnews/size=16/rs=5641/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/agnews/size=16/rs=5641/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/agnews/size=16/rs=5641/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/agnews/size=16/rs=5641/test_label.csv --val_prop 0.3 --random_state 5641 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=16/rs=5641 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=16/rs=5641/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=16/rs=5641/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=16/rs=5641/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 5      | train
--------------------------------------------------------
1         Trainable params
4         Non-trainable params
5         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=64
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=16/rs=5641
+ '[' bias_only == dp_calibration ']'
+ '[' bias_only == temp_scaling ']'
+ '[' bias_only == bias_only ']'
+ method_old=bias_only
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=16/rs=5641/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=16/rs=5641 outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=16/rs=5641/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=16/rs=5641/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/agnews/size=16/rs=5641/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/agnews/size=16/rs=5641/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/agnews/size=16/rs=5641/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/agnews/size=16/rs=5641/test_label.csv --val_prop 0.3 --random_state 5641 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=16/rs=5641 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=16/rs=5641/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=16/rs=5641/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=16/rs=5641/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 5      | train
--------------------------------------------------------
4         Trainable params
1         Non-trainable params
5         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=64
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=16/rs=7832
+ '[' dp_calibration == dp_calibration ']'
+ method_old=affine_scalar
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=16/rs=7832/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=16/rs=7832 outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=16/rs=7832/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=16/rs=7832/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/agnews/size=16/rs=7832/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/agnews/size=16/rs=7832/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/agnews/size=16/rs=7832/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/agnews/size=16/rs=7832/test_label.csv --val_prop 0.3 --random_state 7832 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=16/rs=7832 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=16/rs=7832/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=16/rs=7832/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=16/rs=7832/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 5      | train
--------------------------------------------------------
5         Trainable params
0         Non-trainable params
5         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=64
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=16/rs=7832
+ '[' temp_scaling == dp_calibration ']'
+ '[' temp_scaling == temp_scaling ']'
+ method_old=temp_scaling
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=16/rs=7832/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=16/rs=7832 outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=16/rs=7832/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=16/rs=7832/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/agnews/size=16/rs=7832/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/agnews/size=16/rs=7832/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/agnews/size=16/rs=7832/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/agnews/size=16/rs=7832/test_label.csv --val_prop 0.3 --random_state 7832 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=16/rs=7832 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=16/rs=7832/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=16/rs=7832/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=16/rs=7832/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 5      | train
--------------------------------------------------------
1         Trainable params
4         Non-trainable params
5         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=64
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=16/rs=7832
+ '[' bias_only == dp_calibration ']'
+ '[' bias_only == temp_scaling ']'
+ '[' bias_only == bias_only ']'
+ method_old=bias_only
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=16/rs=7832/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=16/rs=7832 outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=16/rs=7832/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=16/rs=7832/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/agnews/size=16/rs=7832/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/agnews/size=16/rs=7832/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/agnews/size=16/rs=7832/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/agnews/size=16/rs=7832/test_label.csv --val_prop 0.3 --random_state 7832 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=16/rs=7832 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=16/rs=7832/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=16/rs=7832/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=16/rs=7832/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 5      | train
--------------------------------------------------------
4         Trainable params
1         Non-trainable params
5         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for size in ${dataset2samples[$dataset]}
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=1024
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=256/rs=493
+ '[' dp_calibration == dp_calibration ']'
+ method_old=affine_scalar
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=256/rs=493/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=256/rs=493 outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=256/rs=493/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=256/rs=493/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/agnews/size=256/rs=493/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/agnews/size=256/rs=493/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/agnews/size=256/rs=493/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/agnews/size=256/rs=493/test_label.csv --val_prop 0.3 --random_state 493 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=256/rs=493 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=256/rs=493/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=256/rs=493/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=256/rs=493/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 5      | train
--------------------------------------------------------
5         Trainable params
0         Non-trainable params
5         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=1024
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=256/rs=493
+ '[' temp_scaling == dp_calibration ']'
+ '[' temp_scaling == temp_scaling ']'
+ method_old=temp_scaling
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=256/rs=493/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=256/rs=493 outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=256/rs=493/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=256/rs=493/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/agnews/size=256/rs=493/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/agnews/size=256/rs=493/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/agnews/size=256/rs=493/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/agnews/size=256/rs=493/test_label.csv --val_prop 0.3 --random_state 493 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=256/rs=493 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=256/rs=493/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=256/rs=493/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=256/rs=493/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 5      | train
--------------------------------------------------------
1         Trainable params
4         Non-trainable params
5         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=1024
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=256/rs=493
+ '[' bias_only == dp_calibration ']'
+ '[' bias_only == temp_scaling ']'
+ '[' bias_only == bias_only ']'
+ method_old=bias_only
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=256/rs=493/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=256/rs=493 outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=256/rs=493/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=256/rs=493/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/agnews/size=256/rs=493/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/agnews/size=256/rs=493/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/agnews/size=256/rs=493/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/agnews/size=256/rs=493/test_label.csv --val_prop 0.3 --random_state 493 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=256/rs=493 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=256/rs=493/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=256/rs=493/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=256/rs=493/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 5      | train
--------------------------------------------------------
4         Trainable params
1         Non-trainable params
5         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=1024
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=256/rs=821
+ '[' dp_calibration == dp_calibration ']'
+ method_old=affine_scalar
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=256/rs=821/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=256/rs=821 outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=256/rs=821/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=256/rs=821/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/agnews/size=256/rs=821/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/agnews/size=256/rs=821/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/agnews/size=256/rs=821/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/agnews/size=256/rs=821/test_label.csv --val_prop 0.3 --random_state 821 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=256/rs=821 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=256/rs=821/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=256/rs=821/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=256/rs=821/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 5      | train
--------------------------------------------------------
5         Trainable params
0         Non-trainable params
5         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=1024
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=256/rs=821
+ '[' temp_scaling == dp_calibration ']'
+ '[' temp_scaling == temp_scaling ']'
+ method_old=temp_scaling
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=256/rs=821/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=256/rs=821 outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=256/rs=821/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=256/rs=821/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/agnews/size=256/rs=821/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/agnews/size=256/rs=821/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/agnews/size=256/rs=821/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/agnews/size=256/rs=821/test_label.csv --val_prop 0.3 --random_state 821 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=256/rs=821 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=256/rs=821/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=256/rs=821/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=256/rs=821/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 5      | train
--------------------------------------------------------
1         Trainable params
4         Non-trainable params
5         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=1024
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=256/rs=821
+ '[' bias_only == dp_calibration ']'
+ '[' bias_only == temp_scaling ']'
+ '[' bias_only == bias_only ']'
+ method_old=bias_only
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=256/rs=821/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=256/rs=821 outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=256/rs=821/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=256/rs=821/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/agnews/size=256/rs=821/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/agnews/size=256/rs=821/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/agnews/size=256/rs=821/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/agnews/size=256/rs=821/test_label.csv --val_prop 0.3 --random_state 821 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=256/rs=821 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=256/rs=821/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=256/rs=821/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=256/rs=821/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 5      | train
--------------------------------------------------------
4         Trainable params
1         Non-trainable params
5         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=1024
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=256/rs=812
+ '[' dp_calibration == dp_calibration ']'
+ method_old=affine_scalar
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=256/rs=812/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=256/rs=812 outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=256/rs=812/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=256/rs=812/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/agnews/size=256/rs=812/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/agnews/size=256/rs=812/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/agnews/size=256/rs=812/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/agnews/size=256/rs=812/test_label.csv --val_prop 0.3 --random_state 812 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=256/rs=812 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=256/rs=812/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=256/rs=812/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=256/rs=812/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 5      | train
--------------------------------------------------------
5         Trainable params
0         Non-trainable params
5         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=1024
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=256/rs=812
+ '[' temp_scaling == dp_calibration ']'
+ '[' temp_scaling == temp_scaling ']'
+ method_old=temp_scaling
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=256/rs=812/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=256/rs=812 outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=256/rs=812/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=256/rs=812/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/agnews/size=256/rs=812/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/agnews/size=256/rs=812/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/agnews/size=256/rs=812/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/agnews/size=256/rs=812/test_label.csv --val_prop 0.3 --random_state 812 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=256/rs=812 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=256/rs=812/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=256/rs=812/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=256/rs=812/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 5      | train
--------------------------------------------------------
1         Trainable params
4         Non-trainable params
5         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=1024
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=256/rs=812
+ '[' bias_only == dp_calibration ']'
+ '[' bias_only == temp_scaling ']'
+ '[' bias_only == bias_only ']'
+ method_old=bias_only
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=256/rs=812/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=256/rs=812 outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=256/rs=812/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=256/rs=812/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/agnews/size=256/rs=812/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/agnews/size=256/rs=812/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/agnews/size=256/rs=812/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/agnews/size=256/rs=812/test_label.csv --val_prop 0.3 --random_state 812 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=256/rs=812 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=256/rs=812/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=256/rs=812/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=256/rs=812/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 5      | train
--------------------------------------------------------
4         Trainable params
1         Non-trainable params
5         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=1024
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=256/rs=4931
+ '[' dp_calibration == dp_calibration ']'
+ method_old=affine_scalar
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=256/rs=4931/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=256/rs=4931 outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=256/rs=4931/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=256/rs=4931/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/agnews/size=256/rs=4931/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/agnews/size=256/rs=4931/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/agnews/size=256/rs=4931/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/agnews/size=256/rs=4931/test_label.csv --val_prop 0.3 --random_state 4931 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=256/rs=4931 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=256/rs=4931/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=256/rs=4931/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=256/rs=4931/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 5      | train
--------------------------------------------------------
5         Trainable params
0         Non-trainable params
5         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=1024
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=256/rs=4931
+ '[' temp_scaling == dp_calibration ']'
+ '[' temp_scaling == temp_scaling ']'
+ method_old=temp_scaling
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=256/rs=4931/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=256/rs=4931 outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=256/rs=4931/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=256/rs=4931/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/agnews/size=256/rs=4931/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/agnews/size=256/rs=4931/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/agnews/size=256/rs=4931/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/agnews/size=256/rs=4931/test_label.csv --val_prop 0.3 --random_state 4931 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=256/rs=4931 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=256/rs=4931/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=256/rs=4931/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=256/rs=4931/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 5      | train
--------------------------------------------------------
1         Trainable params
4         Non-trainable params
5         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=1024
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=256/rs=4931
+ '[' bias_only == dp_calibration ']'
+ '[' bias_only == temp_scaling ']'
+ '[' bias_only == bias_only ']'
+ method_old=bias_only
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=256/rs=4931/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=256/rs=4931 outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=256/rs=4931/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=256/rs=4931/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/agnews/size=256/rs=4931/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/agnews/size=256/rs=4931/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/agnews/size=256/rs=4931/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/agnews/size=256/rs=4931/test_label.csv --val_prop 0.3 --random_state 4931 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=256/rs=4931 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=256/rs=4931/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=256/rs=4931/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=256/rs=4931/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 5      | train
--------------------------------------------------------
4         Trainable params
1         Non-trainable params
5         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=1024
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=256/rs=8212
+ '[' dp_calibration == dp_calibration ']'
+ method_old=affine_scalar
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=256/rs=8212/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=256/rs=8212 outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=256/rs=8212/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=256/rs=8212/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/agnews/size=256/rs=8212/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/agnews/size=256/rs=8212/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/agnews/size=256/rs=8212/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/agnews/size=256/rs=8212/test_label.csv --val_prop 0.3 --random_state 8212 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=256/rs=8212 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=256/rs=8212/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=256/rs=8212/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration/agnews/size=256/rs=8212/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 5      | train
--------------------------------------------------------
5         Trainable params
0         Non-trainable params
5         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=1024
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=256/rs=8212
+ '[' temp_scaling == dp_calibration ']'
+ '[' temp_scaling == temp_scaling ']'
+ method_old=temp_scaling
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=256/rs=8212/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=256/rs=8212 outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=256/rs=8212/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=256/rs=8212/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/agnews/size=256/rs=8212/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/agnews/size=256/rs=8212/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/agnews/size=256/rs=8212/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/agnews/size=256/rs=8212/test_label.csv --val_prop 0.3 --random_state 8212 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=256/rs=8212 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=256/rs=8212/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=256/rs=8212/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling/agnews/size=256/rs=8212/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 5      | train
--------------------------------------------------------
1         Trainable params
4         Non-trainable params
5         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=1024
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=256/rs=8212
+ '[' bias_only == dp_calibration ']'
+ '[' bias_only == temp_scaling ']'
+ '[' bias_only == bias_only ']'
+ method_old=bias_only
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=256/rs=8212/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=256/rs=8212 outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=256/rs=8212/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=256/rs=8212/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/agnews/size=256/rs=8212/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/agnews/size=256/rs=8212/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/agnews/size=256/rs=8212/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/agnews/size=256/rs=8212/test_label.csv --val_prop 0.3 --random_state 8212 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=256/rs=8212 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=256/rs=8212/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=256/rs=8212/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only/agnews/size=256/rs=8212/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 5      | train
--------------------------------------------------------
4         Trainable params
1         Non-trainable params
5         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for dataset in $DATASETS
+ for size in ${dataset2samples[$dataset]}
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=28
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=2/rs=435
+ '[' dp_calibration == dp_calibration ']'
+ method_old=affine_scalar
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=2/rs=435/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=2/rs=435 outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=2/rs=435/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=2/rs=435/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=435/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=435/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=435/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=435/test_label.csv --val_prop 0.3 --random_state 435 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=2/rs=435 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=2/rs=435/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=2/rs=435/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=2/rs=435/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 15     | train
--------------------------------------------------------
15        Trainable params
0         Non-trainable params
15        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=28
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=2/rs=435
+ '[' temp_scaling == dp_calibration ']'
+ '[' temp_scaling == temp_scaling ']'
+ method_old=temp_scaling
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=2/rs=435/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=2/rs=435 outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=2/rs=435/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=2/rs=435/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=435/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=435/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=435/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=435/test_label.csv --val_prop 0.3 --random_state 435 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=2/rs=435 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=2/rs=435/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=2/rs=435/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=2/rs=435/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 15     | train
--------------------------------------------------------
1         Trainable params
14        Non-trainable params
15        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=28
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=2/rs=435
+ '[' bias_only == dp_calibration ']'
+ '[' bias_only == temp_scaling ']'
+ '[' bias_only == bias_only ']'
+ method_old=bias_only
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=2/rs=435/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=2/rs=435 outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=2/rs=435/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=2/rs=435/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=435/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=435/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=435/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=435/test_label.csv --val_prop 0.3 --random_state 435 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=2/rs=435 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=2/rs=435/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=2/rs=435/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=2/rs=435/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 15     | train
--------------------------------------------------------
14        Trainable params
1         Non-trainable params
15        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=28
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=2/rs=927
+ '[' dp_calibration == dp_calibration ']'
+ method_old=affine_scalar
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=2/rs=927/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=2/rs=927 outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=2/rs=927/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=2/rs=927/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=927/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=927/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=927/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=927/test_label.csv --val_prop 0.3 --random_state 927 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=2/rs=927 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=2/rs=927/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=2/rs=927/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=2/rs=927/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 15     | train
--------------------------------------------------------
15        Trainable params
0         Non-trainable params
15        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=28
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=2/rs=927
+ '[' temp_scaling == dp_calibration ']'
+ '[' temp_scaling == temp_scaling ']'
+ method_old=temp_scaling
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=2/rs=927/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=2/rs=927 outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=2/rs=927/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=2/rs=927/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=927/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=927/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=927/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=927/test_label.csv --val_prop 0.3 --random_state 927 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=2/rs=927 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=2/rs=927/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=2/rs=927/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=2/rs=927/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 15     | train
--------------------------------------------------------
1         Trainable params
14        Non-trainable params
15        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=28
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=2/rs=927
+ '[' bias_only == dp_calibration ']'
+ '[' bias_only == temp_scaling ']'
+ '[' bias_only == bias_only ']'
+ method_old=bias_only
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=2/rs=927/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=2/rs=927 outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=2/rs=927/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=2/rs=927/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=927/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=927/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=927/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=927/test_label.csv --val_prop 0.3 --random_state 927 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=2/rs=927 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=2/rs=927/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=2/rs=927/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=2/rs=927/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 15     | train
--------------------------------------------------------
14        Trainable params
1         Non-trainable params
15        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=28
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=2/rs=972
+ '[' dp_calibration == dp_calibration ']'
+ method_old=affine_scalar
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=2/rs=972/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=2/rs=972 outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=2/rs=972/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=2/rs=972/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=972/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=972/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=972/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=972/test_label.csv --val_prop 0.3 --random_state 972 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=2/rs=972 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=2/rs=972/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=2/rs=972/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=2/rs=972/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 15     | train
--------------------------------------------------------
15        Trainable params
0         Non-trainable params
15        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=28
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=2/rs=972
+ '[' temp_scaling == dp_calibration ']'
+ '[' temp_scaling == temp_scaling ']'
+ method_old=temp_scaling
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=2/rs=972/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=2/rs=972 outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=2/rs=972/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=2/rs=972/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=972/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=972/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=972/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=972/test_label.csv --val_prop 0.3 --random_state 972 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=2/rs=972 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=2/rs=972/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=2/rs=972/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=2/rs=972/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 15     | train
--------------------------------------------------------
1         Trainable params
14        Non-trainable params
15        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=28
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=2/rs=972
+ '[' bias_only == dp_calibration ']'
+ '[' bias_only == temp_scaling ']'
+ '[' bias_only == bias_only ']'
+ method_old=bias_only
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=2/rs=972/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=2/rs=972 outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=2/rs=972/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=2/rs=972/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=972/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=972/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=972/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=972/test_label.csv --val_prop 0.3 --random_state 972 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=2/rs=972 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=2/rs=972/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=2/rs=972/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=2/rs=972/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 15     | train
--------------------------------------------------------
14        Trainable params
1         Non-trainable params
15        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=28
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=2/rs=4351
+ '[' dp_calibration == dp_calibration ']'
+ method_old=affine_scalar
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=2/rs=4351/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=2/rs=4351 outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=2/rs=4351/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=2/rs=4351/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=4351/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=4351/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=4351/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=4351/test_label.csv --val_prop 0.3 --random_state 4351 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=2/rs=4351 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=2/rs=4351/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=2/rs=4351/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=2/rs=4351/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 15     | train
--------------------------------------------------------
15        Trainable params
0         Non-trainable params
15        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=28
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=2/rs=4351
+ '[' temp_scaling == dp_calibration ']'
+ '[' temp_scaling == temp_scaling ']'
+ method_old=temp_scaling
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=2/rs=4351/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=2/rs=4351 outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=2/rs=4351/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=2/rs=4351/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=4351/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=4351/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=4351/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=4351/test_label.csv --val_prop 0.3 --random_state 4351 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=2/rs=4351 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=2/rs=4351/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=2/rs=4351/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=2/rs=4351/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 15     | train
--------------------------------------------------------
1         Trainable params
14        Non-trainable params
15        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=28
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=2/rs=4351
+ '[' bias_only == dp_calibration ']'
+ '[' bias_only == temp_scaling ']'
+ '[' bias_only == bias_only ']'
+ method_old=bias_only
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=2/rs=4351/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=2/rs=4351 outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=2/rs=4351/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=2/rs=4351/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=4351/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=4351/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=4351/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=4351/test_label.csv --val_prop 0.3 --random_state 4351 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=2/rs=4351 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=2/rs=4351/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=2/rs=4351/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=2/rs=4351/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 15     | train
--------------------------------------------------------
14        Trainable params
1         Non-trainable params
15        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=28
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=2/rs=9722
+ '[' dp_calibration == dp_calibration ']'
+ method_old=affine_scalar
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=2/rs=9722/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=2/rs=9722 outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=2/rs=9722/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=2/rs=9722/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=9722/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=9722/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=9722/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=9722/test_label.csv --val_prop 0.3 --random_state 9722 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=2/rs=9722 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=2/rs=9722/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=2/rs=9722/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=2/rs=9722/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 15     | train
--------------------------------------------------------
15        Trainable params
0         Non-trainable params
15        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=28
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=2/rs=9722
+ '[' temp_scaling == dp_calibration ']'
+ '[' temp_scaling == temp_scaling ']'
+ method_old=temp_scaling
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=2/rs=9722/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=2/rs=9722 outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=2/rs=9722/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=2/rs=9722/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=9722/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=9722/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=9722/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=9722/test_label.csv --val_prop 0.3 --random_state 9722 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=2/rs=9722 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=2/rs=9722/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=2/rs=9722/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=2/rs=9722/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 15     | train
--------------------------------------------------------
1         Trainable params
14        Non-trainable params
15        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=28
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=2/rs=9722
+ '[' bias_only == dp_calibration ']'
+ '[' bias_only == temp_scaling ']'
+ '[' bias_only == bias_only ']'
+ method_old=bias_only
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=2/rs=9722/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=2/rs=9722 outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=2/rs=9722/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=2/rs=9722/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=9722/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=9722/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=9722/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=9722/test_label.csv --val_prop 0.3 --random_state 9722 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=2/rs=9722 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=2/rs=9722/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=2/rs=9722/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=2/rs=9722/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 15     | train
--------------------------------------------------------
14        Trainable params
1         Non-trainable params
15        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for size in ${dataset2samples[$dataset]}
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=112
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=8/rs=338
+ '[' dp_calibration == dp_calibration ']'
+ method_old=affine_scalar
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=8/rs=338/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=8/rs=338 outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=8/rs=338/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=8/rs=338/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=338/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=338/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=338/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=338/test_label.csv --val_prop 0.3 --random_state 338 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=8/rs=338 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=8/rs=338/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=8/rs=338/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=8/rs=338/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 15     | train
--------------------------------------------------------
15        Trainable params
0         Non-trainable params
15        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=112
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=8/rs=338
+ '[' temp_scaling == dp_calibration ']'
+ '[' temp_scaling == temp_scaling ']'
+ method_old=temp_scaling
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=8/rs=338/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=8/rs=338 outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=8/rs=338/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=8/rs=338/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=338/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=338/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=338/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=338/test_label.csv --val_prop 0.3 --random_state 338 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=8/rs=338 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=8/rs=338/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=8/rs=338/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=8/rs=338/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 15     | train
--------------------------------------------------------
1         Trainable params
14        Non-trainable params
15        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=112
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=8/rs=338
+ '[' bias_only == dp_calibration ']'
+ '[' bias_only == temp_scaling ']'
+ '[' bias_only == bias_only ']'
+ method_old=bias_only
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=8/rs=338/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=8/rs=338 outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=8/rs=338/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=8/rs=338/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=338/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=338/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=338/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=338/test_label.csv --val_prop 0.3 --random_state 338 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=8/rs=338 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=8/rs=338/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=8/rs=338/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=8/rs=338/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 15     | train
--------------------------------------------------------
14        Trainable params
1         Non-trainable params
15        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=112
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=8/rs=364
+ '[' dp_calibration == dp_calibration ']'
+ method_old=affine_scalar
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=8/rs=364/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=8/rs=364 outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=8/rs=364/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=8/rs=364/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=364/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=364/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=364/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=364/test_label.csv --val_prop 0.3 --random_state 364 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=8/rs=364 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=8/rs=364/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=8/rs=364/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=8/rs=364/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 15     | train
--------------------------------------------------------
15        Trainable params
0         Non-trainable params
15        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=112
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=8/rs=364
+ '[' temp_scaling == dp_calibration ']'
+ '[' temp_scaling == temp_scaling ']'
+ method_old=temp_scaling
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=8/rs=364/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=8/rs=364 outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=8/rs=364/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=8/rs=364/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=364/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=364/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=364/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=364/test_label.csv --val_prop 0.3 --random_state 364 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=8/rs=364 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=8/rs=364/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=8/rs=364/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=8/rs=364/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 15     | train
--------------------------------------------------------
1         Trainable params
14        Non-trainable params
15        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=112
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=8/rs=364
+ '[' bias_only == dp_calibration ']'
+ '[' bias_only == temp_scaling ']'
+ '[' bias_only == bias_only ']'
+ method_old=bias_only
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=8/rs=364/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=8/rs=364 outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=8/rs=364/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=8/rs=364/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=364/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=364/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=364/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=364/test_label.csv --val_prop 0.3 --random_state 364 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=8/rs=364 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=8/rs=364/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=8/rs=364/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=8/rs=364/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 15     | train
--------------------------------------------------------
14        Trainable params
1         Non-trainable params
15        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=112
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=8/rs=383
+ '[' dp_calibration == dp_calibration ']'
+ method_old=affine_scalar
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=8/rs=383/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=8/rs=383 outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=8/rs=383/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=8/rs=383/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=383/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=383/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=383/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=383/test_label.csv --val_prop 0.3 --random_state 383 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=8/rs=383 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=8/rs=383/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=8/rs=383/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=8/rs=383/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 15     | train
--------------------------------------------------------
15        Trainable params
0         Non-trainable params
15        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=112
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=8/rs=383
+ '[' temp_scaling == dp_calibration ']'
+ '[' temp_scaling == temp_scaling ']'
+ method_old=temp_scaling
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=8/rs=383/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=8/rs=383 outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=8/rs=383/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=8/rs=383/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=383/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=383/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=383/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=383/test_label.csv --val_prop 0.3 --random_state 383 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=8/rs=383 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=8/rs=383/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=8/rs=383/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=8/rs=383/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 15     | train
--------------------------------------------------------
1         Trainable params
14        Non-trainable params
15        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=112
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=8/rs=383
+ '[' bias_only == dp_calibration ']'
+ '[' bias_only == temp_scaling ']'
+ '[' bias_only == bias_only ']'
+ method_old=bias_only
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=8/rs=383/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=8/rs=383 outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=8/rs=383/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=8/rs=383/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=383/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=383/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=383/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=383/test_label.csv --val_prop 0.3 --random_state 383 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=8/rs=383 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=8/rs=383/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=8/rs=383/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=8/rs=383/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 15     | train
--------------------------------------------------------
14        Trainable params
1         Non-trainable params
15        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=112
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=8/rs=3641
+ '[' dp_calibration == dp_calibration ']'
+ method_old=affine_scalar
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=8/rs=3641/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=8/rs=3641 outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=8/rs=3641/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=8/rs=3641/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=3641/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=3641/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=3641/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=3641/test_label.csv --val_prop 0.3 --random_state 3641 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=8/rs=3641 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=8/rs=3641/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=8/rs=3641/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=8/rs=3641/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 15     | train
--------------------------------------------------------
15        Trainable params
0         Non-trainable params
15        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=112
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=8/rs=3641
+ '[' temp_scaling == dp_calibration ']'
+ '[' temp_scaling == temp_scaling ']'
+ method_old=temp_scaling
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=8/rs=3641/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=8/rs=3641 outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=8/rs=3641/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=8/rs=3641/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=3641/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=3641/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=3641/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=3641/test_label.csv --val_prop 0.3 --random_state 3641 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=8/rs=3641 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=8/rs=3641/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=8/rs=3641/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=8/rs=3641/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 15     | train
--------------------------------------------------------
1         Trainable params
14        Non-trainable params
15        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=112
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=8/rs=3641
+ '[' bias_only == dp_calibration ']'
+ '[' bias_only == temp_scaling ']'
+ '[' bias_only == bias_only ']'
+ method_old=bias_only
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=8/rs=3641/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=8/rs=3641 outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=8/rs=3641/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=8/rs=3641/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=3641/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=3641/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=3641/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=3641/test_label.csv --val_prop 0.3 --random_state 3641 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=8/rs=3641 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=8/rs=3641/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=8/rs=3641/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=8/rs=3641/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 15     | train
--------------------------------------------------------
14        Trainable params
1         Non-trainable params
15        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=112
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=8/rs=3832
+ '[' dp_calibration == dp_calibration ']'
+ method_old=affine_scalar
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=8/rs=3832/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=8/rs=3832 outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=8/rs=3832/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=8/rs=3832/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=3832/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=3832/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=3832/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=3832/test_label.csv --val_prop 0.3 --random_state 3832 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=8/rs=3832 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=8/rs=3832/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=8/rs=3832/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=8/rs=3832/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 15     | train
--------------------------------------------------------
15        Trainable params
0         Non-trainable params
15        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=112
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=8/rs=3832
+ '[' temp_scaling == dp_calibration ']'
+ '[' temp_scaling == temp_scaling ']'
+ method_old=temp_scaling
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=8/rs=3832/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=8/rs=3832 outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=8/rs=3832/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=8/rs=3832/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=3832/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=3832/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=3832/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=3832/test_label.csv --val_prop 0.3 --random_state 3832 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=8/rs=3832 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=8/rs=3832/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=8/rs=3832/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=8/rs=3832/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 15     | train
--------------------------------------------------------
1         Trainable params
14        Non-trainable params
15        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=112
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=8/rs=3832
+ '[' bias_only == dp_calibration ']'
+ '[' bias_only == temp_scaling ']'
+ '[' bias_only == bias_only ']'
+ method_old=bias_only
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=8/rs=3832/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=8/rs=3832 outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=8/rs=3832/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=8/rs=3832/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=3832/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=3832/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=3832/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=3832/test_label.csv --val_prop 0.3 --random_state 3832 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=8/rs=3832 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=8/rs=3832/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=8/rs=3832/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=8/rs=3832/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 15     | train
--------------------------------------------------------
14        Trainable params
1         Non-trainable params
15        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for size in ${dataset2samples[$dataset]}
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=1792
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=128/rs=129
+ '[' dp_calibration == dp_calibration ']'
+ method_old=affine_scalar
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=128/rs=129/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=128/rs=129 outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=128/rs=129/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=128/rs=129/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=129/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=129/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=129/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=129/test_label.csv --val_prop 0.3 --random_state 129 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=128/rs=129 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=128/rs=129/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=128/rs=129/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=128/rs=129/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 15     | train
--------------------------------------------------------
15        Trainable params
0         Non-trainable params
15        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=1792
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=128/rs=129
+ '[' temp_scaling == dp_calibration ']'
+ '[' temp_scaling == temp_scaling ']'
+ method_old=temp_scaling
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=128/rs=129/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=128/rs=129 outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=128/rs=129/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=128/rs=129/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=129/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=129/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=129/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=129/test_label.csv --val_prop 0.3 --random_state 129 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=128/rs=129 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=128/rs=129/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=128/rs=129/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=128/rs=129/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 15     | train
--------------------------------------------------------
1         Trainable params
14        Non-trainable params
15        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=1792
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=128/rs=129
+ '[' bias_only == dp_calibration ']'
+ '[' bias_only == temp_scaling ']'
+ '[' bias_only == bias_only ']'
+ method_old=bias_only
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=128/rs=129/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=128/rs=129 outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=128/rs=129/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=128/rs=129/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=129/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=129/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=129/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=129/test_label.csv --val_prop 0.3 --random_state 129 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=128/rs=129 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=128/rs=129/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=128/rs=129/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=128/rs=129/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 15     | train
--------------------------------------------------------
14        Trainable params
1         Non-trainable params
15        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=1792
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=128/rs=131
+ '[' dp_calibration == dp_calibration ']'
+ method_old=affine_scalar
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=128/rs=131/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=128/rs=131 outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=128/rs=131/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=128/rs=131/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=131/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=131/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=131/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=131/test_label.csv --val_prop 0.3 --random_state 131 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=128/rs=131 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=128/rs=131/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=128/rs=131/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=128/rs=131/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 15     | train
--------------------------------------------------------
15        Trainable params
0         Non-trainable params
15        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=1792
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=128/rs=131
+ '[' temp_scaling == dp_calibration ']'
+ '[' temp_scaling == temp_scaling ']'
+ method_old=temp_scaling
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=128/rs=131/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=128/rs=131 outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=128/rs=131/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=128/rs=131/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=131/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=131/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=131/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=131/test_label.csv --val_prop 0.3 --random_state 131 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=128/rs=131 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=128/rs=131/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=128/rs=131/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=128/rs=131/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 15     | train
--------------------------------------------------------
1         Trainable params
14        Non-trainable params
15        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=1792
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=128/rs=131
+ '[' bias_only == dp_calibration ']'
+ '[' bias_only == temp_scaling ']'
+ '[' bias_only == bias_only ']'
+ method_old=bias_only
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=128/rs=131/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=128/rs=131 outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=128/rs=131/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=128/rs=131/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=131/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=131/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=131/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=131/test_label.csv --val_prop 0.3 --random_state 131 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=128/rs=131 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=128/rs=131/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=128/rs=131/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=128/rs=131/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 15     | train
--------------------------------------------------------
14        Trainable params
1         Non-trainable params
15        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=1792
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=128/rs=543
+ '[' dp_calibration == dp_calibration ']'
+ method_old=affine_scalar
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=128/rs=543/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=128/rs=543 outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=128/rs=543/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=128/rs=543/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=543/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=543/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=543/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=543/test_label.csv --val_prop 0.3 --random_state 543 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=128/rs=543 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=128/rs=543/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=128/rs=543/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=128/rs=543/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 15     | train
--------------------------------------------------------
15        Trainable params
0         Non-trainable params
15        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=1792
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=128/rs=543
+ '[' temp_scaling == dp_calibration ']'
+ '[' temp_scaling == temp_scaling ']'
+ method_old=temp_scaling
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=128/rs=543/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=128/rs=543 outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=128/rs=543/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=128/rs=543/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=543/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=543/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=543/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=543/test_label.csv --val_prop 0.3 --random_state 543 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=128/rs=543 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=128/rs=543/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=128/rs=543/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=128/rs=543/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 15     | train
--------------------------------------------------------
1         Trainable params
14        Non-trainable params
15        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=1792
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=128/rs=543
+ '[' bias_only == dp_calibration ']'
+ '[' bias_only == temp_scaling ']'
+ '[' bias_only == bias_only ']'
+ method_old=bias_only
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=128/rs=543/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=128/rs=543 outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=128/rs=543/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=128/rs=543/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=543/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=543/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=543/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=543/test_label.csv --val_prop 0.3 --random_state 543 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=128/rs=543 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=128/rs=543/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=128/rs=543/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=128/rs=543/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 15     | train
--------------------------------------------------------
14        Trainable params
1         Non-trainable params
15        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=1792
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=128/rs=878
+ '[' dp_calibration == dp_calibration ']'
+ method_old=affine_scalar
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=128/rs=878/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=128/rs=878 outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=128/rs=878/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=128/rs=878/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=878/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=878/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=878/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=878/test_label.csv --val_prop 0.3 --random_state 878 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=128/rs=878 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=128/rs=878/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=128/rs=878/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=128/rs=878/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 15     | train
--------------------------------------------------------
15        Trainable params
0         Non-trainable params
15        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=1792
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=128/rs=878
+ '[' temp_scaling == dp_calibration ']'
+ '[' temp_scaling == temp_scaling ']'
+ method_old=temp_scaling
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=128/rs=878/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=128/rs=878 outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=128/rs=878/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=128/rs=878/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=878/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=878/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=878/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=878/test_label.csv --val_prop 0.3 --random_state 878 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=128/rs=878 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=128/rs=878/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=128/rs=878/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=128/rs=878/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 15     | train
--------------------------------------------------------
1         Trainable params
14        Non-trainable params
15        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=1792
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=128/rs=878
+ '[' bias_only == dp_calibration ']'
+ '[' bias_only == temp_scaling ']'
+ '[' bias_only == bias_only ']'
+ method_old=bias_only
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=128/rs=878/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=128/rs=878 outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=128/rs=878/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=128/rs=878/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=878/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=878/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=878/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=878/test_label.csv --val_prop 0.3 --random_state 878 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=128/rs=878 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=128/rs=878/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=128/rs=878/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=128/rs=878/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 15     | train
--------------------------------------------------------
14        Trainable params
1         Non-trainable params
15        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=1792
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=128/rs=909
+ '[' dp_calibration == dp_calibration ']'
+ method_old=affine_scalar
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=128/rs=909/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=128/rs=909 outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=128/rs=909/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=128/rs=909/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=909/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=909/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=909/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=909/test_label.csv --val_prop 0.3 --random_state 909 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=128/rs=909 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=128/rs=909/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=128/rs=909/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration/dbpedia/size=128/rs=909/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 15     | train
--------------------------------------------------------
15        Trainable params
0         Non-trainable params
15        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=1792
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=128/rs=909
+ '[' temp_scaling == dp_calibration ']'
+ '[' temp_scaling == temp_scaling ']'
+ method_old=temp_scaling
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=128/rs=909/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=128/rs=909 outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=128/rs=909/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=128/rs=909/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=909/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=909/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=909/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=909/test_label.csv --val_prop 0.3 --random_state 909 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=128/rs=909 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=128/rs=909/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=128/rs=909/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling/dbpedia/size=128/rs=909/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 15     | train
--------------------------------------------------------
1         Trainable params
14        Non-trainable params
15        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=1792
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=128/rs=909
+ '[' bias_only == dp_calibration ']'
+ '[' bias_only == temp_scaling ']'
+ '[' bias_only == bias_only ']'
+ method_old=bias_only
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=128/rs=909/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=128/rs=909 outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=128/rs=909/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=128/rs=909/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=909/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=909/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=909/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=909/test_label.csv --val_prop 0.3 --random_state 909 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=128/rs=909 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=128/rs=909/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=128/rs=909/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only/dbpedia/size=128/rs=909/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 15     | train
--------------------------------------------------------
14        Trainable params
1         Non-trainable params
15        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for dataset in $DATASETS
+ for size in ${dataset2samples[$dataset]}
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=40
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=2/rs=435
+ '[' dp_calibration == dp_calibration ']'
+ method_old=affine_scalar
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=2/rs=435/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=2/rs=435 outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=2/rs=435/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=2/rs=435/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=435/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=435/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=435/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=435/test_label.csv --val_prop 0.3 --random_state 435 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=2/rs=435 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=2/rs=435/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=2/rs=435/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=2/rs=435/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 21     | train
--------------------------------------------------------
21        Trainable params
0         Non-trainable params
21        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=40
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=2/rs=435
+ '[' temp_scaling == dp_calibration ']'
+ '[' temp_scaling == temp_scaling ']'
+ method_old=temp_scaling
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=2/rs=435/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=2/rs=435 outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=2/rs=435/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=2/rs=435/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=435/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=435/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=435/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=435/test_label.csv --val_prop 0.3 --random_state 435 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=2/rs=435 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=2/rs=435/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=2/rs=435/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=2/rs=435/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 21     | train
--------------------------------------------------------
1         Trainable params
20        Non-trainable params
21        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=40
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=2/rs=435
+ '[' bias_only == dp_calibration ']'
+ '[' bias_only == temp_scaling ']'
+ '[' bias_only == bias_only ']'
+ method_old=bias_only
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=2/rs=435/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=2/rs=435 outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=2/rs=435/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=2/rs=435/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=435/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=435/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=435/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=435/test_label.csv --val_prop 0.3 --random_state 435 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=2/rs=435 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=2/rs=435/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=2/rs=435/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=2/rs=435/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 21     | train
--------------------------------------------------------
20        Trainable params
1         Non-trainable params
21        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=40
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=2/rs=927
+ '[' dp_calibration == dp_calibration ']'
+ method_old=affine_scalar
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=2/rs=927/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=2/rs=927 outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=2/rs=927/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=2/rs=927/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=927/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=927/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=927/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=927/test_label.csv --val_prop 0.3 --random_state 927 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=2/rs=927 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=2/rs=927/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=2/rs=927/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=2/rs=927/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 21     | train
--------------------------------------------------------
21        Trainable params
0         Non-trainable params
21        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=40
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=2/rs=927
+ '[' temp_scaling == dp_calibration ']'
+ '[' temp_scaling == temp_scaling ']'
+ method_old=temp_scaling
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=2/rs=927/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=2/rs=927 outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=2/rs=927/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=2/rs=927/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=927/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=927/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=927/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=927/test_label.csv --val_prop 0.3 --random_state 927 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=2/rs=927 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=2/rs=927/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=2/rs=927/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=2/rs=927/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 21     | train
--------------------------------------------------------
1         Trainable params
20        Non-trainable params
21        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=40
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=2/rs=927
+ '[' bias_only == dp_calibration ']'
+ '[' bias_only == temp_scaling ']'
+ '[' bias_only == bias_only ']'
+ method_old=bias_only
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=2/rs=927/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=2/rs=927 outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=2/rs=927/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=2/rs=927/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=927/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=927/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=927/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=927/test_label.csv --val_prop 0.3 --random_state 927 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=2/rs=927 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=2/rs=927/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=2/rs=927/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=2/rs=927/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 21     | train
--------------------------------------------------------
20        Trainable params
1         Non-trainable params
21        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=40
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=2/rs=972
+ '[' dp_calibration == dp_calibration ']'
+ method_old=affine_scalar
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=2/rs=972/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=2/rs=972 outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=2/rs=972/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=2/rs=972/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=972/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=972/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=972/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=972/test_label.csv --val_prop 0.3 --random_state 972 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=2/rs=972 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=2/rs=972/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=2/rs=972/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=2/rs=972/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 21     | train
--------------------------------------------------------
21        Trainable params
0         Non-trainable params
21        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=40
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=2/rs=972
+ '[' temp_scaling == dp_calibration ']'
+ '[' temp_scaling == temp_scaling ']'
+ method_old=temp_scaling
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=2/rs=972/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=2/rs=972 outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=2/rs=972/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=2/rs=972/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=972/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=972/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=972/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=972/test_label.csv --val_prop 0.3 --random_state 972 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=2/rs=972 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=2/rs=972/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=2/rs=972/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=2/rs=972/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 21     | train
--------------------------------------------------------
1         Trainable params
20        Non-trainable params
21        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=40
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=2/rs=972
+ '[' bias_only == dp_calibration ']'
+ '[' bias_only == temp_scaling ']'
+ '[' bias_only == bias_only ']'
+ method_old=bias_only
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=2/rs=972/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=2/rs=972 outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=2/rs=972/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=2/rs=972/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=972/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=972/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=972/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=972/test_label.csv --val_prop 0.3 --random_state 972 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=2/rs=972 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=2/rs=972/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=2/rs=972/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=2/rs=972/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 21     | train
--------------------------------------------------------
20        Trainable params
1         Non-trainable params
21        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=40
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=2/rs=4351
+ '[' dp_calibration == dp_calibration ']'
+ method_old=affine_scalar
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=2/rs=4351/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=2/rs=4351 outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=2/rs=4351/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=2/rs=4351/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=4351/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=4351/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=4351/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=4351/test_label.csv --val_prop 0.3 --random_state 4351 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=2/rs=4351 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=2/rs=4351/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=2/rs=4351/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=2/rs=4351/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 21     | train
--------------------------------------------------------
21        Trainable params
0         Non-trainable params
21        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=40
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=2/rs=4351
+ '[' temp_scaling == dp_calibration ']'
+ '[' temp_scaling == temp_scaling ']'
+ method_old=temp_scaling
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=2/rs=4351/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=2/rs=4351 outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=2/rs=4351/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=2/rs=4351/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=4351/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=4351/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=4351/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=4351/test_label.csv --val_prop 0.3 --random_state 4351 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=2/rs=4351 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=2/rs=4351/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=2/rs=4351/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=2/rs=4351/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 21     | train
--------------------------------------------------------
1         Trainable params
20        Non-trainable params
21        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=40
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=2/rs=4351
+ '[' bias_only == dp_calibration ']'
+ '[' bias_only == temp_scaling ']'
+ '[' bias_only == bias_only ']'
+ method_old=bias_only
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=2/rs=4351/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=2/rs=4351 outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=2/rs=4351/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=2/rs=4351/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=4351/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=4351/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=4351/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=4351/test_label.csv --val_prop 0.3 --random_state 4351 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=2/rs=4351 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=2/rs=4351/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=2/rs=4351/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=2/rs=4351/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 21     | train
--------------------------------------------------------
20        Trainable params
1         Non-trainable params
21        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=40
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=2/rs=9722
+ '[' dp_calibration == dp_calibration ']'
+ method_old=affine_scalar
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=2/rs=9722/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=2/rs=9722 outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=2/rs=9722/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=2/rs=9722/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=9722/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=9722/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=9722/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=9722/test_label.csv --val_prop 0.3 --random_state 9722 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=2/rs=9722 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=2/rs=9722/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=2/rs=9722/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=2/rs=9722/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 21     | train
--------------------------------------------------------
21        Trainable params
0         Non-trainable params
21        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=40
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=2/rs=9722
+ '[' temp_scaling == dp_calibration ']'
+ '[' temp_scaling == temp_scaling ']'
+ method_old=temp_scaling
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=2/rs=9722/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=2/rs=9722 outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=2/rs=9722/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=2/rs=9722/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=9722/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=9722/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=9722/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=9722/test_label.csv --val_prop 0.3 --random_state 9722 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=2/rs=9722 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=2/rs=9722/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=2/rs=9722/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=2/rs=9722/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 21     | train
--------------------------------------------------------
1         Trainable params
20        Non-trainable params
21        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=40
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=2/rs=9722
+ '[' bias_only == dp_calibration ']'
+ '[' bias_only == temp_scaling ']'
+ '[' bias_only == bias_only ']'
+ method_old=bias_only
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=2/rs=9722/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=2/rs=9722 outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=2/rs=9722/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=2/rs=9722/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=9722/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=9722/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=9722/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=9722/test_label.csv --val_prop 0.3 --random_state 9722 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=2/rs=9722 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=2/rs=9722/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=2/rs=9722/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=2/rs=9722/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 21     | train
--------------------------------------------------------
20        Trainable params
1         Non-trainable params
21        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for size in ${dataset2samples[$dataset]}
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=160
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=8/rs=338
+ '[' dp_calibration == dp_calibration ']'
+ method_old=affine_scalar
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=8/rs=338/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=8/rs=338 outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=8/rs=338/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=8/rs=338/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=338/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=338/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=338/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=338/test_label.csv --val_prop 0.3 --random_state 338 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=8/rs=338 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=8/rs=338/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=8/rs=338/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=8/rs=338/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 21     | train
--------------------------------------------------------
21        Trainable params
0         Non-trainable params
21        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=160
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=8/rs=338
+ '[' temp_scaling == dp_calibration ']'
+ '[' temp_scaling == temp_scaling ']'
+ method_old=temp_scaling
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=8/rs=338/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=8/rs=338 outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=8/rs=338/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=8/rs=338/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=338/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=338/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=338/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=338/test_label.csv --val_prop 0.3 --random_state 338 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=8/rs=338 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=8/rs=338/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=8/rs=338/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=8/rs=338/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 21     | train
--------------------------------------------------------
1         Trainable params
20        Non-trainable params
21        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=160
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=8/rs=338
+ '[' bias_only == dp_calibration ']'
+ '[' bias_only == temp_scaling ']'
+ '[' bias_only == bias_only ']'
+ method_old=bias_only
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=8/rs=338/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=8/rs=338 outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=8/rs=338/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=8/rs=338/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=338/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=338/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=338/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=338/test_label.csv --val_prop 0.3 --random_state 338 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=8/rs=338 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=8/rs=338/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=8/rs=338/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=8/rs=338/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 21     | train
--------------------------------------------------------
20        Trainable params
1         Non-trainable params
21        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=160
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=8/rs=364
+ '[' dp_calibration == dp_calibration ']'
+ method_old=affine_scalar
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=8/rs=364/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=8/rs=364 outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=8/rs=364/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=8/rs=364/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=364/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=364/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=364/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=364/test_label.csv --val_prop 0.3 --random_state 364 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=8/rs=364 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=8/rs=364/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=8/rs=364/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=8/rs=364/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 21     | train
--------------------------------------------------------
21        Trainable params
0         Non-trainable params
21        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=160
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=8/rs=364
+ '[' temp_scaling == dp_calibration ']'
+ '[' temp_scaling == temp_scaling ']'
+ method_old=temp_scaling
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=8/rs=364/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=8/rs=364 outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=8/rs=364/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=8/rs=364/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=364/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=364/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=364/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=364/test_label.csv --val_prop 0.3 --random_state 364 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=8/rs=364 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=8/rs=364/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=8/rs=364/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=8/rs=364/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 21     | train
--------------------------------------------------------
1         Trainable params
20        Non-trainable params
21        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=160
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=8/rs=364
+ '[' bias_only == dp_calibration ']'
+ '[' bias_only == temp_scaling ']'
+ '[' bias_only == bias_only ']'
+ method_old=bias_only
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=8/rs=364/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=8/rs=364 outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=8/rs=364/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=8/rs=364/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=364/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=364/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=364/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=364/test_label.csv --val_prop 0.3 --random_state 364 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=8/rs=364 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=8/rs=364/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=8/rs=364/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=8/rs=364/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 21     | train
--------------------------------------------------------
20        Trainable params
1         Non-trainable params
21        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=160
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=8/rs=383
+ '[' dp_calibration == dp_calibration ']'
+ method_old=affine_scalar
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=8/rs=383/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=8/rs=383 outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=8/rs=383/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=8/rs=383/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=383/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=383/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=383/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=383/test_label.csv --val_prop 0.3 --random_state 383 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=8/rs=383 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=8/rs=383/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=8/rs=383/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=8/rs=383/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 21     | train
--------------------------------------------------------
21        Trainable params
0         Non-trainable params
21        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=160
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=8/rs=383
+ '[' temp_scaling == dp_calibration ']'
+ '[' temp_scaling == temp_scaling ']'
+ method_old=temp_scaling
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=8/rs=383/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=8/rs=383 outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=8/rs=383/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=8/rs=383/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=383/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=383/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=383/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=383/test_label.csv --val_prop 0.3 --random_state 383 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=8/rs=383 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=8/rs=383/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=8/rs=383/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=8/rs=383/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 21     | train
--------------------------------------------------------
1         Trainable params
20        Non-trainable params
21        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=160
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=8/rs=383
+ '[' bias_only == dp_calibration ']'
+ '[' bias_only == temp_scaling ']'
+ '[' bias_only == bias_only ']'
+ method_old=bias_only
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=8/rs=383/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=8/rs=383 outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=8/rs=383/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=8/rs=383/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=383/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=383/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=383/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=383/test_label.csv --val_prop 0.3 --random_state 383 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=8/rs=383 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=8/rs=383/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=8/rs=383/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=8/rs=383/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 21     | train
--------------------------------------------------------
20        Trainable params
1         Non-trainable params
21        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=160
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=8/rs=3641
+ '[' dp_calibration == dp_calibration ']'
+ method_old=affine_scalar
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=8/rs=3641/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=8/rs=3641 outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=8/rs=3641/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=8/rs=3641/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=3641/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=3641/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=3641/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=3641/test_label.csv --val_prop 0.3 --random_state 3641 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=8/rs=3641 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=8/rs=3641/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=8/rs=3641/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=8/rs=3641/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 21     | train
--------------------------------------------------------
21        Trainable params
0         Non-trainable params
21        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=160
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=8/rs=3641
+ '[' temp_scaling == dp_calibration ']'
+ '[' temp_scaling == temp_scaling ']'
+ method_old=temp_scaling
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=8/rs=3641/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=8/rs=3641 outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=8/rs=3641/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=8/rs=3641/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=3641/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=3641/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=3641/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=3641/test_label.csv --val_prop 0.3 --random_state 3641 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=8/rs=3641 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=8/rs=3641/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=8/rs=3641/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=8/rs=3641/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 21     | train
--------------------------------------------------------
1         Trainable params
20        Non-trainable params
21        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=160
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=8/rs=3641
+ '[' bias_only == dp_calibration ']'
+ '[' bias_only == temp_scaling ']'
+ '[' bias_only == bias_only ']'
+ method_old=bias_only
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=8/rs=3641/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=8/rs=3641 outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=8/rs=3641/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=8/rs=3641/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=3641/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=3641/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=3641/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=3641/test_label.csv --val_prop 0.3 --random_state 3641 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=8/rs=3641 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=8/rs=3641/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=8/rs=3641/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=8/rs=3641/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 21     | train
--------------------------------------------------------
20        Trainable params
1         Non-trainable params
21        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=160
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=8/rs=3832
+ '[' dp_calibration == dp_calibration ']'
+ method_old=affine_scalar
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=8/rs=3832/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=8/rs=3832 outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=8/rs=3832/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=8/rs=3832/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=3832/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=3832/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=3832/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=3832/test_label.csv --val_prop 0.3 --random_state 3832 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=8/rs=3832 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=8/rs=3832/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=8/rs=3832/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=8/rs=3832/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 21     | train
--------------------------------------------------------
21        Trainable params
0         Non-trainable params
21        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=160
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=8/rs=3832
+ '[' temp_scaling == dp_calibration ']'
+ '[' temp_scaling == temp_scaling ']'
+ method_old=temp_scaling
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=8/rs=3832/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=8/rs=3832 outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=8/rs=3832/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=8/rs=3832/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=3832/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=3832/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=3832/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=3832/test_label.csv --val_prop 0.3 --random_state 3832 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=8/rs=3832 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=8/rs=3832/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=8/rs=3832/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=8/rs=3832/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 21     | train
--------------------------------------------------------
1         Trainable params
20        Non-trainable params
21        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=160
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=8/rs=3832
+ '[' bias_only == dp_calibration ']'
+ '[' bias_only == temp_scaling ']'
+ '[' bias_only == bias_only ']'
+ method_old=bias_only
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=8/rs=3832/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=8/rs=3832 outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=8/rs=3832/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=8/rs=3832/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=3832/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=3832/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=3832/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=3832/test_label.csv --val_prop 0.3 --random_state 3832 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=8/rs=3832 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=8/rs=3832/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=8/rs=3832/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=8/rs=3832/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 21     | train
--------------------------------------------------------
20        Trainable params
1         Non-trainable params
21        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for size in ${dataset2samples[$dataset]}
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=2560
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=128/rs=129
+ '[' dp_calibration == dp_calibration ']'
+ method_old=affine_scalar
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=128/rs=129/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=128/rs=129 outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=128/rs=129/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=128/rs=129/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=129/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=129/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=129/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=129/test_label.csv --val_prop 0.3 --random_state 129 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=128/rs=129 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=128/rs=129/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=128/rs=129/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=128/rs=129/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 21     | train
--------------------------------------------------------
21        Trainable params
0         Non-trainable params
21        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=2560
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=128/rs=129
+ '[' temp_scaling == dp_calibration ']'
+ '[' temp_scaling == temp_scaling ']'
+ method_old=temp_scaling
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=128/rs=129/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=128/rs=129 outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=128/rs=129/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=128/rs=129/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=129/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=129/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=129/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=129/test_label.csv --val_prop 0.3 --random_state 129 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=128/rs=129 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=128/rs=129/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=128/rs=129/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=128/rs=129/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 21     | train
--------------------------------------------------------
1         Trainable params
20        Non-trainable params
21        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=2560
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=128/rs=129
+ '[' bias_only == dp_calibration ']'
+ '[' bias_only == temp_scaling ']'
+ '[' bias_only == bias_only ']'
+ method_old=bias_only
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=128/rs=129/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=128/rs=129 outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=128/rs=129/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=128/rs=129/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=129/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=129/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=129/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=129/test_label.csv --val_prop 0.3 --random_state 129 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=128/rs=129 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=128/rs=129/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=128/rs=129/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=128/rs=129/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 21     | train
--------------------------------------------------------
20        Trainable params
1         Non-trainable params
21        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=2560
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=128/rs=131
+ '[' dp_calibration == dp_calibration ']'
+ method_old=affine_scalar
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=128/rs=131/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=128/rs=131 outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=128/rs=131/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=128/rs=131/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=131/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=131/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=131/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=131/test_label.csv --val_prop 0.3 --random_state 131 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=128/rs=131 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=128/rs=131/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=128/rs=131/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=128/rs=131/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 21     | train
--------------------------------------------------------
21        Trainable params
0         Non-trainable params
21        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=2560
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=128/rs=131
+ '[' temp_scaling == dp_calibration ']'
+ '[' temp_scaling == temp_scaling ']'
+ method_old=temp_scaling
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=128/rs=131/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=128/rs=131 outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=128/rs=131/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=128/rs=131/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=131/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=131/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=131/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=131/test_label.csv --val_prop 0.3 --random_state 131 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=128/rs=131 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=128/rs=131/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=128/rs=131/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=128/rs=131/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 21     | train
--------------------------------------------------------
1         Trainable params
20        Non-trainable params
21        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=2560
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=128/rs=131
+ '[' bias_only == dp_calibration ']'
+ '[' bias_only == temp_scaling ']'
+ '[' bias_only == bias_only ']'
+ method_old=bias_only
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=128/rs=131/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=128/rs=131 outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=128/rs=131/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=128/rs=131/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=131/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=131/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=131/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=131/test_label.csv --val_prop 0.3 --random_state 131 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=128/rs=131 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=128/rs=131/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=128/rs=131/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=128/rs=131/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 21     | train
--------------------------------------------------------
20        Trainable params
1         Non-trainable params
21        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=2560
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=128/rs=543
+ '[' dp_calibration == dp_calibration ']'
+ method_old=affine_scalar
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=128/rs=543/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=128/rs=543 outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=128/rs=543/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=128/rs=543/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=543/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=543/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=543/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=543/test_label.csv --val_prop 0.3 --random_state 543 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=128/rs=543 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=128/rs=543/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=128/rs=543/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=128/rs=543/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 21     | train
--------------------------------------------------------
21        Trainable params
0         Non-trainable params
21        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=2560
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=128/rs=543
+ '[' temp_scaling == dp_calibration ']'
+ '[' temp_scaling == temp_scaling ']'
+ method_old=temp_scaling
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=128/rs=543/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=128/rs=543 outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=128/rs=543/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=128/rs=543/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=543/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=543/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=543/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=543/test_label.csv --val_prop 0.3 --random_state 543 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=128/rs=543 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=128/rs=543/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=128/rs=543/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=128/rs=543/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 21     | train
--------------------------------------------------------
1         Trainable params
20        Non-trainable params
21        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=2560
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=128/rs=543
+ '[' bias_only == dp_calibration ']'
+ '[' bias_only == temp_scaling ']'
+ '[' bias_only == bias_only ']'
+ method_old=bias_only
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=128/rs=543/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=128/rs=543 outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=128/rs=543/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=128/rs=543/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=543/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=543/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=543/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=543/test_label.csv --val_prop 0.3 --random_state 543 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=128/rs=543 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=128/rs=543/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=128/rs=543/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=128/rs=543/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 21     | train
--------------------------------------------------------
20        Trainable params
1         Non-trainable params
21        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=2560
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=128/rs=878
+ '[' dp_calibration == dp_calibration ']'
+ method_old=affine_scalar
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=128/rs=878/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=128/rs=878 outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=128/rs=878/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=128/rs=878/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=878/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=878/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=878/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=878/test_label.csv --val_prop 0.3 --random_state 878 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=128/rs=878 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=128/rs=878/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=128/rs=878/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=128/rs=878/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 21     | train
--------------------------------------------------------
21        Trainable params
0         Non-trainable params
21        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=2560
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=128/rs=878
+ '[' temp_scaling == dp_calibration ']'
+ '[' temp_scaling == temp_scaling ']'
+ method_old=temp_scaling
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=128/rs=878/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=128/rs=878 outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=128/rs=878/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=128/rs=878/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=878/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=878/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=878/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=878/test_label.csv --val_prop 0.3 --random_state 878 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=128/rs=878 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=128/rs=878/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=128/rs=878/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=128/rs=878/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 21     | train
--------------------------------------------------------
1         Trainable params
20        Non-trainable params
21        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=2560
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=128/rs=878
+ '[' bias_only == dp_calibration ']'
+ '[' bias_only == temp_scaling ']'
+ '[' bias_only == bias_only ']'
+ method_old=bias_only
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=128/rs=878/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=128/rs=878 outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=128/rs=878/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=128/rs=878/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=878/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=878/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=878/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=878/test_label.csv --val_prop 0.3 --random_state 878 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=128/rs=878 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=128/rs=878/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=128/rs=878/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=128/rs=878/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 21     | train
--------------------------------------------------------
20        Trainable params
1         Non-trainable params
21        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=2560
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=128/rs=909
+ '[' dp_calibration == dp_calibration ']'
+ method_old=affine_scalar
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=128/rs=909/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=128/rs=909 outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=128/rs=909/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=128/rs=909/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=909/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=909/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=909/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=909/test_label.csv --val_prop 0.3 --random_state 909 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=128/rs=909 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=128/rs=909/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=128/rs=909/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration/20newsgroups/size=128/rs=909/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 21     | train
--------------------------------------------------------
21        Trainable params
0         Non-trainable params
21        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=2560
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=128/rs=909
+ '[' temp_scaling == dp_calibration ']'
+ '[' temp_scaling == temp_scaling ']'
+ method_old=temp_scaling
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=128/rs=909/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=128/rs=909 outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=128/rs=909/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=128/rs=909/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=909/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=909/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=909/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=909/test_label.csv --val_prop 0.3 --random_state 909 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=128/rs=909 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=128/rs=909/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=128/rs=909/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling/20newsgroups/size=128/rs=909/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 21     | train
--------------------------------------------------------
1         Trainable params
20        Non-trainable params
21        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=2560
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=128/rs=909
+ '[' bias_only == dp_calibration ']'
+ '[' bias_only == temp_scaling ']'
+ '[' bias_only == bias_only ']'
+ method_old=bias_only
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=128/rs=909/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=128/rs=909 outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=128/rs=909/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=128/rs=909/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=909/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=909/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=909/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=909/test_label.csv --val_prop 0.3 --random_state 909 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=128/rs=909 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=128/rs=909/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=128/rs=909/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only/20newsgroups/size=128/rs=909/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 21     | train
--------------------------------------------------------
20        Trainable params
1         Non-trainable params
21        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for dataset in $DATASETS
+ for size in ${dataset2samples[$dataset]}
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=77
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=1/rs=322
+ '[' dp_calibration == dp_calibration ']'
+ method_old=affine_scalar
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=1/rs=322/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=1/rs=322 outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=1/rs=322/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=1/rs=322/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/banking77/size=1/rs=322/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/banking77/size=1/rs=322/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/banking77/size=1/rs=322/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/banking77/size=1/rs=322/test_label.csv --val_prop 0.3 --random_state 322 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=1/rs=322 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=1/rs=322/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=1/rs=322/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=1/rs=322/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 78     | train
--------------------------------------------------------
78        Trainable params
0         Non-trainable params
78        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=77
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=1/rs=322
+ '[' temp_scaling == dp_calibration ']'
+ '[' temp_scaling == temp_scaling ']'
+ method_old=temp_scaling
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=1/rs=322/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=1/rs=322 outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=1/rs=322/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=1/rs=322/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/banking77/size=1/rs=322/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/banking77/size=1/rs=322/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/banking77/size=1/rs=322/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/banking77/size=1/rs=322/test_label.csv --val_prop 0.3 --random_state 322 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=1/rs=322 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=1/rs=322/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=1/rs=322/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=1/rs=322/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 78     | train
--------------------------------------------------------
1         Trainable params
77        Non-trainable params
78        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=77
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=1/rs=322
+ '[' bias_only == dp_calibration ']'
+ '[' bias_only == temp_scaling ']'
+ '[' bias_only == bias_only ']'
+ method_old=bias_only
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=1/rs=322/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=1/rs=322 outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=1/rs=322/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=1/rs=322/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/banking77/size=1/rs=322/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/banking77/size=1/rs=322/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/banking77/size=1/rs=322/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/banking77/size=1/rs=322/test_label.csv --val_prop 0.3 --random_state 322 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=1/rs=322 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=1/rs=322/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=1/rs=322/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=1/rs=322/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 78     | train
--------------------------------------------------------
77        Trainable params
1         Non-trainable params
78        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=77
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=1/rs=444
+ '[' dp_calibration == dp_calibration ']'
+ method_old=affine_scalar
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=1/rs=444/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=1/rs=444 outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=1/rs=444/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=1/rs=444/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/banking77/size=1/rs=444/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/banking77/size=1/rs=444/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/banking77/size=1/rs=444/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/banking77/size=1/rs=444/test_label.csv --val_prop 0.3 --random_state 444 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=1/rs=444 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=1/rs=444/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=1/rs=444/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=1/rs=444/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 78     | train
--------------------------------------------------------
78        Trainable params
0         Non-trainable params
78        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=77
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=1/rs=444
+ '[' temp_scaling == dp_calibration ']'
+ '[' temp_scaling == temp_scaling ']'
+ method_old=temp_scaling
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=1/rs=444/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=1/rs=444 outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=1/rs=444/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=1/rs=444/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/banking77/size=1/rs=444/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/banking77/size=1/rs=444/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/banking77/size=1/rs=444/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/banking77/size=1/rs=444/test_label.csv --val_prop 0.3 --random_state 444 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=1/rs=444 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=1/rs=444/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=1/rs=444/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=1/rs=444/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 78     | train
--------------------------------------------------------
1         Trainable params
77        Non-trainable params
78        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=77
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=1/rs=444
+ '[' bias_only == dp_calibration ']'
+ '[' bias_only == temp_scaling ']'
+ '[' bias_only == bias_only ']'
+ method_old=bias_only
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=1/rs=444/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=1/rs=444 outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=1/rs=444/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=1/rs=444/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/banking77/size=1/rs=444/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/banking77/size=1/rs=444/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/banking77/size=1/rs=444/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/banking77/size=1/rs=444/test_label.csv --val_prop 0.3 --random_state 444 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=1/rs=444 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=1/rs=444/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=1/rs=444/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=1/rs=444/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 78     | train
--------------------------------------------------------
77        Trainable params
1         Non-trainable params
78        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=77
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=1/rs=848
+ '[' dp_calibration == dp_calibration ']'
+ method_old=affine_scalar
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=1/rs=848/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=1/rs=848 outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=1/rs=848/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=1/rs=848/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/banking77/size=1/rs=848/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/banking77/size=1/rs=848/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/banking77/size=1/rs=848/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/banking77/size=1/rs=848/test_label.csv --val_prop 0.3 --random_state 848 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=1/rs=848 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=1/rs=848/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=1/rs=848/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=1/rs=848/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 78     | train
--------------------------------------------------------
78        Trainable params
0         Non-trainable params
78        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=77
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=1/rs=848
+ '[' temp_scaling == dp_calibration ']'
+ '[' temp_scaling == temp_scaling ']'
+ method_old=temp_scaling
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=1/rs=848/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=1/rs=848 outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=1/rs=848/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=1/rs=848/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/banking77/size=1/rs=848/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/banking77/size=1/rs=848/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/banking77/size=1/rs=848/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/banking77/size=1/rs=848/test_label.csv --val_prop 0.3 --random_state 848 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=1/rs=848 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=1/rs=848/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=1/rs=848/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=1/rs=848/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 78     | train
--------------------------------------------------------
1         Trainable params
77        Non-trainable params
78        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=77
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=1/rs=848
+ '[' bias_only == dp_calibration ']'
+ '[' bias_only == temp_scaling ']'
+ '[' bias_only == bias_only ']'
+ method_old=bias_only
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=1/rs=848/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=1/rs=848 outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=1/rs=848/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=1/rs=848/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/banking77/size=1/rs=848/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/banking77/size=1/rs=848/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/banking77/size=1/rs=848/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/banking77/size=1/rs=848/test_label.csv --val_prop 0.3 --random_state 848 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=1/rs=848 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=1/rs=848/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=1/rs=848/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=1/rs=848/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 78     | train
--------------------------------------------------------
77        Trainable params
1         Non-trainable params
78        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=77
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=1/rs=858
+ '[' dp_calibration == dp_calibration ']'
+ method_old=affine_scalar
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=1/rs=858/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=1/rs=858 outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=1/rs=858/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=1/rs=858/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/banking77/size=1/rs=858/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/banking77/size=1/rs=858/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/banking77/size=1/rs=858/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/banking77/size=1/rs=858/test_label.csv --val_prop 0.3 --random_state 858 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=1/rs=858 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=1/rs=858/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=1/rs=858/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=1/rs=858/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 78     | train
--------------------------------------------------------
78        Trainable params
0         Non-trainable params
78        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=77
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=1/rs=858
+ '[' temp_scaling == dp_calibration ']'
+ '[' temp_scaling == temp_scaling ']'
+ method_old=temp_scaling
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=1/rs=858/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=1/rs=858 outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=1/rs=858/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=1/rs=858/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/banking77/size=1/rs=858/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/banking77/size=1/rs=858/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/banking77/size=1/rs=858/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/banking77/size=1/rs=858/test_label.csv --val_prop 0.3 --random_state 858 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=1/rs=858 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=1/rs=858/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=1/rs=858/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=1/rs=858/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 78     | train
--------------------------------------------------------
1         Trainable params
77        Non-trainable params
78        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=77
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=1/rs=858
+ '[' bias_only == dp_calibration ']'
+ '[' bias_only == temp_scaling ']'
+ '[' bias_only == bias_only ']'
+ method_old=bias_only
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=1/rs=858/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=1/rs=858 outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=1/rs=858/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=1/rs=858/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/banking77/size=1/rs=858/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/banking77/size=1/rs=858/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/banking77/size=1/rs=858/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/banking77/size=1/rs=858/test_label.csv --val_prop 0.3 --random_state 858 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=1/rs=858 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=1/rs=858/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=1/rs=858/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=1/rs=858/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 78     | train
--------------------------------------------------------
77        Trainable params
1         Non-trainable params
78        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=77
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=1/rs=868
+ '[' dp_calibration == dp_calibration ']'
+ method_old=affine_scalar
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=1/rs=868/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=1/rs=868 outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=1/rs=868/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=1/rs=868/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/banking77/size=1/rs=868/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/banking77/size=1/rs=868/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/banking77/size=1/rs=868/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/banking77/size=1/rs=868/test_label.csv --val_prop 0.3 --random_state 868 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=1/rs=868 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=1/rs=868/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=1/rs=868/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=1/rs=868/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 78     | train
--------------------------------------------------------
78        Trainable params
0         Non-trainable params
78        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=77
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=1/rs=868
+ '[' temp_scaling == dp_calibration ']'
+ '[' temp_scaling == temp_scaling ']'
+ method_old=temp_scaling
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=1/rs=868/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=1/rs=868 outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=1/rs=868/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=1/rs=868/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/banking77/size=1/rs=868/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/banking77/size=1/rs=868/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/banking77/size=1/rs=868/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/banking77/size=1/rs=868/test_label.csv --val_prop 0.3 --random_state 868 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=1/rs=868 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=1/rs=868/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=1/rs=868/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=1/rs=868/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 78     | train
--------------------------------------------------------
1         Trainable params
77        Non-trainable params
78        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=77
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=1/rs=868
+ '[' bias_only == dp_calibration ']'
+ '[' bias_only == temp_scaling ']'
+ '[' bias_only == bias_only ']'
+ method_old=bias_only
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=1/rs=868/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=1/rs=868 outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=1/rs=868/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=1/rs=868/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/banking77/size=1/rs=868/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/banking77/size=1/rs=868/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/banking77/size=1/rs=868/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/banking77/size=1/rs=868/test_label.csv --val_prop 0.3 --random_state 868 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=1/rs=868 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=1/rs=868/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=1/rs=868/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=1/rs=868/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 78     | train
--------------------------------------------------------
77        Trainable params
1         Non-trainable params
78        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for size in ${dataset2samples[$dataset]}
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=308
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=4/rs=295
+ '[' dp_calibration == dp_calibration ']'
+ method_old=affine_scalar
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=4/rs=295/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=4/rs=295 outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=4/rs=295/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=4/rs=295/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/banking77/size=4/rs=295/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/banking77/size=4/rs=295/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/banking77/size=4/rs=295/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/banking77/size=4/rs=295/test_label.csv --val_prop 0.3 --random_state 295 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=4/rs=295 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=4/rs=295/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=4/rs=295/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=4/rs=295/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 78     | train
--------------------------------------------------------
78        Trainable params
0         Non-trainable params
78        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=308
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=4/rs=295
+ '[' temp_scaling == dp_calibration ']'
+ '[' temp_scaling == temp_scaling ']'
+ method_old=temp_scaling
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=4/rs=295/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=4/rs=295 outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=4/rs=295/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=4/rs=295/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/banking77/size=4/rs=295/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/banking77/size=4/rs=295/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/banking77/size=4/rs=295/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/banking77/size=4/rs=295/test_label.csv --val_prop 0.3 --random_state 295 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=4/rs=295 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=4/rs=295/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=4/rs=295/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=4/rs=295/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 78     | train
--------------------------------------------------------
1         Trainable params
77        Non-trainable params
78        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=308
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=4/rs=295
+ '[' bias_only == dp_calibration ']'
+ '[' bias_only == temp_scaling ']'
+ '[' bias_only == bias_only ']'
+ method_old=bias_only
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=4/rs=295/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=4/rs=295 outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=4/rs=295/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=4/rs=295/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/banking77/size=4/rs=295/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/banking77/size=4/rs=295/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/banking77/size=4/rs=295/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/banking77/size=4/rs=295/test_label.csv --val_prop 0.3 --random_state 295 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=4/rs=295 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=4/rs=295/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=4/rs=295/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=4/rs=295/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 78     | train
--------------------------------------------------------
77        Trainable params
1         Non-trainable params
78        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=308
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=4/rs=926
+ '[' dp_calibration == dp_calibration ']'
+ method_old=affine_scalar
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=4/rs=926/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=4/rs=926 outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=4/rs=926/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=4/rs=926/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/banking77/size=4/rs=926/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/banking77/size=4/rs=926/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/banking77/size=4/rs=926/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/banking77/size=4/rs=926/test_label.csv --val_prop 0.3 --random_state 926 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=4/rs=926 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=4/rs=926/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=4/rs=926/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=4/rs=926/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 78     | train
--------------------------------------------------------
78        Trainable params
0         Non-trainable params
78        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=308
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=4/rs=926
+ '[' temp_scaling == dp_calibration ']'
+ '[' temp_scaling == temp_scaling ']'
+ method_old=temp_scaling
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=4/rs=926/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=4/rs=926 outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=4/rs=926/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=4/rs=926/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/banking77/size=4/rs=926/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/banking77/size=4/rs=926/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/banking77/size=4/rs=926/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/banking77/size=4/rs=926/test_label.csv --val_prop 0.3 --random_state 926 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=4/rs=926 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=4/rs=926/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=4/rs=926/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=4/rs=926/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 78     | train
--------------------------------------------------------
1         Trainable params
77        Non-trainable params
78        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=308
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=4/rs=926
+ '[' bias_only == dp_calibration ']'
+ '[' bias_only == temp_scaling ']'
+ '[' bias_only == bias_only ']'
+ method_old=bias_only
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=4/rs=926/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=4/rs=926 outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=4/rs=926/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=4/rs=926/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/banking77/size=4/rs=926/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/banking77/size=4/rs=926/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/banking77/size=4/rs=926/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/banking77/size=4/rs=926/test_label.csv --val_prop 0.3 --random_state 926 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=4/rs=926 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=4/rs=926/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=4/rs=926/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=4/rs=926/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 78     | train
--------------------------------------------------------
77        Trainable params
1         Non-trainable params
78        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=308
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=4/rs=962
+ '[' dp_calibration == dp_calibration ']'
+ method_old=affine_scalar
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=4/rs=962/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=4/rs=962 outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=4/rs=962/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=4/rs=962/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/banking77/size=4/rs=962/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/banking77/size=4/rs=962/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/banking77/size=4/rs=962/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/banking77/size=4/rs=962/test_label.csv --val_prop 0.3 --random_state 962 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=4/rs=962 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=4/rs=962/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=4/rs=962/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=4/rs=962/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 78     | train
--------------------------------------------------------
78        Trainable params
0         Non-trainable params
78        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=308
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=4/rs=962
+ '[' temp_scaling == dp_calibration ']'
+ '[' temp_scaling == temp_scaling ']'
+ method_old=temp_scaling
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=4/rs=962/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=4/rs=962 outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=4/rs=962/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=4/rs=962/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/banking77/size=4/rs=962/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/banking77/size=4/rs=962/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/banking77/size=4/rs=962/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/banking77/size=4/rs=962/test_label.csv --val_prop 0.3 --random_state 962 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=4/rs=962 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=4/rs=962/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=4/rs=962/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=4/rs=962/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 78     | train
--------------------------------------------------------
1         Trainable params
77        Non-trainable params
78        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=308
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=4/rs=962
+ '[' bias_only == dp_calibration ']'
+ '[' bias_only == temp_scaling ']'
+ '[' bias_only == bias_only ']'
+ method_old=bias_only
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=4/rs=962/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=4/rs=962 outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=4/rs=962/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=4/rs=962/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/banking77/size=4/rs=962/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/banking77/size=4/rs=962/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/banking77/size=4/rs=962/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/banking77/size=4/rs=962/test_label.csv --val_prop 0.3 --random_state 962 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=4/rs=962 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=4/rs=962/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=4/rs=962/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=4/rs=962/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 78     | train
--------------------------------------------------------
77        Trainable params
1         Non-trainable params
78        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=308
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=4/rs=2951
+ '[' dp_calibration == dp_calibration ']'
+ method_old=affine_scalar
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=4/rs=2951/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=4/rs=2951 outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=4/rs=2951/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=4/rs=2951/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/banking77/size=4/rs=2951/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/banking77/size=4/rs=2951/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/banking77/size=4/rs=2951/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/banking77/size=4/rs=2951/test_label.csv --val_prop 0.3 --random_state 2951 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=4/rs=2951 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=4/rs=2951/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=4/rs=2951/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=4/rs=2951/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 78     | train
--------------------------------------------------------
78        Trainable params
0         Non-trainable params
78        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=308
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=4/rs=2951
+ '[' temp_scaling == dp_calibration ']'
+ '[' temp_scaling == temp_scaling ']'
+ method_old=temp_scaling
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=4/rs=2951/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=4/rs=2951 outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=4/rs=2951/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=4/rs=2951/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/banking77/size=4/rs=2951/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/banking77/size=4/rs=2951/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/banking77/size=4/rs=2951/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/banking77/size=4/rs=2951/test_label.csv --val_prop 0.3 --random_state 2951 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=4/rs=2951 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=4/rs=2951/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=4/rs=2951/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=4/rs=2951/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 78     | train
--------------------------------------------------------
1         Trainable params
77        Non-trainable params
78        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=308
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=4/rs=2951
+ '[' bias_only == dp_calibration ']'
+ '[' bias_only == temp_scaling ']'
+ '[' bias_only == bias_only ']'
+ method_old=bias_only
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=4/rs=2951/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=4/rs=2951 outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=4/rs=2951/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=4/rs=2951/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/banking77/size=4/rs=2951/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/banking77/size=4/rs=2951/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/banking77/size=4/rs=2951/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/banking77/size=4/rs=2951/test_label.csv --val_prop 0.3 --random_state 2951 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=4/rs=2951 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=4/rs=2951/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=4/rs=2951/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=4/rs=2951/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 78     | train
--------------------------------------------------------
77        Trainable params
1         Non-trainable params
78        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=308
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=4/rs=9622
+ '[' dp_calibration == dp_calibration ']'
+ method_old=affine_scalar
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=4/rs=9622/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=4/rs=9622 outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=4/rs=9622/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=4/rs=9622/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/banking77/size=4/rs=9622/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/banking77/size=4/rs=9622/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/banking77/size=4/rs=9622/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/banking77/size=4/rs=9622/test_label.csv --val_prop 0.3 --random_state 9622 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=4/rs=9622 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=4/rs=9622/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=4/rs=9622/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=4/rs=9622/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 78     | train
--------------------------------------------------------
78        Trainable params
0         Non-trainable params
78        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=308
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=4/rs=9622
+ '[' temp_scaling == dp_calibration ']'
+ '[' temp_scaling == temp_scaling ']'
+ method_old=temp_scaling
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=4/rs=9622/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=4/rs=9622 outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=4/rs=9622/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=4/rs=9622/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/banking77/size=4/rs=9622/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/banking77/size=4/rs=9622/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/banking77/size=4/rs=9622/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/banking77/size=4/rs=9622/test_label.csv --val_prop 0.3 --random_state 9622 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=4/rs=9622 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=4/rs=9622/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=4/rs=9622/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=4/rs=9622/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 78     | train
--------------------------------------------------------
1         Trainable params
77        Non-trainable params
78        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=308
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=4/rs=9622
+ '[' bias_only == dp_calibration ']'
+ '[' bias_only == temp_scaling ']'
+ '[' bias_only == bias_only ']'
+ method_old=bias_only
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=4/rs=9622/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=4/rs=9622 outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=4/rs=9622/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=4/rs=9622/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/banking77/size=4/rs=9622/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/banking77/size=4/rs=9622/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/banking77/size=4/rs=9622/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/banking77/size=4/rs=9622/test_label.csv --val_prop 0.3 --random_state 9622 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=4/rs=9622 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=4/rs=9622/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=4/rs=9622/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=4/rs=9622/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 78     | train
--------------------------------------------------------
77        Trainable params
1         Non-trainable params
78        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for size in ${dataset2samples[$dataset]}
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=4928
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=64/rs=131
+ '[' dp_calibration == dp_calibration ']'
+ method_old=affine_scalar
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=64/rs=131/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=64/rs=131 outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=64/rs=131/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=64/rs=131/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/banking77/size=64/rs=131/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/banking77/size=64/rs=131/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/banking77/size=64/rs=131/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/banking77/size=64/rs=131/test_label.csv --val_prop 0.3 --random_state 131 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=64/rs=131 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=64/rs=131/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=64/rs=131/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=64/rs=131/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 78     | train
--------------------------------------------------------
78        Trainable params
0         Non-trainable params
78        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=4928
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=64/rs=131
+ '[' temp_scaling == dp_calibration ']'
+ '[' temp_scaling == temp_scaling ']'
+ method_old=temp_scaling
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=64/rs=131/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=64/rs=131 outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=64/rs=131/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=64/rs=131/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/banking77/size=64/rs=131/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/banking77/size=64/rs=131/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/banking77/size=64/rs=131/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/banking77/size=64/rs=131/test_label.csv --val_prop 0.3 --random_state 131 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=64/rs=131 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=64/rs=131/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=64/rs=131/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=64/rs=131/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 78     | train
--------------------------------------------------------
1         Trainable params
77        Non-trainable params
78        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=4928
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=64/rs=131
+ '[' bias_only == dp_calibration ']'
+ '[' bias_only == temp_scaling ']'
+ '[' bias_only == bias_only ']'
+ method_old=bias_only
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=64/rs=131/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=64/rs=131 outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=64/rs=131/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=64/rs=131/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/banking77/size=64/rs=131/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/banking77/size=64/rs=131/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/banking77/size=64/rs=131/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/banking77/size=64/rs=131/test_label.csv --val_prop 0.3 --random_state 131 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=64/rs=131 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=64/rs=131/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=64/rs=131/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=64/rs=131/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 78     | train
--------------------------------------------------------
77        Trainable params
1         Non-trainable params
78        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=4928
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=64/rs=888
+ '[' dp_calibration == dp_calibration ']'
+ method_old=affine_scalar
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=64/rs=888/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=64/rs=888 outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=64/rs=888/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=64/rs=888/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/banking77/size=64/rs=888/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/banking77/size=64/rs=888/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/banking77/size=64/rs=888/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/banking77/size=64/rs=888/test_label.csv --val_prop 0.3 --random_state 888 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=64/rs=888 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=64/rs=888/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=64/rs=888/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=64/rs=888/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 78     | train
--------------------------------------------------------
78        Trainable params
0         Non-trainable params
78        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=4928
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=64/rs=888
+ '[' temp_scaling == dp_calibration ']'
+ '[' temp_scaling == temp_scaling ']'
+ method_old=temp_scaling
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=64/rs=888/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=64/rs=888 outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=64/rs=888/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=64/rs=888/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/banking77/size=64/rs=888/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/banking77/size=64/rs=888/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/banking77/size=64/rs=888/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/banking77/size=64/rs=888/test_label.csv --val_prop 0.3 --random_state 888 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=64/rs=888 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=64/rs=888/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=64/rs=888/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=64/rs=888/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 78     | train
--------------------------------------------------------
1         Trainable params
77        Non-trainable params
78        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=4928
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=64/rs=888
+ '[' bias_only == dp_calibration ']'
+ '[' bias_only == temp_scaling ']'
+ '[' bias_only == bias_only ']'
+ method_old=bias_only
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=64/rs=888/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=64/rs=888 outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=64/rs=888/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=64/rs=888/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/banking77/size=64/rs=888/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/banking77/size=64/rs=888/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/banking77/size=64/rs=888/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/banking77/size=64/rs=888/test_label.csv --val_prop 0.3 --random_state 888 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=64/rs=888 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=64/rs=888/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=64/rs=888/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=64/rs=888/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 78     | train
--------------------------------------------------------
77        Trainable params
1         Non-trainable params
78        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=4928
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=64/rs=893
+ '[' dp_calibration == dp_calibration ']'
+ method_old=affine_scalar
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=64/rs=893/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=64/rs=893 outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=64/rs=893/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=64/rs=893/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/banking77/size=64/rs=893/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/banking77/size=64/rs=893/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/banking77/size=64/rs=893/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/banking77/size=64/rs=893/test_label.csv --val_prop 0.3 --random_state 893 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=64/rs=893 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=64/rs=893/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=64/rs=893/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=64/rs=893/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 78     | train
--------------------------------------------------------
78        Trainable params
0         Non-trainable params
78        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=4928
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=64/rs=893
+ '[' temp_scaling == dp_calibration ']'
+ '[' temp_scaling == temp_scaling ']'
+ method_old=temp_scaling
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=64/rs=893/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=64/rs=893 outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=64/rs=893/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=64/rs=893/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/banking77/size=64/rs=893/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/banking77/size=64/rs=893/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/banking77/size=64/rs=893/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/banking77/size=64/rs=893/test_label.csv --val_prop 0.3 --random_state 893 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=64/rs=893 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=64/rs=893/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=64/rs=893/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=64/rs=893/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 78     | train
--------------------------------------------------------
1         Trainable params
77        Non-trainable params
78        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=4928
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=64/rs=893
+ '[' bias_only == dp_calibration ']'
+ '[' bias_only == temp_scaling ']'
+ '[' bias_only == bias_only ']'
+ method_old=bias_only
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=64/rs=893/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=64/rs=893 outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=64/rs=893/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=64/rs=893/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/banking77/size=64/rs=893/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/banking77/size=64/rs=893/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/banking77/size=64/rs=893/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/banking77/size=64/rs=893/test_label.csv --val_prop 0.3 --random_state 893 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=64/rs=893 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=64/rs=893/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=64/rs=893/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=64/rs=893/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 78     | train
--------------------------------------------------------
77        Trainable params
1         Non-trainable params
78        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=4928
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=64/rs=912
+ '[' dp_calibration == dp_calibration ']'
+ method_old=affine_scalar
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=64/rs=912/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=64/rs=912 outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=64/rs=912/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=64/rs=912/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/banking77/size=64/rs=912/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/banking77/size=64/rs=912/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/banking77/size=64/rs=912/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/banking77/size=64/rs=912/test_label.csv --val_prop 0.3 --random_state 912 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=64/rs=912 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=64/rs=912/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=64/rs=912/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=64/rs=912/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 78     | train
--------------------------------------------------------
78        Trainable params
0         Non-trainable params
78        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=4928
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=64/rs=912
+ '[' temp_scaling == dp_calibration ']'
+ '[' temp_scaling == temp_scaling ']'
+ method_old=temp_scaling
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=64/rs=912/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=64/rs=912 outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=64/rs=912/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=64/rs=912/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/banking77/size=64/rs=912/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/banking77/size=64/rs=912/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/banking77/size=64/rs=912/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/banking77/size=64/rs=912/test_label.csv --val_prop 0.3 --random_state 912 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=64/rs=912 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=64/rs=912/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=64/rs=912/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=64/rs=912/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 78     | train
--------------------------------------------------------
1         Trainable params
77        Non-trainable params
78        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=4928
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=64/rs=912
+ '[' bias_only == dp_calibration ']'
+ '[' bias_only == temp_scaling ']'
+ '[' bias_only == bias_only ']'
+ method_old=bias_only
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=64/rs=912/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=64/rs=912 outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=64/rs=912/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=64/rs=912/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/banking77/size=64/rs=912/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/banking77/size=64/rs=912/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/banking77/size=64/rs=912/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/banking77/size=64/rs=912/test_label.csv --val_prop 0.3 --random_state 912 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=64/rs=912 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=64/rs=912/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=64/rs=912/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=64/rs=912/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 78     | train
--------------------------------------------------------
77        Trainable params
1         Non-trainable params
78        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=4928
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=64/rs=933
+ '[' dp_calibration == dp_calibration ']'
+ method_old=affine_scalar
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=64/rs=933/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=64/rs=933 outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=64/rs=933/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=64/rs=933/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/banking77/size=64/rs=933/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/banking77/size=64/rs=933/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/banking77/size=64/rs=933/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/banking77/size=64/rs=933/test_label.csv --val_prop 0.3 --random_state 933 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=64/rs=933 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=64/rs=933/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=64/rs=933/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration/banking77/size=64/rs=933/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 78     | train
--------------------------------------------------------
78        Trainable params
0         Non-trainable params
78        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=4928
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=64/rs=933
+ '[' temp_scaling == dp_calibration ']'
+ '[' temp_scaling == temp_scaling ']'
+ method_old=temp_scaling
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=64/rs=933/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=64/rs=933 outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=64/rs=933/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=64/rs=933/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/banking77/size=64/rs=933/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/banking77/size=64/rs=933/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/banking77/size=64/rs=933/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/banking77/size=64/rs=933/test_label.csv --val_prop 0.3 --random_state 933 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=64/rs=933 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=64/rs=933/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=64/rs=933/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling/banking77/size=64/rs=933/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 78     | train
--------------------------------------------------------
1         Trainable params
77        Non-trainable params
78        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=4928
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=64/rs=933
+ '[' bias_only == dp_calibration ']'
+ '[' bias_only == temp_scaling ']'
+ '[' bias_only == bias_only ']'
+ method_old=bias_only
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=64/rs=933/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=64/rs=933 outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=64/rs=933/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=64/rs=933/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/banking77/size=64/rs=933/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/banking77/size=64/rs=933/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/banking77/size=64/rs=933/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/banking77/size=64/rs=933/test_label.csv --val_prop 0.3 --random_state 933 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=64/rs=933 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=64/rs=933/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=64/rs=933/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only/banking77/size=64/rs=933/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 78     | train
--------------------------------------------------------
77        Trainable params
1         Non-trainable params
78        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ ./scripts/lora_plus_affine_cal_no_es.sh
+ CUDA_VISIBLE_DEVICES=0
+ accelerator=cpu
+ max_ls=40
+ learning_rate=1e-2
+ max_epochs=30
+ model2checkpoint=(['tinyllama']='/mnt/extra/lestienne/lit-checkpoints/TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T')
+ declare -A model2checkpoint
+ DATASETS='sst2 agnews dbpedia 20newsgroups banking77'
+ methods='dp_calibration temp_scaling bias_only'
+ dataset2numclasses=(['sst2']='2' ['agnews']='4' ['dbpedia']='14' ['20newsgroups']='20' ['banking77']='77')
+ declare -A dataset2numclasses
+ dataset2samples=(['sst2']='8 32 512' ['agnews']='4 16 256' ['dbpedia']='2 8 128' ['20newsgroups']='2 8 128' ['banking77']='1 4 64')
+ declare -A dataset2samples
+ dataset2seed=(['sst2_8']='639 923 932 6391 9322' ['sst2_32']='1564 1738 1783 15641 17832' ['sst2_512']='111 121 767 890 999' ['agnews_4']='295 926 962 2951 9622' ['agnews_16']='738 564 783 5641 7832' ['agnews_256']='493 821 812 4931 8212' ['dbpedia_2']='435 927 972 4351 9722' ['dbpedia_8']='338 364 383 3641 3832' ['dbpedia_128']='129 131 543 878 909' ['20newsgroups_2']='435 927 972 4351 9722' ['20newsgroups_8']='338 364 383 3641 3832' ['20newsgroups_128']='129 131 543 878 909' ['banking77_1']='322 444 848 858 868' ['banking77_4']='295 926 962 2951 9622' ['banking77_64']='131 888 893 912 933')
+ declare -A dataset2seed
+ for model in ${!model2checkpoint[@]}
+ checkpoint=/mnt/extra/lestienne/lit-checkpoints/TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T
+ for dataset in $DATASETS
+ for size in ${dataset2samples[$dataset]}
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=16
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=8/rs=639
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=8/rs=639/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=8/rs=639 outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=8/rs=639/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=8/rs=639/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/sst2/size=8/rs=639/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/sst2/size=8/rs=639/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/sst2/size=8/rs=639/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/sst2/size=8/rs=639/test_label.csv --val_prop 0 --random_state 639 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=8/rs=639 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=8/rs=639/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=8/rs=639/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=8/rs=639/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 3      | train
--------------------------------------------------------
3         Trainable params
0         Non-trainable params
3         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=16
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=8/rs=639
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=8/rs=639/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=8/rs=639 outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=8/rs=639/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=8/rs=639/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/sst2/size=8/rs=639/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/sst2/size=8/rs=639/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/sst2/size=8/rs=639/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/sst2/size=8/rs=639/test_label.csv --val_prop 0 --random_state 639 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=8/rs=639 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=8/rs=639/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=8/rs=639/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=8/rs=639/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 3      | train
--------------------------------------------------------
1         Trainable params
2         Non-trainable params
3         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=16
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=8/rs=639
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=8/rs=639/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=8/rs=639 outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=8/rs=639/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=8/rs=639/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/sst2/size=8/rs=639/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/sst2/size=8/rs=639/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/sst2/size=8/rs=639/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/sst2/size=8/rs=639/test_label.csv --val_prop 0 --random_state 639 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=8/rs=639 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=8/rs=639/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=8/rs=639/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=8/rs=639/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 3      | train
--------------------------------------------------------
2         Trainable params
1         Non-trainable params
3         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=16
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=8/rs=923
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=8/rs=923/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=8/rs=923 outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=8/rs=923/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=8/rs=923/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/sst2/size=8/rs=923/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/sst2/size=8/rs=923/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/sst2/size=8/rs=923/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/sst2/size=8/rs=923/test_label.csv --val_prop 0 --random_state 923 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=8/rs=923 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=8/rs=923/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=8/rs=923/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=8/rs=923/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 3      | train
--------------------------------------------------------
3         Trainable params
0         Non-trainable params
3         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=16
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=8/rs=923
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=8/rs=923/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=8/rs=923 outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=8/rs=923/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=8/rs=923/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/sst2/size=8/rs=923/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/sst2/size=8/rs=923/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/sst2/size=8/rs=923/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/sst2/size=8/rs=923/test_label.csv --val_prop 0 --random_state 923 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=8/rs=923 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=8/rs=923/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=8/rs=923/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=8/rs=923/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 3      | train
--------------------------------------------------------
1         Trainable params
2         Non-trainable params
3         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=16
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=8/rs=923
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=8/rs=923/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=8/rs=923 outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=8/rs=923/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=8/rs=923/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/sst2/size=8/rs=923/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/sst2/size=8/rs=923/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/sst2/size=8/rs=923/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/sst2/size=8/rs=923/test_label.csv --val_prop 0 --random_state 923 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=8/rs=923 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=8/rs=923/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=8/rs=923/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=8/rs=923/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 3      | train
--------------------------------------------------------
2         Trainable params
1         Non-trainable params
3         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=16
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=8/rs=932
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=8/rs=932/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=8/rs=932 outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=8/rs=932/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=8/rs=932/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/sst2/size=8/rs=932/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/sst2/size=8/rs=932/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/sst2/size=8/rs=932/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/sst2/size=8/rs=932/test_label.csv --val_prop 0 --random_state 932 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=8/rs=932 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=8/rs=932/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=8/rs=932/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=8/rs=932/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 3      | train
--------------------------------------------------------
3         Trainable params
0         Non-trainable params
3         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=16
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=8/rs=932
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=8/rs=932/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=8/rs=932 outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=8/rs=932/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=8/rs=932/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/sst2/size=8/rs=932/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/sst2/size=8/rs=932/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/sst2/size=8/rs=932/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/sst2/size=8/rs=932/test_label.csv --val_prop 0 --random_state 932 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=8/rs=932 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=8/rs=932/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=8/rs=932/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=8/rs=932/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 3      | train
--------------------------------------------------------
1         Trainable params
2         Non-trainable params
3         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=16
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=8/rs=932
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=8/rs=932/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=8/rs=932 outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=8/rs=932/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=8/rs=932/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/sst2/size=8/rs=932/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/sst2/size=8/rs=932/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/sst2/size=8/rs=932/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/sst2/size=8/rs=932/test_label.csv --val_prop 0 --random_state 932 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=8/rs=932 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=8/rs=932/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=8/rs=932/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=8/rs=932/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 3      | train
--------------------------------------------------------
2         Trainable params
1         Non-trainable params
3         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=16
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=8/rs=6391
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=8/rs=6391/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=8/rs=6391 outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=8/rs=6391/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=8/rs=6391/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/sst2/size=8/rs=6391/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/sst2/size=8/rs=6391/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/sst2/size=8/rs=6391/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/sst2/size=8/rs=6391/test_label.csv --val_prop 0 --random_state 6391 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=8/rs=6391 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=8/rs=6391/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=8/rs=6391/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=8/rs=6391/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 3      | train
--------------------------------------------------------
3         Trainable params
0         Non-trainable params
3         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=16
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=8/rs=6391
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=8/rs=6391/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=8/rs=6391 outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=8/rs=6391/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=8/rs=6391/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/sst2/size=8/rs=6391/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/sst2/size=8/rs=6391/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/sst2/size=8/rs=6391/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/sst2/size=8/rs=6391/test_label.csv --val_prop 0 --random_state 6391 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=8/rs=6391 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=8/rs=6391/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=8/rs=6391/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=8/rs=6391/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 3      | train
--------------------------------------------------------
1         Trainable params
2         Non-trainable params
3         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=16
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=8/rs=6391
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=8/rs=6391/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=8/rs=6391 outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=8/rs=6391/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=8/rs=6391/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/sst2/size=8/rs=6391/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/sst2/size=8/rs=6391/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/sst2/size=8/rs=6391/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/sst2/size=8/rs=6391/test_label.csv --val_prop 0 --random_state 6391 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=8/rs=6391 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=8/rs=6391/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=8/rs=6391/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=8/rs=6391/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 3      | train
--------------------------------------------------------
2         Trainable params
1         Non-trainable params
3         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=16
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=8/rs=9322
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=8/rs=9322/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=8/rs=9322 outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=8/rs=9322/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=8/rs=9322/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/sst2/size=8/rs=9322/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/sst2/size=8/rs=9322/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/sst2/size=8/rs=9322/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/sst2/size=8/rs=9322/test_label.csv --val_prop 0 --random_state 9322 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=8/rs=9322 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=8/rs=9322/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=8/rs=9322/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=8/rs=9322/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 3      | train
--------------------------------------------------------
3         Trainable params
0         Non-trainable params
3         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=16
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=8/rs=9322
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=8/rs=9322/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=8/rs=9322 outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=8/rs=9322/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=8/rs=9322/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/sst2/size=8/rs=9322/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/sst2/size=8/rs=9322/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/sst2/size=8/rs=9322/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/sst2/size=8/rs=9322/test_label.csv --val_prop 0 --random_state 9322 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=8/rs=9322 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=8/rs=9322/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=8/rs=9322/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=8/rs=9322/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 3      | train
--------------------------------------------------------
1         Trainable params
2         Non-trainable params
3         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=16
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=8/rs=9322
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=8/rs=9322/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=8/rs=9322 outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=8/rs=9322/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=8/rs=9322/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/sst2/size=8/rs=9322/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/sst2/size=8/rs=9322/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/sst2/size=8/rs=9322/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/sst2/size=8/rs=9322/test_label.csv --val_prop 0 --random_state 9322 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=8/rs=9322 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=8/rs=9322/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=8/rs=9322/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=8/rs=9322/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 3      | train
--------------------------------------------------------
2         Trainable params
1         Non-trainable params
3         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for size in ${dataset2samples[$dataset]}
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=64
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=32/rs=1564
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=32/rs=1564/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=32/rs=1564 outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=32/rs=1564/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=32/rs=1564/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/sst2/size=32/rs=1564/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/sst2/size=32/rs=1564/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/sst2/size=32/rs=1564/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/sst2/size=32/rs=1564/test_label.csv --val_prop 0 --random_state 1564 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=32/rs=1564 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=32/rs=1564/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=32/rs=1564/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=32/rs=1564/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 3      | train
--------------------------------------------------------
3         Trainable params
0         Non-trainable params
3         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=64
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=32/rs=1564
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=32/rs=1564/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=32/rs=1564 outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=32/rs=1564/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=32/rs=1564/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/sst2/size=32/rs=1564/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/sst2/size=32/rs=1564/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/sst2/size=32/rs=1564/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/sst2/size=32/rs=1564/test_label.csv --val_prop 0 --random_state 1564 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=32/rs=1564 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=32/rs=1564/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=32/rs=1564/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=32/rs=1564/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 3      | train
--------------------------------------------------------
1         Trainable params
2         Non-trainable params
3         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=64
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=32/rs=1564
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=32/rs=1564/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=32/rs=1564 outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=32/rs=1564/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=32/rs=1564/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/sst2/size=32/rs=1564/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/sst2/size=32/rs=1564/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/sst2/size=32/rs=1564/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/sst2/size=32/rs=1564/test_label.csv --val_prop 0 --random_state 1564 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=32/rs=1564 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=32/rs=1564/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=32/rs=1564/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=32/rs=1564/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 3      | train
--------------------------------------------------------
2         Trainable params
1         Non-trainable params
3         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=64
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=32/rs=1738
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=32/rs=1738/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=32/rs=1738 outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=32/rs=1738/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=32/rs=1738/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/sst2/size=32/rs=1738/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/sst2/size=32/rs=1738/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/sst2/size=32/rs=1738/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/sst2/size=32/rs=1738/test_label.csv --val_prop 0 --random_state 1738 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=32/rs=1738 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=32/rs=1738/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=32/rs=1738/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=32/rs=1738/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 3      | train
--------------------------------------------------------
3         Trainable params
0         Non-trainable params
3         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=64
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=32/rs=1738
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=32/rs=1738/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=32/rs=1738 outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=32/rs=1738/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=32/rs=1738/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/sst2/size=32/rs=1738/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/sst2/size=32/rs=1738/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/sst2/size=32/rs=1738/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/sst2/size=32/rs=1738/test_label.csv --val_prop 0 --random_state 1738 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=32/rs=1738 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=32/rs=1738/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=32/rs=1738/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=32/rs=1738/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 3      | train
--------------------------------------------------------
1         Trainable params
2         Non-trainable params
3         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=64
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=32/rs=1738
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=32/rs=1738/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=32/rs=1738 outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=32/rs=1738/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=32/rs=1738/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/sst2/size=32/rs=1738/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/sst2/size=32/rs=1738/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/sst2/size=32/rs=1738/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/sst2/size=32/rs=1738/test_label.csv --val_prop 0 --random_state 1738 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=32/rs=1738 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=32/rs=1738/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=32/rs=1738/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=32/rs=1738/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 3      | train
--------------------------------------------------------
2         Trainable params
1         Non-trainable params
3         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=64
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=32/rs=1783
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=32/rs=1783/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=32/rs=1783 outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=32/rs=1783/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=32/rs=1783/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/sst2/size=32/rs=1783/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/sst2/size=32/rs=1783/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/sst2/size=32/rs=1783/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/sst2/size=32/rs=1783/test_label.csv --val_prop 0 --random_state 1783 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=32/rs=1783 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=32/rs=1783/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=32/rs=1783/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=32/rs=1783/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 3      | train
--------------------------------------------------------
3         Trainable params
0         Non-trainable params
3         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=64
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=32/rs=1783
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=32/rs=1783/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=32/rs=1783 outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=32/rs=1783/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=32/rs=1783/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/sst2/size=32/rs=1783/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/sst2/size=32/rs=1783/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/sst2/size=32/rs=1783/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/sst2/size=32/rs=1783/test_label.csv --val_prop 0 --random_state 1783 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=32/rs=1783 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=32/rs=1783/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=32/rs=1783/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=32/rs=1783/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 3      | train
--------------------------------------------------------
1         Trainable params
2         Non-trainable params
3         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=64
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=32/rs=1783
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=32/rs=1783/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=32/rs=1783 outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=32/rs=1783/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=32/rs=1783/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/sst2/size=32/rs=1783/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/sst2/size=32/rs=1783/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/sst2/size=32/rs=1783/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/sst2/size=32/rs=1783/test_label.csv --val_prop 0 --random_state 1783 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=32/rs=1783 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=32/rs=1783/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=32/rs=1783/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=32/rs=1783/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 3      | train
--------------------------------------------------------
2         Trainable params
1         Non-trainable params
3         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=64
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=32/rs=15641
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=32/rs=15641/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=32/rs=15641 outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=32/rs=15641/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=32/rs=15641/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/sst2/size=32/rs=15641/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/sst2/size=32/rs=15641/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/sst2/size=32/rs=15641/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/sst2/size=32/rs=15641/test_label.csv --val_prop 0 --random_state 15641 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=32/rs=15641 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=32/rs=15641/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=32/rs=15641/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=32/rs=15641/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 3      | train
--------------------------------------------------------
3         Trainable params
0         Non-trainable params
3         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=64
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=32/rs=15641
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=32/rs=15641/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=32/rs=15641 outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=32/rs=15641/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=32/rs=15641/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/sst2/size=32/rs=15641/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/sst2/size=32/rs=15641/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/sst2/size=32/rs=15641/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/sst2/size=32/rs=15641/test_label.csv --val_prop 0 --random_state 15641 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=32/rs=15641 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=32/rs=15641/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=32/rs=15641/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=32/rs=15641/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 3      | train
--------------------------------------------------------
1         Trainable params
2         Non-trainable params
3         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=64
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=32/rs=15641
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=32/rs=15641/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=32/rs=15641 outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=32/rs=15641/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=32/rs=15641/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/sst2/size=32/rs=15641/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/sst2/size=32/rs=15641/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/sst2/size=32/rs=15641/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/sst2/size=32/rs=15641/test_label.csv --val_prop 0 --random_state 15641 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=32/rs=15641 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=32/rs=15641/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=32/rs=15641/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=32/rs=15641/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 3      | train
--------------------------------------------------------
2         Trainable params
1         Non-trainable params
3         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=64
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=32/rs=17832
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=32/rs=17832/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=32/rs=17832 outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=32/rs=17832/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=32/rs=17832/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/sst2/size=32/rs=17832/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/sst2/size=32/rs=17832/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/sst2/size=32/rs=17832/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/sst2/size=32/rs=17832/test_label.csv --val_prop 0 --random_state 17832 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=32/rs=17832 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=32/rs=17832/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=32/rs=17832/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=32/rs=17832/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 3      | train
--------------------------------------------------------
3         Trainable params
0         Non-trainable params
3         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=64
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=32/rs=17832
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=32/rs=17832/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=32/rs=17832 outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=32/rs=17832/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=32/rs=17832/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/sst2/size=32/rs=17832/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/sst2/size=32/rs=17832/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/sst2/size=32/rs=17832/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/sst2/size=32/rs=17832/test_label.csv --val_prop 0 --random_state 17832 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=32/rs=17832 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=32/rs=17832/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=32/rs=17832/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=32/rs=17832/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 3      | train
--------------------------------------------------------
1         Trainable params
2         Non-trainable params
3         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=64
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=32/rs=17832
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=32/rs=17832/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=32/rs=17832 outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=32/rs=17832/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=32/rs=17832/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/sst2/size=32/rs=17832/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/sst2/size=32/rs=17832/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/sst2/size=32/rs=17832/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/sst2/size=32/rs=17832/test_label.csv --val_prop 0 --random_state 17832 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=32/rs=17832 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=32/rs=17832/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=32/rs=17832/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=32/rs=17832/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 3      | train
--------------------------------------------------------
2         Trainable params
1         Non-trainable params
3         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for size in ${dataset2samples[$dataset]}
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=1024
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=512/rs=111
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=512/rs=111/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=512/rs=111 outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=512/rs=111/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=512/rs=111/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/sst2/size=512/rs=111/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/sst2/size=512/rs=111/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/sst2/size=512/rs=111/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/sst2/size=512/rs=111/test_label.csv --val_prop 0 --random_state 111 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=512/rs=111 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=512/rs=111/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=512/rs=111/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=512/rs=111/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 3      | train
--------------------------------------------------------
3         Trainable params
0         Non-trainable params
3         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=1024
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=512/rs=111
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=512/rs=111/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=512/rs=111 outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=512/rs=111/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=512/rs=111/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/sst2/size=512/rs=111/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/sst2/size=512/rs=111/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/sst2/size=512/rs=111/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/sst2/size=512/rs=111/test_label.csv --val_prop 0 --random_state 111 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=512/rs=111 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=512/rs=111/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=512/rs=111/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=512/rs=111/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 3      | train
--------------------------------------------------------
1         Trainable params
2         Non-trainable params
3         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=1024
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=512/rs=111
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=512/rs=111/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=512/rs=111 outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=512/rs=111/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=512/rs=111/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/sst2/size=512/rs=111/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/sst2/size=512/rs=111/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/sst2/size=512/rs=111/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/sst2/size=512/rs=111/test_label.csv --val_prop 0 --random_state 111 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=512/rs=111 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=512/rs=111/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=512/rs=111/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=512/rs=111/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 3      | train
--------------------------------------------------------
2         Trainable params
1         Non-trainable params
3         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=1024
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=512/rs=121
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=512/rs=121/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=512/rs=121 outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=512/rs=121/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=512/rs=121/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/sst2/size=512/rs=121/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/sst2/size=512/rs=121/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/sst2/size=512/rs=121/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/sst2/size=512/rs=121/test_label.csv --val_prop 0 --random_state 121 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=512/rs=121 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=512/rs=121/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=512/rs=121/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=512/rs=121/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 3      | train
--------------------------------------------------------
3         Trainable params
0         Non-trainable params
3         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=1024
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=512/rs=121
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=512/rs=121/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=512/rs=121 outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=512/rs=121/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=512/rs=121/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/sst2/size=512/rs=121/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/sst2/size=512/rs=121/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/sst2/size=512/rs=121/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/sst2/size=512/rs=121/test_label.csv --val_prop 0 --random_state 121 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=512/rs=121 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=512/rs=121/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=512/rs=121/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=512/rs=121/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 3      | train
--------------------------------------------------------
1         Trainable params
2         Non-trainable params
3         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=1024
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=512/rs=121
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=512/rs=121/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=512/rs=121 outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=512/rs=121/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=512/rs=121/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/sst2/size=512/rs=121/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/sst2/size=512/rs=121/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/sst2/size=512/rs=121/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/sst2/size=512/rs=121/test_label.csv --val_prop 0 --random_state 121 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=512/rs=121 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=512/rs=121/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=512/rs=121/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=512/rs=121/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 3      | train
--------------------------------------------------------
2         Trainable params
1         Non-trainable params
3         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=1024
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=512/rs=767
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=512/rs=767/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=512/rs=767 outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=512/rs=767/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=512/rs=767/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/sst2/size=512/rs=767/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/sst2/size=512/rs=767/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/sst2/size=512/rs=767/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/sst2/size=512/rs=767/test_label.csv --val_prop 0 --random_state 767 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=512/rs=767 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=512/rs=767/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=512/rs=767/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=512/rs=767/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 3      | train
--------------------------------------------------------
3         Trainable params
0         Non-trainable params
3         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=1024
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=512/rs=767
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=512/rs=767/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=512/rs=767 outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=512/rs=767/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=512/rs=767/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/sst2/size=512/rs=767/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/sst2/size=512/rs=767/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/sst2/size=512/rs=767/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/sst2/size=512/rs=767/test_label.csv --val_prop 0 --random_state 767 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=512/rs=767 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=512/rs=767/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=512/rs=767/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=512/rs=767/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 3      | train
--------------------------------------------------------
1         Trainable params
2         Non-trainable params
3         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=1024
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=512/rs=767
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=512/rs=767/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=512/rs=767 outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=512/rs=767/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=512/rs=767/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/sst2/size=512/rs=767/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/sst2/size=512/rs=767/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/sst2/size=512/rs=767/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/sst2/size=512/rs=767/test_label.csv --val_prop 0 --random_state 767 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=512/rs=767 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=512/rs=767/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=512/rs=767/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=512/rs=767/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 3      | train
--------------------------------------------------------
2         Trainable params
1         Non-trainable params
3         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=1024
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=512/rs=890
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=512/rs=890/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=512/rs=890 outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=512/rs=890/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=512/rs=890/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/sst2/size=512/rs=890/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/sst2/size=512/rs=890/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/sst2/size=512/rs=890/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/sst2/size=512/rs=890/test_label.csv --val_prop 0 --random_state 890 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=512/rs=890 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=512/rs=890/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=512/rs=890/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=512/rs=890/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 3      | train
--------------------------------------------------------
3         Trainable params
0         Non-trainable params
3         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=1024
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=512/rs=890
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=512/rs=890/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=512/rs=890 outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=512/rs=890/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=512/rs=890/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/sst2/size=512/rs=890/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/sst2/size=512/rs=890/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/sst2/size=512/rs=890/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/sst2/size=512/rs=890/test_label.csv --val_prop 0 --random_state 890 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=512/rs=890 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=512/rs=890/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=512/rs=890/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=512/rs=890/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 3      | train
--------------------------------------------------------
1         Trainable params
2         Non-trainable params
3         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=1024
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=512/rs=890
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=512/rs=890/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=512/rs=890 outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=512/rs=890/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=512/rs=890/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/sst2/size=512/rs=890/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/sst2/size=512/rs=890/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/sst2/size=512/rs=890/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/sst2/size=512/rs=890/test_label.csv --val_prop 0 --random_state 890 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=512/rs=890 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=512/rs=890/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=512/rs=890/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=512/rs=890/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 3      | train
--------------------------------------------------------
2         Trainable params
1         Non-trainable params
3         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=1024
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=512/rs=999
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=512/rs=999/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=512/rs=999 outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=512/rs=999/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=512/rs=999/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/sst2/size=512/rs=999/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/sst2/size=512/rs=999/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/sst2/size=512/rs=999/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/sst2/size=512/rs=999/test_label.csv --val_prop 0 --random_state 999 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=512/rs=999 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=512/rs=999/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=512/rs=999/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/sst2/size=512/rs=999/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 3      | train
--------------------------------------------------------
3         Trainable params
0         Non-trainable params
3         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=1024
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=512/rs=999
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=512/rs=999/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=512/rs=999 outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=512/rs=999/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=512/rs=999/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/sst2/size=512/rs=999/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/sst2/size=512/rs=999/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/sst2/size=512/rs=999/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/sst2/size=512/rs=999/test_label.csv --val_prop 0 --random_state 999 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=512/rs=999 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=512/rs=999/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=512/rs=999/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/sst2/size=512/rs=999/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 3      | train
--------------------------------------------------------
1         Trainable params
2         Non-trainable params
3         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=1024
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=512/rs=999
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=512/rs=999/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=512/rs=999 outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=512/rs=999/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=512/rs=999/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/sst2/size=512/rs=999/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/sst2/size=512/rs=999/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/sst2/size=512/rs=999/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/sst2/size=512/rs=999/test_label.csv --val_prop 0 --random_state 999 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=512/rs=999 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=512/rs=999/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=512/rs=999/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/sst2/size=512/rs=999/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 3      | train
--------------------------------------------------------
2         Trainable params
1         Non-trainable params
3         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for dataset in $DATASETS
+ for size in ${dataset2samples[$dataset]}
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=16
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=4/rs=295
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=4/rs=295/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=4/rs=295 outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=4/rs=295/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=4/rs=295/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/agnews/size=4/rs=295/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/agnews/size=4/rs=295/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/agnews/size=4/rs=295/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/agnews/size=4/rs=295/test_label.csv --val_prop 0 --random_state 295 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=4/rs=295 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=4/rs=295/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=4/rs=295/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=4/rs=295/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 5      | train
--------------------------------------------------------
5         Trainable params
0         Non-trainable params
5         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=16
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=4/rs=295
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=4/rs=295/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=4/rs=295 outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=4/rs=295/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=4/rs=295/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/agnews/size=4/rs=295/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/agnews/size=4/rs=295/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/agnews/size=4/rs=295/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/agnews/size=4/rs=295/test_label.csv --val_prop 0 --random_state 295 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=4/rs=295 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=4/rs=295/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=4/rs=295/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=4/rs=295/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 5      | train
--------------------------------------------------------
1         Trainable params
4         Non-trainable params
5         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=16
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=4/rs=295
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=4/rs=295/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=4/rs=295 outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=4/rs=295/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=4/rs=295/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/agnews/size=4/rs=295/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/agnews/size=4/rs=295/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/agnews/size=4/rs=295/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/agnews/size=4/rs=295/test_label.csv --val_prop 0 --random_state 295 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=4/rs=295 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=4/rs=295/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=4/rs=295/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=4/rs=295/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 5      | train
--------------------------------------------------------
4         Trainable params
1         Non-trainable params
5         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=16
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=4/rs=926
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=4/rs=926/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=4/rs=926 outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=4/rs=926/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=4/rs=926/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/agnews/size=4/rs=926/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/agnews/size=4/rs=926/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/agnews/size=4/rs=926/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/agnews/size=4/rs=926/test_label.csv --val_prop 0 --random_state 926 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=4/rs=926 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=4/rs=926/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=4/rs=926/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=4/rs=926/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 5      | train
--------------------------------------------------------
5         Trainable params
0         Non-trainable params
5         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=16
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=4/rs=926
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=4/rs=926/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=4/rs=926 outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=4/rs=926/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=4/rs=926/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/agnews/size=4/rs=926/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/agnews/size=4/rs=926/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/agnews/size=4/rs=926/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/agnews/size=4/rs=926/test_label.csv --val_prop 0 --random_state 926 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=4/rs=926 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=4/rs=926/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=4/rs=926/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=4/rs=926/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 5      | train
--------------------------------------------------------
1         Trainable params
4         Non-trainable params
5         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=16
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=4/rs=926
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=4/rs=926/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=4/rs=926 outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=4/rs=926/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=4/rs=926/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/agnews/size=4/rs=926/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/agnews/size=4/rs=926/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/agnews/size=4/rs=926/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/agnews/size=4/rs=926/test_label.csv --val_prop 0 --random_state 926 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=4/rs=926 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=4/rs=926/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=4/rs=926/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=4/rs=926/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 5      | train
--------------------------------------------------------
4         Trainable params
1         Non-trainable params
5         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=16
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=4/rs=962
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=4/rs=962/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=4/rs=962 outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=4/rs=962/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=4/rs=962/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/agnews/size=4/rs=962/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/agnews/size=4/rs=962/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/agnews/size=4/rs=962/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/agnews/size=4/rs=962/test_label.csv --val_prop 0 --random_state 962 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=4/rs=962 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=4/rs=962/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=4/rs=962/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=4/rs=962/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 5      | train
--------------------------------------------------------
5         Trainable params
0         Non-trainable params
5         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=16
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=4/rs=962
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=4/rs=962/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=4/rs=962 outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=4/rs=962/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=4/rs=962/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/agnews/size=4/rs=962/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/agnews/size=4/rs=962/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/agnews/size=4/rs=962/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/agnews/size=4/rs=962/test_label.csv --val_prop 0 --random_state 962 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=4/rs=962 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=4/rs=962/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=4/rs=962/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=4/rs=962/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 5      | train
--------------------------------------------------------
1         Trainable params
4         Non-trainable params
5         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=16
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=4/rs=962
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=4/rs=962/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=4/rs=962 outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=4/rs=962/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=4/rs=962/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/agnews/size=4/rs=962/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/agnews/size=4/rs=962/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/agnews/size=4/rs=962/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/agnews/size=4/rs=962/test_label.csv --val_prop 0 --random_state 962 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=4/rs=962 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=4/rs=962/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=4/rs=962/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=4/rs=962/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 5      | train
--------------------------------------------------------
4         Trainable params
1         Non-trainable params
5         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=16
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=4/rs=2951
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=4/rs=2951/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=4/rs=2951 outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=4/rs=2951/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=4/rs=2951/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/agnews/size=4/rs=2951/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/agnews/size=4/rs=2951/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/agnews/size=4/rs=2951/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/agnews/size=4/rs=2951/test_label.csv --val_prop 0 --random_state 2951 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=4/rs=2951 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=4/rs=2951/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=4/rs=2951/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=4/rs=2951/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 5      | train
--------------------------------------------------------
5         Trainable params
0         Non-trainable params
5         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=16
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=4/rs=2951
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=4/rs=2951/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=4/rs=2951 outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=4/rs=2951/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=4/rs=2951/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/agnews/size=4/rs=2951/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/agnews/size=4/rs=2951/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/agnews/size=4/rs=2951/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/agnews/size=4/rs=2951/test_label.csv --val_prop 0 --random_state 2951 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=4/rs=2951 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=4/rs=2951/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=4/rs=2951/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=4/rs=2951/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 5      | train
--------------------------------------------------------
1         Trainable params
4         Non-trainable params
5         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=16
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=4/rs=2951
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=4/rs=2951/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=4/rs=2951 outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=4/rs=2951/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=4/rs=2951/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/agnews/size=4/rs=2951/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/agnews/size=4/rs=2951/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/agnews/size=4/rs=2951/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/agnews/size=4/rs=2951/test_label.csv --val_prop 0 --random_state 2951 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=4/rs=2951 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=4/rs=2951/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=4/rs=2951/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=4/rs=2951/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 5      | train
--------------------------------------------------------
4         Trainable params
1         Non-trainable params
5         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=16
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=4/rs=9622
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=4/rs=9622/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=4/rs=9622 outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=4/rs=9622/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=4/rs=9622/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/agnews/size=4/rs=9622/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/agnews/size=4/rs=9622/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/agnews/size=4/rs=9622/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/agnews/size=4/rs=9622/test_label.csv --val_prop 0 --random_state 9622 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=4/rs=9622 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=4/rs=9622/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=4/rs=9622/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=4/rs=9622/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 5      | train
--------------------------------------------------------
5         Trainable params
0         Non-trainable params
5         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=16
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=4/rs=9622
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=4/rs=9622/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=4/rs=9622 outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=4/rs=9622/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=4/rs=9622/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/agnews/size=4/rs=9622/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/agnews/size=4/rs=9622/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/agnews/size=4/rs=9622/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/agnews/size=4/rs=9622/test_label.csv --val_prop 0 --random_state 9622 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=4/rs=9622 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=4/rs=9622/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=4/rs=9622/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=4/rs=9622/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 5      | train
--------------------------------------------------------
1         Trainable params
4         Non-trainable params
5         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=16
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=4/rs=9622
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=4/rs=9622/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=4/rs=9622 outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=4/rs=9622/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=4/rs=9622/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/agnews/size=4/rs=9622/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/agnews/size=4/rs=9622/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/agnews/size=4/rs=9622/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/agnews/size=4/rs=9622/test_label.csv --val_prop 0 --random_state 9622 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=4/rs=9622 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=4/rs=9622/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=4/rs=9622/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=4/rs=9622/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 5      | train
--------------------------------------------------------
4         Trainable params
1         Non-trainable params
5         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for size in ${dataset2samples[$dataset]}
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=64
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=16/rs=738
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=16/rs=738/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=16/rs=738 outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=16/rs=738/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=16/rs=738/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/agnews/size=16/rs=738/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/agnews/size=16/rs=738/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/agnews/size=16/rs=738/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/agnews/size=16/rs=738/test_label.csv --val_prop 0 --random_state 738 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=16/rs=738 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=16/rs=738/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=16/rs=738/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=16/rs=738/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 5      | train
--------------------------------------------------------
5         Trainable params
0         Non-trainable params
5         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=64
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=16/rs=738
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=16/rs=738/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=16/rs=738 outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=16/rs=738/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=16/rs=738/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/agnews/size=16/rs=738/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/agnews/size=16/rs=738/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/agnews/size=16/rs=738/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/agnews/size=16/rs=738/test_label.csv --val_prop 0 --random_state 738 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=16/rs=738 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=16/rs=738/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=16/rs=738/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=16/rs=738/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 5      | train
--------------------------------------------------------
1         Trainable params
4         Non-trainable params
5         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=64
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=16/rs=738
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=16/rs=738/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=16/rs=738 outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=16/rs=738/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=16/rs=738/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/agnews/size=16/rs=738/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/agnews/size=16/rs=738/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/agnews/size=16/rs=738/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/agnews/size=16/rs=738/test_label.csv --val_prop 0 --random_state 738 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=16/rs=738 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=16/rs=738/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=16/rs=738/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=16/rs=738/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 5      | train
--------------------------------------------------------
4         Trainable params
1         Non-trainable params
5         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=64
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=16/rs=564
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=16/rs=564/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=16/rs=564 outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=16/rs=564/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=16/rs=564/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/agnews/size=16/rs=564/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/agnews/size=16/rs=564/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/agnews/size=16/rs=564/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/agnews/size=16/rs=564/test_label.csv --val_prop 0 --random_state 564 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=16/rs=564 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=16/rs=564/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=16/rs=564/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=16/rs=564/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 5      | train
--------------------------------------------------------
5         Trainable params
0         Non-trainable params
5         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=64
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=16/rs=564
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=16/rs=564/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=16/rs=564 outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=16/rs=564/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=16/rs=564/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/agnews/size=16/rs=564/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/agnews/size=16/rs=564/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/agnews/size=16/rs=564/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/agnews/size=16/rs=564/test_label.csv --val_prop 0 --random_state 564 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=16/rs=564 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=16/rs=564/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=16/rs=564/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=16/rs=564/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 5      | train
--------------------------------------------------------
1         Trainable params
4         Non-trainable params
5         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=64
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=16/rs=564
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=16/rs=564/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=16/rs=564 outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=16/rs=564/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=16/rs=564/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/agnews/size=16/rs=564/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/agnews/size=16/rs=564/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/agnews/size=16/rs=564/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/agnews/size=16/rs=564/test_label.csv --val_prop 0 --random_state 564 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=16/rs=564 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=16/rs=564/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=16/rs=564/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=16/rs=564/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 5      | train
--------------------------------------------------------
4         Trainable params
1         Non-trainable params
5         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=64
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=16/rs=783
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=16/rs=783/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=16/rs=783 outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=16/rs=783/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=16/rs=783/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/agnews/size=16/rs=783/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/agnews/size=16/rs=783/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/agnews/size=16/rs=783/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/agnews/size=16/rs=783/test_label.csv --val_prop 0 --random_state 783 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=16/rs=783 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=16/rs=783/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=16/rs=783/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=16/rs=783/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 5      | train
--------------------------------------------------------
5         Trainable params
0         Non-trainable params
5         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=64
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=16/rs=783
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=16/rs=783/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=16/rs=783 outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=16/rs=783/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=16/rs=783/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/agnews/size=16/rs=783/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/agnews/size=16/rs=783/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/agnews/size=16/rs=783/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/agnews/size=16/rs=783/test_label.csv --val_prop 0 --random_state 783 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=16/rs=783 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=16/rs=783/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=16/rs=783/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=16/rs=783/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 5      | train
--------------------------------------------------------
1         Trainable params
4         Non-trainable params
5         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=64
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=16/rs=783
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=16/rs=783/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=16/rs=783 outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=16/rs=783/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=16/rs=783/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/agnews/size=16/rs=783/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/agnews/size=16/rs=783/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/agnews/size=16/rs=783/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/agnews/size=16/rs=783/test_label.csv --val_prop 0 --random_state 783 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=16/rs=783 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=16/rs=783/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=16/rs=783/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=16/rs=783/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 5      | train
--------------------------------------------------------
4         Trainable params
1         Non-trainable params
5         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=64
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=16/rs=5641
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=16/rs=5641/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=16/rs=5641 outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=16/rs=5641/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=16/rs=5641/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/agnews/size=16/rs=5641/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/agnews/size=16/rs=5641/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/agnews/size=16/rs=5641/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/agnews/size=16/rs=5641/test_label.csv --val_prop 0 --random_state 5641 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=16/rs=5641 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=16/rs=5641/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=16/rs=5641/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=16/rs=5641/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 5      | train
--------------------------------------------------------
5         Trainable params
0         Non-trainable params
5         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=64
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=16/rs=5641
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=16/rs=5641/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=16/rs=5641 outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=16/rs=5641/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=16/rs=5641/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/agnews/size=16/rs=5641/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/agnews/size=16/rs=5641/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/agnews/size=16/rs=5641/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/agnews/size=16/rs=5641/test_label.csv --val_prop 0 --random_state 5641 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=16/rs=5641 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=16/rs=5641/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=16/rs=5641/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=16/rs=5641/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 5      | train
--------------------------------------------------------
1         Trainable params
4         Non-trainable params
5         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=64
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=16/rs=5641
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=16/rs=5641/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=16/rs=5641 outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=16/rs=5641/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=16/rs=5641/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/agnews/size=16/rs=5641/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/agnews/size=16/rs=5641/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/agnews/size=16/rs=5641/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/agnews/size=16/rs=5641/test_label.csv --val_prop 0 --random_state 5641 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=16/rs=5641 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=16/rs=5641/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=16/rs=5641/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=16/rs=5641/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 5      | train
--------------------------------------------------------
4         Trainable params
1         Non-trainable params
5         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=64
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=16/rs=7832
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=16/rs=7832/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=16/rs=7832 outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=16/rs=7832/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=16/rs=7832/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/agnews/size=16/rs=7832/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/agnews/size=16/rs=7832/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/agnews/size=16/rs=7832/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/agnews/size=16/rs=7832/test_label.csv --val_prop 0 --random_state 7832 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=16/rs=7832 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=16/rs=7832/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=16/rs=7832/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=16/rs=7832/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 5      | train
--------------------------------------------------------
5         Trainable params
0         Non-trainable params
5         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=64
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=16/rs=7832
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=16/rs=7832/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=16/rs=7832 outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=16/rs=7832/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=16/rs=7832/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/agnews/size=16/rs=7832/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/agnews/size=16/rs=7832/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/agnews/size=16/rs=7832/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/agnews/size=16/rs=7832/test_label.csv --val_prop 0 --random_state 7832 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=16/rs=7832 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=16/rs=7832/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=16/rs=7832/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=16/rs=7832/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 5      | train
--------------------------------------------------------
1         Trainable params
4         Non-trainable params
5         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=64
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=16/rs=7832
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=16/rs=7832/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=16/rs=7832 outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=16/rs=7832/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=16/rs=7832/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/agnews/size=16/rs=7832/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/agnews/size=16/rs=7832/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/agnews/size=16/rs=7832/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/agnews/size=16/rs=7832/test_label.csv --val_prop 0 --random_state 7832 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=16/rs=7832 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=16/rs=7832/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=16/rs=7832/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=16/rs=7832/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 5      | train
--------------------------------------------------------
4         Trainable params
1         Non-trainable params
5         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for size in ${dataset2samples[$dataset]}
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=1024
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=256/rs=493
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=256/rs=493/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=256/rs=493 outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=256/rs=493/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=256/rs=493/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/agnews/size=256/rs=493/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/agnews/size=256/rs=493/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/agnews/size=256/rs=493/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/agnews/size=256/rs=493/test_label.csv --val_prop 0 --random_state 493 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=256/rs=493 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=256/rs=493/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=256/rs=493/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=256/rs=493/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 5      | train
--------------------------------------------------------
5         Trainable params
0         Non-trainable params
5         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=1024
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=256/rs=493
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=256/rs=493/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=256/rs=493 outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=256/rs=493/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=256/rs=493/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/agnews/size=256/rs=493/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/agnews/size=256/rs=493/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/agnews/size=256/rs=493/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/agnews/size=256/rs=493/test_label.csv --val_prop 0 --random_state 493 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=256/rs=493 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=256/rs=493/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=256/rs=493/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=256/rs=493/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 5      | train
--------------------------------------------------------
1         Trainable params
4         Non-trainable params
5         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=1024
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=256/rs=493
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=256/rs=493/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=256/rs=493 outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=256/rs=493/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=256/rs=493/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/agnews/size=256/rs=493/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/agnews/size=256/rs=493/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/agnews/size=256/rs=493/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/agnews/size=256/rs=493/test_label.csv --val_prop 0 --random_state 493 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=256/rs=493 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=256/rs=493/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=256/rs=493/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=256/rs=493/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 5      | train
--------------------------------------------------------
4         Trainable params
1         Non-trainable params
5         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=1024
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=256/rs=821
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=256/rs=821/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=256/rs=821 outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=256/rs=821/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=256/rs=821/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/agnews/size=256/rs=821/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/agnews/size=256/rs=821/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/agnews/size=256/rs=821/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/agnews/size=256/rs=821/test_label.csv --val_prop 0 --random_state 821 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=256/rs=821 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=256/rs=821/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=256/rs=821/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=256/rs=821/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 5      | train
--------------------------------------------------------
5         Trainable params
0         Non-trainable params
5         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=1024
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=256/rs=821
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=256/rs=821/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=256/rs=821 outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=256/rs=821/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=256/rs=821/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/agnews/size=256/rs=821/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/agnews/size=256/rs=821/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/agnews/size=256/rs=821/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/agnews/size=256/rs=821/test_label.csv --val_prop 0 --random_state 821 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=256/rs=821 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=256/rs=821/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=256/rs=821/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=256/rs=821/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 5      | train
--------------------------------------------------------
1         Trainable params
4         Non-trainable params
5         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=1024
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=256/rs=821
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=256/rs=821/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=256/rs=821 outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=256/rs=821/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=256/rs=821/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/agnews/size=256/rs=821/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/agnews/size=256/rs=821/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/agnews/size=256/rs=821/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/agnews/size=256/rs=821/test_label.csv --val_prop 0 --random_state 821 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=256/rs=821 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=256/rs=821/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=256/rs=821/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=256/rs=821/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 5      | train
--------------------------------------------------------
4         Trainable params
1         Non-trainable params
5         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=1024
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=256/rs=812
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=256/rs=812/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=256/rs=812 outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=256/rs=812/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=256/rs=812/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/agnews/size=256/rs=812/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/agnews/size=256/rs=812/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/agnews/size=256/rs=812/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/agnews/size=256/rs=812/test_label.csv --val_prop 0 --random_state 812 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=256/rs=812 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=256/rs=812/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=256/rs=812/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=256/rs=812/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 5      | train
--------------------------------------------------------
5         Trainable params
0         Non-trainable params
5         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=1024
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=256/rs=812
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=256/rs=812/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=256/rs=812 outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=256/rs=812/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=256/rs=812/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/agnews/size=256/rs=812/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/agnews/size=256/rs=812/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/agnews/size=256/rs=812/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/agnews/size=256/rs=812/test_label.csv --val_prop 0 --random_state 812 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=256/rs=812 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=256/rs=812/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=256/rs=812/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=256/rs=812/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 5      | train
--------------------------------------------------------
1         Trainable params
4         Non-trainable params
5         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=1024
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=256/rs=812
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=256/rs=812/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=256/rs=812 outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=256/rs=812/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=256/rs=812/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/agnews/size=256/rs=812/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/agnews/size=256/rs=812/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/agnews/size=256/rs=812/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/agnews/size=256/rs=812/test_label.csv --val_prop 0 --random_state 812 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=256/rs=812 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=256/rs=812/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=256/rs=812/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=256/rs=812/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 5      | train
--------------------------------------------------------
4         Trainable params
1         Non-trainable params
5         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=1024
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=256/rs=4931
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=256/rs=4931/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=256/rs=4931 outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=256/rs=4931/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=256/rs=4931/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/agnews/size=256/rs=4931/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/agnews/size=256/rs=4931/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/agnews/size=256/rs=4931/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/agnews/size=256/rs=4931/test_label.csv --val_prop 0 --random_state 4931 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=256/rs=4931 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=256/rs=4931/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=256/rs=4931/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=256/rs=4931/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 5      | train
--------------------------------------------------------
5         Trainable params
0         Non-trainable params
5         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=1024
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=256/rs=4931
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=256/rs=4931/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=256/rs=4931 outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=256/rs=4931/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=256/rs=4931/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/agnews/size=256/rs=4931/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/agnews/size=256/rs=4931/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/agnews/size=256/rs=4931/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/agnews/size=256/rs=4931/test_label.csv --val_prop 0 --random_state 4931 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=256/rs=4931 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=256/rs=4931/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=256/rs=4931/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=256/rs=4931/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 5      | train
--------------------------------------------------------
1         Trainable params
4         Non-trainable params
5         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=1024
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=256/rs=4931
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=256/rs=4931/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=256/rs=4931 outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=256/rs=4931/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=256/rs=4931/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/agnews/size=256/rs=4931/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/agnews/size=256/rs=4931/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/agnews/size=256/rs=4931/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/agnews/size=256/rs=4931/test_label.csv --val_prop 0 --random_state 4931 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=256/rs=4931 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=256/rs=4931/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=256/rs=4931/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=256/rs=4931/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 5      | train
--------------------------------------------------------
4         Trainable params
1         Non-trainable params
5         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=1024
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=256/rs=8212
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=256/rs=8212/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=256/rs=8212 outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=256/rs=8212/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=256/rs=8212/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/agnews/size=256/rs=8212/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/agnews/size=256/rs=8212/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/agnews/size=256/rs=8212/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/agnews/size=256/rs=8212/test_label.csv --val_prop 0 --random_state 8212 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=256/rs=8212 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=256/rs=8212/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=256/rs=8212/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/agnews/size=256/rs=8212/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 5      | train
--------------------------------------------------------
5         Trainable params
0         Non-trainable params
5         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=1024
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=256/rs=8212
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=256/rs=8212/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=256/rs=8212 outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=256/rs=8212/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=256/rs=8212/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/agnews/size=256/rs=8212/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/agnews/size=256/rs=8212/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/agnews/size=256/rs=8212/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/agnews/size=256/rs=8212/test_label.csv --val_prop 0 --random_state 8212 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=256/rs=8212 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=256/rs=8212/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=256/rs=8212/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/agnews/size=256/rs=8212/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 5      | train
--------------------------------------------------------
1         Trainable params
4         Non-trainable params
5         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=1024
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=256/rs=8212
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=256/rs=8212/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=256/rs=8212 outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=256/rs=8212/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=256/rs=8212/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/agnews/size=256/rs=8212/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/agnews/size=256/rs=8212/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/agnews/size=256/rs=8212/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/agnews/size=256/rs=8212/test_label.csv --val_prop 0 --random_state 8212 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=256/rs=8212 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=256/rs=8212/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=256/rs=8212/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/agnews/size=256/rs=8212/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 5      | train
--------------------------------------------------------
4         Trainable params
1         Non-trainable params
5         Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for dataset in $DATASETS
+ for size in ${dataset2samples[$dataset]}
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=28
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=2/rs=435
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=2/rs=435/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=2/rs=435 outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=2/rs=435/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=2/rs=435/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=435/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=435/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=435/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=435/test_label.csv --val_prop 0 --random_state 435 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=2/rs=435 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=2/rs=435/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=2/rs=435/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=2/rs=435/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 15     | train
--------------------------------------------------------
15        Trainable params
0         Non-trainable params
15        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=28
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=2/rs=435
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=2/rs=435/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=2/rs=435 outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=2/rs=435/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=2/rs=435/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=435/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=435/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=435/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=435/test_label.csv --val_prop 0 --random_state 435 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=2/rs=435 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=2/rs=435/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=2/rs=435/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=2/rs=435/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 15     | train
--------------------------------------------------------
1         Trainable params
14        Non-trainable params
15        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=28
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=2/rs=435
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=2/rs=435/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=2/rs=435 outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=2/rs=435/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=2/rs=435/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=435/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=435/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=435/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=435/test_label.csv --val_prop 0 --random_state 435 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=2/rs=435 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=2/rs=435/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=2/rs=435/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=2/rs=435/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 15     | train
--------------------------------------------------------
14        Trainable params
1         Non-trainable params
15        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=28
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=2/rs=927
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=2/rs=927/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=2/rs=927 outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=2/rs=927/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=2/rs=927/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=927/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=927/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=927/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=927/test_label.csv --val_prop 0 --random_state 927 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=2/rs=927 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=2/rs=927/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=2/rs=927/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=2/rs=927/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 15     | train
--------------------------------------------------------
15        Trainable params
0         Non-trainable params
15        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=28
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=2/rs=927
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=2/rs=927/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=2/rs=927 outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=2/rs=927/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=2/rs=927/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=927/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=927/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=927/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=927/test_label.csv --val_prop 0 --random_state 927 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=2/rs=927 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=2/rs=927/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=2/rs=927/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=2/rs=927/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 15     | train
--------------------------------------------------------
1         Trainable params
14        Non-trainable params
15        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=28
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=2/rs=927
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=2/rs=927/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=2/rs=927 outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=2/rs=927/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=2/rs=927/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=927/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=927/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=927/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=927/test_label.csv --val_prop 0 --random_state 927 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=2/rs=927 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=2/rs=927/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=2/rs=927/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=2/rs=927/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 15     | train
--------------------------------------------------------
14        Trainable params
1         Non-trainable params
15        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=28
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=2/rs=972
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=2/rs=972/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=2/rs=972 outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=2/rs=972/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=2/rs=972/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=972/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=972/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=972/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=972/test_label.csv --val_prop 0 --random_state 972 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=2/rs=972 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=2/rs=972/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=2/rs=972/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=2/rs=972/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 15     | train
--------------------------------------------------------
15        Trainable params
0         Non-trainable params
15        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=28
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=2/rs=972
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=2/rs=972/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=2/rs=972 outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=2/rs=972/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=2/rs=972/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=972/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=972/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=972/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=972/test_label.csv --val_prop 0 --random_state 972 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=2/rs=972 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=2/rs=972/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=2/rs=972/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=2/rs=972/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 15     | train
--------------------------------------------------------
1         Trainable params
14        Non-trainable params
15        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=28
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=2/rs=972
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=2/rs=972/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=2/rs=972 outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=2/rs=972/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=2/rs=972/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=972/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=972/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=972/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=972/test_label.csv --val_prop 0 --random_state 972 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=2/rs=972 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=2/rs=972/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=2/rs=972/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=2/rs=972/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 15     | train
--------------------------------------------------------
14        Trainable params
1         Non-trainable params
15        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=28
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=2/rs=4351
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=2/rs=4351/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=2/rs=4351 outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=2/rs=4351/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=2/rs=4351/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=4351/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=4351/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=4351/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=4351/test_label.csv --val_prop 0 --random_state 4351 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=2/rs=4351 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=2/rs=4351/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=2/rs=4351/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=2/rs=4351/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 15     | train
--------------------------------------------------------
15        Trainable params
0         Non-trainable params
15        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=28
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=2/rs=4351
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=2/rs=4351/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=2/rs=4351 outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=2/rs=4351/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=2/rs=4351/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=4351/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=4351/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=4351/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=4351/test_label.csv --val_prop 0 --random_state 4351 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=2/rs=4351 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=2/rs=4351/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=2/rs=4351/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=2/rs=4351/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 15     | train
--------------------------------------------------------
1         Trainable params
14        Non-trainable params
15        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=28
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=2/rs=4351
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=2/rs=4351/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=2/rs=4351 outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=2/rs=4351/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=2/rs=4351/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=4351/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=4351/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=4351/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=4351/test_label.csv --val_prop 0 --random_state 4351 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=2/rs=4351 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=2/rs=4351/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=2/rs=4351/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=2/rs=4351/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 15     | train
--------------------------------------------------------
14        Trainable params
1         Non-trainable params
15        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=28
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=2/rs=9722
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=2/rs=9722/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=2/rs=9722 outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=2/rs=9722/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=2/rs=9722/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=9722/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=9722/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=9722/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=9722/test_label.csv --val_prop 0 --random_state 9722 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=2/rs=9722 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=2/rs=9722/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=2/rs=9722/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=2/rs=9722/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 15     | train
--------------------------------------------------------
15        Trainable params
0         Non-trainable params
15        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=28
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=2/rs=9722
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=2/rs=9722/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=2/rs=9722 outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=2/rs=9722/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=2/rs=9722/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=9722/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=9722/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=9722/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=9722/test_label.csv --val_prop 0 --random_state 9722 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=2/rs=9722 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=2/rs=9722/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=2/rs=9722/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=2/rs=9722/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 15     | train
--------------------------------------------------------
1         Trainable params
14        Non-trainable params
15        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=28
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=2/rs=9722
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=2/rs=9722/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=2/rs=9722 outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=2/rs=9722/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=2/rs=9722/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=9722/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=9722/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=9722/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/dbpedia/size=2/rs=9722/test_label.csv --val_prop 0 --random_state 9722 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=2/rs=9722 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=2/rs=9722/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=2/rs=9722/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=2/rs=9722/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 15     | train
--------------------------------------------------------
14        Trainable params
1         Non-trainable params
15        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for size in ${dataset2samples[$dataset]}
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=112
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=8/rs=338
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=8/rs=338/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=8/rs=338 outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=8/rs=338/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=8/rs=338/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=338/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=338/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=338/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=338/test_label.csv --val_prop 0 --random_state 338 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=8/rs=338 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=8/rs=338/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=8/rs=338/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=8/rs=338/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 15     | train
--------------------------------------------------------
15        Trainable params
0         Non-trainable params
15        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=112
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=8/rs=338
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=8/rs=338/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=8/rs=338 outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=8/rs=338/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=8/rs=338/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=338/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=338/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=338/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=338/test_label.csv --val_prop 0 --random_state 338 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=8/rs=338 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=8/rs=338/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=8/rs=338/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=8/rs=338/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 15     | train
--------------------------------------------------------
1         Trainable params
14        Non-trainable params
15        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=112
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=8/rs=338
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=8/rs=338/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=8/rs=338 outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=8/rs=338/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=8/rs=338/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=338/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=338/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=338/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=338/test_label.csv --val_prop 0 --random_state 338 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=8/rs=338 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=8/rs=338/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=8/rs=338/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=8/rs=338/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 15     | train
--------------------------------------------------------
14        Trainable params
1         Non-trainable params
15        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=112
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=8/rs=364
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=8/rs=364/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=8/rs=364 outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=8/rs=364/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=8/rs=364/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=364/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=364/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=364/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=364/test_label.csv --val_prop 0 --random_state 364 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=8/rs=364 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=8/rs=364/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=8/rs=364/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=8/rs=364/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 15     | train
--------------------------------------------------------
15        Trainable params
0         Non-trainable params
15        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=112
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=8/rs=364
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=8/rs=364/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=8/rs=364 outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=8/rs=364/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=8/rs=364/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=364/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=364/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=364/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=364/test_label.csv --val_prop 0 --random_state 364 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=8/rs=364 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=8/rs=364/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=8/rs=364/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=8/rs=364/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 15     | train
--------------------------------------------------------
1         Trainable params
14        Non-trainable params
15        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=112
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=8/rs=364
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=8/rs=364/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=8/rs=364 outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=8/rs=364/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=8/rs=364/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=364/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=364/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=364/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=364/test_label.csv --val_prop 0 --random_state 364 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=8/rs=364 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=8/rs=364/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=8/rs=364/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=8/rs=364/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 15     | train
--------------------------------------------------------
14        Trainable params
1         Non-trainable params
15        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=112
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=8/rs=383
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=8/rs=383/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=8/rs=383 outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=8/rs=383/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=8/rs=383/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=383/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=383/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=383/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=383/test_label.csv --val_prop 0 --random_state 383 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=8/rs=383 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=8/rs=383/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=8/rs=383/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=8/rs=383/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 15     | train
--------------------------------------------------------
15        Trainable params
0         Non-trainable params
15        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=112
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=8/rs=383
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=8/rs=383/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=8/rs=383 outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=8/rs=383/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=8/rs=383/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=383/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=383/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=383/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=383/test_label.csv --val_prop 0 --random_state 383 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=8/rs=383 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=8/rs=383/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=8/rs=383/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=8/rs=383/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 15     | train
--------------------------------------------------------
1         Trainable params
14        Non-trainable params
15        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=112
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=8/rs=383
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=8/rs=383/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=8/rs=383 outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=8/rs=383/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=8/rs=383/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=383/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=383/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=383/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=383/test_label.csv --val_prop 0 --random_state 383 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=8/rs=383 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=8/rs=383/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=8/rs=383/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=8/rs=383/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 15     | train
--------------------------------------------------------
14        Trainable params
1         Non-trainable params
15        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=112
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=8/rs=3641
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=8/rs=3641/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=8/rs=3641 outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=8/rs=3641/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=8/rs=3641/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=3641/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=3641/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=3641/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=3641/test_label.csv --val_prop 0 --random_state 3641 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=8/rs=3641 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=8/rs=3641/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=8/rs=3641/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=8/rs=3641/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 15     | train
--------------------------------------------------------
15        Trainable params
0         Non-trainable params
15        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=112
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=8/rs=3641
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=8/rs=3641/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=8/rs=3641 outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=8/rs=3641/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=8/rs=3641/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=3641/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=3641/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=3641/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=3641/test_label.csv --val_prop 0 --random_state 3641 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=8/rs=3641 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=8/rs=3641/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=8/rs=3641/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=8/rs=3641/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 15     | train
--------------------------------------------------------
1         Trainable params
14        Non-trainable params
15        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=112
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=8/rs=3641
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=8/rs=3641/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=8/rs=3641 outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=8/rs=3641/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=8/rs=3641/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=3641/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=3641/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=3641/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=3641/test_label.csv --val_prop 0 --random_state 3641 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=8/rs=3641 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=8/rs=3641/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=8/rs=3641/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=8/rs=3641/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 15     | train
--------------------------------------------------------
14        Trainable params
1         Non-trainable params
15        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=112
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=8/rs=3832
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=8/rs=3832/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=8/rs=3832 outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=8/rs=3832/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=8/rs=3832/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=3832/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=3832/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=3832/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=3832/test_label.csv --val_prop 0 --random_state 3832 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=8/rs=3832 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=8/rs=3832/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=8/rs=3832/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=8/rs=3832/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 15     | train
--------------------------------------------------------
15        Trainable params
0         Non-trainable params
15        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=112
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=8/rs=3832
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=8/rs=3832/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=8/rs=3832 outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=8/rs=3832/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=8/rs=3832/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=3832/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=3832/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=3832/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=3832/test_label.csv --val_prop 0 --random_state 3832 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=8/rs=3832 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=8/rs=3832/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=8/rs=3832/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=8/rs=3832/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 15     | train
--------------------------------------------------------
1         Trainable params
14        Non-trainable params
15        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=112
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=8/rs=3832
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=8/rs=3832/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=8/rs=3832 outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=8/rs=3832/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=8/rs=3832/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=3832/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=3832/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=3832/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/dbpedia/size=8/rs=3832/test_label.csv --val_prop 0 --random_state 3832 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=8/rs=3832 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=8/rs=3832/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=8/rs=3832/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=8/rs=3832/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 15     | train
--------------------------------------------------------
14        Trainable params
1         Non-trainable params
15        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for size in ${dataset2samples[$dataset]}
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=1792
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=128/rs=129
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=128/rs=129/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=128/rs=129 outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=128/rs=129/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=128/rs=129/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=129/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=129/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=129/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=129/test_label.csv --val_prop 0 --random_state 129 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=128/rs=129 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=128/rs=129/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=128/rs=129/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=128/rs=129/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 15     | train
--------------------------------------------------------
15        Trainable params
0         Non-trainable params
15        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=1792
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=128/rs=129
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=128/rs=129/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=128/rs=129 outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=128/rs=129/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=128/rs=129/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=129/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=129/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=129/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=129/test_label.csv --val_prop 0 --random_state 129 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=128/rs=129 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=128/rs=129/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=128/rs=129/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=128/rs=129/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 15     | train
--------------------------------------------------------
1         Trainable params
14        Non-trainable params
15        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=1792
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=128/rs=129
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=128/rs=129/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=128/rs=129 outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=128/rs=129/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=128/rs=129/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=129/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=129/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=129/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=129/test_label.csv --val_prop 0 --random_state 129 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=128/rs=129 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=128/rs=129/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=128/rs=129/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=128/rs=129/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 15     | train
--------------------------------------------------------
14        Trainable params
1         Non-trainable params
15        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=1792
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=128/rs=131
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=128/rs=131/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=128/rs=131 outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=128/rs=131/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=128/rs=131/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=131/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=131/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=131/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=131/test_label.csv --val_prop 0 --random_state 131 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=128/rs=131 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=128/rs=131/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=128/rs=131/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=128/rs=131/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 15     | train
--------------------------------------------------------
15        Trainable params
0         Non-trainable params
15        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=1792
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=128/rs=131
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=128/rs=131/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=128/rs=131 outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=128/rs=131/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=128/rs=131/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=131/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=131/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=131/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=131/test_label.csv --val_prop 0 --random_state 131 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=128/rs=131 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=128/rs=131/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=128/rs=131/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=128/rs=131/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 15     | train
--------------------------------------------------------
1         Trainable params
14        Non-trainable params
15        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=1792
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=128/rs=131
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=128/rs=131/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=128/rs=131 outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=128/rs=131/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=128/rs=131/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=131/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=131/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=131/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=131/test_label.csv --val_prop 0 --random_state 131 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=128/rs=131 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=128/rs=131/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=128/rs=131/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=128/rs=131/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 15     | train
--------------------------------------------------------
14        Trainable params
1         Non-trainable params
15        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=1792
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=128/rs=543
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=128/rs=543/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=128/rs=543 outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=128/rs=543/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=128/rs=543/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=543/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=543/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=543/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=543/test_label.csv --val_prop 0 --random_state 543 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=128/rs=543 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=128/rs=543/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=128/rs=543/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=128/rs=543/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 15     | train
--------------------------------------------------------
15        Trainable params
0         Non-trainable params
15        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=1792
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=128/rs=543
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=128/rs=543/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=128/rs=543 outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=128/rs=543/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=128/rs=543/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=543/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=543/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=543/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=543/test_label.csv --val_prop 0 --random_state 543 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=128/rs=543 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=128/rs=543/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=128/rs=543/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=128/rs=543/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 15     | train
--------------------------------------------------------
1         Trainable params
14        Non-trainable params
15        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=1792
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=128/rs=543
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=128/rs=543/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=128/rs=543 outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=128/rs=543/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=128/rs=543/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=543/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=543/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=543/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=543/test_label.csv --val_prop 0 --random_state 543 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=128/rs=543 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=128/rs=543/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=128/rs=543/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=128/rs=543/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 15     | train
--------------------------------------------------------
14        Trainable params
1         Non-trainable params
15        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=1792
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=128/rs=878
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=128/rs=878/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=128/rs=878 outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=128/rs=878/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=128/rs=878/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=878/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=878/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=878/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=878/test_label.csv --val_prop 0 --random_state 878 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=128/rs=878 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=128/rs=878/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=128/rs=878/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=128/rs=878/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 15     | train
--------------------------------------------------------
15        Trainable params
0         Non-trainable params
15        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=1792
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=128/rs=878
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=128/rs=878/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=128/rs=878 outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=128/rs=878/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=128/rs=878/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=878/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=878/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=878/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=878/test_label.csv --val_prop 0 --random_state 878 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=128/rs=878 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=128/rs=878/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=128/rs=878/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=128/rs=878/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 15     | train
--------------------------------------------------------
1         Trainable params
14        Non-trainable params
15        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=1792
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=128/rs=878
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=128/rs=878/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=128/rs=878 outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=128/rs=878/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=128/rs=878/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=878/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=878/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=878/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=878/test_label.csv --val_prop 0 --random_state 878 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=128/rs=878 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=128/rs=878/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=128/rs=878/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=128/rs=878/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 15     | train
--------------------------------------------------------
14        Trainable params
1         Non-trainable params
15        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=1792
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=128/rs=909
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=128/rs=909/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=128/rs=909 outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=128/rs=909/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=128/rs=909/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=909/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=909/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=909/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=909/test_label.csv --val_prop 0 --random_state 909 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=128/rs=909 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=128/rs=909/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=128/rs=909/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/dbpedia/size=128/rs=909/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 15     | train
--------------------------------------------------------
15        Trainable params
0         Non-trainable params
15        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=1792
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=128/rs=909
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=128/rs=909/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=128/rs=909 outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=128/rs=909/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=128/rs=909/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=909/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=909/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=909/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=909/test_label.csv --val_prop 0 --random_state 909 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=128/rs=909 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=128/rs=909/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=128/rs=909/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/dbpedia/size=128/rs=909/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 15     | train
--------------------------------------------------------
1         Trainable params
14        Non-trainable params
15        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=1792
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=128/rs=909
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=128/rs=909/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=128/rs=909 outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=128/rs=909/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=128/rs=909/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=909/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=909/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=909/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/dbpedia/size=128/rs=909/test_label.csv --val_prop 0 --random_state 909 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=128/rs=909 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=128/rs=909/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=128/rs=909/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/dbpedia/size=128/rs=909/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 15     | train
--------------------------------------------------------
14        Trainable params
1         Non-trainable params
15        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for dataset in $DATASETS
+ for size in ${dataset2samples[$dataset]}
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=40
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=2/rs=435
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=2/rs=435/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=2/rs=435 outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=2/rs=435/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=2/rs=435/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=435/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=435/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=435/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=435/test_label.csv --val_prop 0 --random_state 435 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=2/rs=435 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=2/rs=435/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=2/rs=435/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=2/rs=435/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 21     | train
--------------------------------------------------------
21        Trainable params
0         Non-trainable params
21        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=40
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=2/rs=435
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=2/rs=435/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=2/rs=435 outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=2/rs=435/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=2/rs=435/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=435/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=435/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=435/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=435/test_label.csv --val_prop 0 --random_state 435 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=2/rs=435 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=2/rs=435/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=2/rs=435/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=2/rs=435/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 21     | train
--------------------------------------------------------
1         Trainable params
20        Non-trainable params
21        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=40
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=2/rs=435
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=2/rs=435/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=2/rs=435 outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=2/rs=435/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=2/rs=435/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=435/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=435/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=435/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=435/test_label.csv --val_prop 0 --random_state 435 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=2/rs=435 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=2/rs=435/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=2/rs=435/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=2/rs=435/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 21     | train
--------------------------------------------------------
20        Trainable params
1         Non-trainable params
21        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=40
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=2/rs=927
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=2/rs=927/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=2/rs=927 outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=2/rs=927/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=2/rs=927/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=927/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=927/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=927/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=927/test_label.csv --val_prop 0 --random_state 927 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=2/rs=927 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=2/rs=927/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=2/rs=927/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=2/rs=927/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 21     | train
--------------------------------------------------------
21        Trainable params
0         Non-trainable params
21        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=40
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=2/rs=927
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=2/rs=927/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=2/rs=927 outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=2/rs=927/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=2/rs=927/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=927/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=927/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=927/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=927/test_label.csv --val_prop 0 --random_state 927 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=2/rs=927 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=2/rs=927/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=2/rs=927/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=2/rs=927/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 21     | train
--------------------------------------------------------
1         Trainable params
20        Non-trainable params
21        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=40
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=2/rs=927
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=2/rs=927/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=2/rs=927 outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=2/rs=927/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=2/rs=927/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=927/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=927/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=927/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=927/test_label.csv --val_prop 0 --random_state 927 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=2/rs=927 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=2/rs=927/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=2/rs=927/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=2/rs=927/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 21     | train
--------------------------------------------------------
20        Trainable params
1         Non-trainable params
21        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=40
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=2/rs=972
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=2/rs=972/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=2/rs=972 outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=2/rs=972/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=2/rs=972/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=972/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=972/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=972/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=972/test_label.csv --val_prop 0 --random_state 972 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=2/rs=972 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=2/rs=972/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=2/rs=972/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=2/rs=972/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 21     | train
--------------------------------------------------------
21        Trainable params
0         Non-trainable params
21        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=40
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=2/rs=972
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=2/rs=972/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=2/rs=972 outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=2/rs=972/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=2/rs=972/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=972/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=972/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=972/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=972/test_label.csv --val_prop 0 --random_state 972 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=2/rs=972 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=2/rs=972/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=2/rs=972/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=2/rs=972/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 21     | train
--------------------------------------------------------
1         Trainable params
20        Non-trainable params
21        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=40
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=2/rs=972
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=2/rs=972/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=2/rs=972 outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=2/rs=972/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=2/rs=972/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=972/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=972/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=972/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=972/test_label.csv --val_prop 0 --random_state 972 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=2/rs=972 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=2/rs=972/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=2/rs=972/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=2/rs=972/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 21     | train
--------------------------------------------------------
20        Trainable params
1         Non-trainable params
21        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=40
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=2/rs=4351
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=2/rs=4351/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=2/rs=4351 outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=2/rs=4351/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=2/rs=4351/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=4351/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=4351/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=4351/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=4351/test_label.csv --val_prop 0 --random_state 4351 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=2/rs=4351 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=2/rs=4351/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=2/rs=4351/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=2/rs=4351/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 21     | train
--------------------------------------------------------
21        Trainable params
0         Non-trainable params
21        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=40
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=2/rs=4351
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=2/rs=4351/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=2/rs=4351 outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=2/rs=4351/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=2/rs=4351/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=4351/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=4351/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=4351/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=4351/test_label.csv --val_prop 0 --random_state 4351 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=2/rs=4351 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=2/rs=4351/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=2/rs=4351/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=2/rs=4351/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 21     | train
--------------------------------------------------------
1         Trainable params
20        Non-trainable params
21        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=40
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=2/rs=4351
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=2/rs=4351/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=2/rs=4351 outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=2/rs=4351/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=2/rs=4351/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=4351/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=4351/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=4351/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=4351/test_label.csv --val_prop 0 --random_state 4351 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=2/rs=4351 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=2/rs=4351/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=2/rs=4351/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=2/rs=4351/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 21     | train
--------------------------------------------------------
20        Trainable params
1         Non-trainable params
21        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=40
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=2/rs=9722
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=2/rs=9722/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=2/rs=9722 outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=2/rs=9722/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=2/rs=9722/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=9722/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=9722/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=9722/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=9722/test_label.csv --val_prop 0 --random_state 9722 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=2/rs=9722 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=2/rs=9722/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=2/rs=9722/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=2/rs=9722/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 21     | train
--------------------------------------------------------
21        Trainable params
0         Non-trainable params
21        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=40
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=2/rs=9722
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=2/rs=9722/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=2/rs=9722 outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=2/rs=9722/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=2/rs=9722/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=9722/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=9722/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=9722/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=9722/test_label.csv --val_prop 0 --random_state 9722 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=2/rs=9722 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=2/rs=9722/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=2/rs=9722/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=2/rs=9722/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 21     | train
--------------------------------------------------------
1         Trainable params
20        Non-trainable params
21        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=40
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=2/rs=9722
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=2/rs=9722/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=2/rs=9722 outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=2/rs=9722/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=2/rs=9722/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=9722/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=9722/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=9722/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=2/rs=9722/test_label.csv --val_prop 0 --random_state 9722 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=2/rs=9722 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=2/rs=9722/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=2/rs=9722/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=2/rs=9722/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 21     | train
--------------------------------------------------------
20        Trainable params
1         Non-trainable params
21        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for size in ${dataset2samples[$dataset]}
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=160
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=8/rs=338
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=8/rs=338/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=8/rs=338 outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=8/rs=338/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=8/rs=338/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=338/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=338/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=338/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=338/test_label.csv --val_prop 0 --random_state 338 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=8/rs=338 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=8/rs=338/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=8/rs=338/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=8/rs=338/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 21     | train
--------------------------------------------------------
21        Trainable params
0         Non-trainable params
21        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=160
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=8/rs=338
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=8/rs=338/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=8/rs=338 outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=8/rs=338/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=8/rs=338/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=338/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=338/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=338/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=338/test_label.csv --val_prop 0 --random_state 338 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=8/rs=338 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=8/rs=338/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=8/rs=338/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=8/rs=338/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 21     | train
--------------------------------------------------------
1         Trainable params
20        Non-trainable params
21        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=160
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=8/rs=338
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=8/rs=338/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=8/rs=338 outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=8/rs=338/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=8/rs=338/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=338/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=338/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=338/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=338/test_label.csv --val_prop 0 --random_state 338 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=8/rs=338 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=8/rs=338/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=8/rs=338/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=8/rs=338/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 21     | train
--------------------------------------------------------
20        Trainable params
1         Non-trainable params
21        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=160
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=8/rs=364
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=8/rs=364/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=8/rs=364 outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=8/rs=364/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=8/rs=364/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=364/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=364/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=364/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=364/test_label.csv --val_prop 0 --random_state 364 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=8/rs=364 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=8/rs=364/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=8/rs=364/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=8/rs=364/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 21     | train
--------------------------------------------------------
21        Trainable params
0         Non-trainable params
21        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=160
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=8/rs=364
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=8/rs=364/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=8/rs=364 outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=8/rs=364/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=8/rs=364/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=364/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=364/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=364/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=364/test_label.csv --val_prop 0 --random_state 364 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=8/rs=364 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=8/rs=364/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=8/rs=364/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=8/rs=364/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 21     | train
--------------------------------------------------------
1         Trainable params
20        Non-trainable params
21        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=160
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=8/rs=364
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=8/rs=364/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=8/rs=364 outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=8/rs=364/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=8/rs=364/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=364/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=364/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=364/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=364/test_label.csv --val_prop 0 --random_state 364 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=8/rs=364 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=8/rs=364/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=8/rs=364/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=8/rs=364/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 21     | train
--------------------------------------------------------
20        Trainable params
1         Non-trainable params
21        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=160
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=8/rs=383
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=8/rs=383/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=8/rs=383 outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=8/rs=383/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=8/rs=383/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=383/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=383/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=383/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=383/test_label.csv --val_prop 0 --random_state 383 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=8/rs=383 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=8/rs=383/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=8/rs=383/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=8/rs=383/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 21     | train
--------------------------------------------------------
21        Trainable params
0         Non-trainable params
21        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=160
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=8/rs=383
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=8/rs=383/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=8/rs=383 outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=8/rs=383/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=8/rs=383/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=383/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=383/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=383/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=383/test_label.csv --val_prop 0 --random_state 383 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=8/rs=383 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=8/rs=383/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=8/rs=383/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=8/rs=383/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 21     | train
--------------------------------------------------------
1         Trainable params
20        Non-trainable params
21        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=160
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=8/rs=383
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=8/rs=383/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=8/rs=383 outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=8/rs=383/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=8/rs=383/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=383/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=383/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=383/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=383/test_label.csv --val_prop 0 --random_state 383 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=8/rs=383 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=8/rs=383/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=8/rs=383/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=8/rs=383/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 21     | train
--------------------------------------------------------
20        Trainable params
1         Non-trainable params
21        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=160
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=8/rs=3641
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=8/rs=3641/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=8/rs=3641 outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=8/rs=3641/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=8/rs=3641/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=3641/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=3641/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=3641/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=3641/test_label.csv --val_prop 0 --random_state 3641 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=8/rs=3641 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=8/rs=3641/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=8/rs=3641/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=8/rs=3641/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 21     | train
--------------------------------------------------------
21        Trainable params
0         Non-trainable params
21        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=160
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=8/rs=3641
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=8/rs=3641/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=8/rs=3641 outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=8/rs=3641/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=8/rs=3641/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=3641/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=3641/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=3641/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=3641/test_label.csv --val_prop 0 --random_state 3641 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=8/rs=3641 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=8/rs=3641/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=8/rs=3641/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=8/rs=3641/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 21     | train
--------------------------------------------------------
1         Trainable params
20        Non-trainable params
21        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=160
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=8/rs=3641
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=8/rs=3641/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=8/rs=3641 outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=8/rs=3641/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=8/rs=3641/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=3641/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=3641/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=3641/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=3641/test_label.csv --val_prop 0 --random_state 3641 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=8/rs=3641 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=8/rs=3641/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=8/rs=3641/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=8/rs=3641/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 21     | train
--------------------------------------------------------
20        Trainable params
1         Non-trainable params
21        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=160
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=8/rs=3832
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=8/rs=3832/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=8/rs=3832 outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=8/rs=3832/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=8/rs=3832/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=3832/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=3832/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=3832/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=3832/test_label.csv --val_prop 0 --random_state 3832 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=8/rs=3832 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=8/rs=3832/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=8/rs=3832/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=8/rs=3832/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 21     | train
--------------------------------------------------------
21        Trainable params
0         Non-trainable params
21        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=160
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=8/rs=3832
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=8/rs=3832/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=8/rs=3832 outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=8/rs=3832/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=8/rs=3832/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=3832/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=3832/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=3832/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=3832/test_label.csv --val_prop 0 --random_state 3832 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=8/rs=3832 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=8/rs=3832/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=8/rs=3832/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=8/rs=3832/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 21     | train
--------------------------------------------------------
1         Trainable params
20        Non-trainable params
21        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=160
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=8/rs=3832
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=8/rs=3832/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=8/rs=3832 outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=8/rs=3832/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=8/rs=3832/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=3832/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=3832/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=3832/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=8/rs=3832/test_label.csv --val_prop 0 --random_state 3832 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=8/rs=3832 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=8/rs=3832/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=8/rs=3832/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=8/rs=3832/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 21     | train
--------------------------------------------------------
20        Trainable params
1         Non-trainable params
21        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for size in ${dataset2samples[$dataset]}
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=2560
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=128/rs=129
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=128/rs=129/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=128/rs=129 outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=128/rs=129/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=128/rs=129/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=129/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=129/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=129/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=129/test_label.csv --val_prop 0 --random_state 129 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=128/rs=129 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=128/rs=129/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=128/rs=129/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=128/rs=129/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 21     | train
--------------------------------------------------------
21        Trainable params
0         Non-trainable params
21        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=2560
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=128/rs=129
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=128/rs=129/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=128/rs=129 outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=128/rs=129/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=128/rs=129/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=129/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=129/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=129/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=129/test_label.csv --val_prop 0 --random_state 129 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=128/rs=129 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=128/rs=129/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=128/rs=129/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=128/rs=129/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 21     | train
--------------------------------------------------------
1         Trainable params
20        Non-trainable params
21        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=2560
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=128/rs=129
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=128/rs=129/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=128/rs=129 outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=128/rs=129/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=128/rs=129/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=129/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=129/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=129/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=129/test_label.csv --val_prop 0 --random_state 129 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=128/rs=129 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=128/rs=129/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=128/rs=129/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=128/rs=129/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 21     | train
--------------------------------------------------------
20        Trainable params
1         Non-trainable params
21        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=2560
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=128/rs=131
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=128/rs=131/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=128/rs=131 outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=128/rs=131/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=128/rs=131/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=131/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=131/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=131/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=131/test_label.csv --val_prop 0 --random_state 131 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=128/rs=131 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=128/rs=131/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=128/rs=131/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=128/rs=131/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 21     | train
--------------------------------------------------------
21        Trainable params
0         Non-trainable params
21        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=2560
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=128/rs=131
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=128/rs=131/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=128/rs=131 outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=128/rs=131/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=128/rs=131/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=131/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=131/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=131/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=131/test_label.csv --val_prop 0 --random_state 131 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=128/rs=131 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=128/rs=131/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=128/rs=131/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=128/rs=131/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 21     | train
--------------------------------------------------------
1         Trainable params
20        Non-trainable params
21        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=2560
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=128/rs=131
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=128/rs=131/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=128/rs=131 outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=128/rs=131/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=128/rs=131/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=131/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=131/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=131/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=131/test_label.csv --val_prop 0 --random_state 131 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=128/rs=131 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=128/rs=131/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=128/rs=131/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=128/rs=131/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 21     | train
--------------------------------------------------------
20        Trainable params
1         Non-trainable params
21        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=2560
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=128/rs=543
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=128/rs=543/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=128/rs=543 outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=128/rs=543/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=128/rs=543/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=543/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=543/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=543/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=543/test_label.csv --val_prop 0 --random_state 543 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=128/rs=543 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=128/rs=543/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=128/rs=543/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=128/rs=543/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 21     | train
--------------------------------------------------------
21        Trainable params
0         Non-trainable params
21        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=2560
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=128/rs=543
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=128/rs=543/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=128/rs=543 outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=128/rs=543/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=128/rs=543/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=543/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=543/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=543/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=543/test_label.csv --val_prop 0 --random_state 543 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=128/rs=543 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=128/rs=543/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=128/rs=543/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=128/rs=543/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 21     | train
--------------------------------------------------------
1         Trainable params
20        Non-trainable params
21        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=2560
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=128/rs=543
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=128/rs=543/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=128/rs=543 outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=128/rs=543/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=128/rs=543/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=543/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=543/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=543/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=543/test_label.csv --val_prop 0 --random_state 543 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=128/rs=543 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=128/rs=543/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=128/rs=543/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=128/rs=543/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 21     | train
--------------------------------------------------------
20        Trainable params
1         Non-trainable params
21        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=2560
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=128/rs=878
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=128/rs=878/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=128/rs=878 outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=128/rs=878/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=128/rs=878/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=878/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=878/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=878/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=878/test_label.csv --val_prop 0 --random_state 878 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=128/rs=878 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=128/rs=878/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=128/rs=878/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=128/rs=878/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 21     | train
--------------------------------------------------------
21        Trainable params
0         Non-trainable params
21        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=2560
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=128/rs=878
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=128/rs=878/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=128/rs=878 outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=128/rs=878/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=128/rs=878/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=878/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=878/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=878/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=878/test_label.csv --val_prop 0 --random_state 878 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=128/rs=878 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=128/rs=878/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=128/rs=878/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=128/rs=878/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 21     | train
--------------------------------------------------------
1         Trainable params
20        Non-trainable params
21        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=2560
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=128/rs=878
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=128/rs=878/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=128/rs=878 outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=128/rs=878/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=128/rs=878/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=878/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=878/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=878/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=878/test_label.csv --val_prop 0 --random_state 878 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=128/rs=878 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=128/rs=878/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=128/rs=878/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=128/rs=878/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 21     | train
--------------------------------------------------------
20        Trainable params
1         Non-trainable params
21        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=2560
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=128/rs=909
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=128/rs=909/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=128/rs=909 outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=128/rs=909/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=128/rs=909/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=909/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=909/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=909/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=909/test_label.csv --val_prop 0 --random_state 909 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=128/rs=909 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=128/rs=909/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=128/rs=909/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/20newsgroups/size=128/rs=909/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 21     | train
--------------------------------------------------------
21        Trainable params
0         Non-trainable params
21        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=2560
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=128/rs=909
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=128/rs=909/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=128/rs=909 outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=128/rs=909/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=128/rs=909/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=909/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=909/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=909/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=909/test_label.csv --val_prop 0 --random_state 909 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=128/rs=909 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=128/rs=909/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=128/rs=909/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/20newsgroups/size=128/rs=909/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 21     | train
--------------------------------------------------------
1         Trainable params
20        Non-trainable params
21        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=2560
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=128/rs=909
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=128/rs=909/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=128/rs=909 outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=128/rs=909/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=128/rs=909/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=909/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=909/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=909/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/20newsgroups/size=128/rs=909/test_label.csv --val_prop 0 --random_state 909 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=128/rs=909 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=128/rs=909/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=128/rs=909/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/20newsgroups/size=128/rs=909/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 21     | train
--------------------------------------------------------
20        Trainable params
1         Non-trainable params
21        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for dataset in $DATASETS
+ for size in ${dataset2samples[$dataset]}
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=77
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=1/rs=322
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=1/rs=322/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=1/rs=322 outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=1/rs=322/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=1/rs=322/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/banking77/size=1/rs=322/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/banking77/size=1/rs=322/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/banking77/size=1/rs=322/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/banking77/size=1/rs=322/test_label.csv --val_prop 0 --random_state 322 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=1/rs=322 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=1/rs=322/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=1/rs=322/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=1/rs=322/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 78     | train
--------------------------------------------------------
78        Trainable params
0         Non-trainable params
78        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=77
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=1/rs=322
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=1/rs=322/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=1/rs=322 outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=1/rs=322/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=1/rs=322/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/banking77/size=1/rs=322/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/banking77/size=1/rs=322/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/banking77/size=1/rs=322/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/banking77/size=1/rs=322/test_label.csv --val_prop 0 --random_state 322 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=1/rs=322 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=1/rs=322/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=1/rs=322/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=1/rs=322/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 78     | train
--------------------------------------------------------
1         Trainable params
77        Non-trainable params
78        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=77
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=1/rs=322
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=1/rs=322/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=1/rs=322 outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=1/rs=322/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=1/rs=322/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/banking77/size=1/rs=322/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/banking77/size=1/rs=322/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/banking77/size=1/rs=322/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/banking77/size=1/rs=322/test_label.csv --val_prop 0 --random_state 322 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=1/rs=322 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=1/rs=322/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=1/rs=322/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=1/rs=322/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 78     | train
--------------------------------------------------------
77        Trainable params
1         Non-trainable params
78        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=77
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=1/rs=444
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=1/rs=444/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=1/rs=444 outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=1/rs=444/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=1/rs=444/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/banking77/size=1/rs=444/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/banking77/size=1/rs=444/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/banking77/size=1/rs=444/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/banking77/size=1/rs=444/test_label.csv --val_prop 0 --random_state 444 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=1/rs=444 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=1/rs=444/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=1/rs=444/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=1/rs=444/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 78     | train
--------------------------------------------------------
78        Trainable params
0         Non-trainable params
78        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=77
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=1/rs=444
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=1/rs=444/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=1/rs=444 outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=1/rs=444/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=1/rs=444/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/banking77/size=1/rs=444/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/banking77/size=1/rs=444/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/banking77/size=1/rs=444/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/banking77/size=1/rs=444/test_label.csv --val_prop 0 --random_state 444 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=1/rs=444 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=1/rs=444/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=1/rs=444/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=1/rs=444/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 78     | train
--------------------------------------------------------
1         Trainable params
77        Non-trainable params
78        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=77
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=1/rs=444
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=1/rs=444/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=1/rs=444 outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=1/rs=444/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=1/rs=444/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/banking77/size=1/rs=444/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/banking77/size=1/rs=444/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/banking77/size=1/rs=444/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/banking77/size=1/rs=444/test_label.csv --val_prop 0 --random_state 444 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=1/rs=444 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=1/rs=444/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=1/rs=444/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=1/rs=444/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 78     | train
--------------------------------------------------------
77        Trainable params
1         Non-trainable params
78        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=77
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=1/rs=848
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=1/rs=848/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=1/rs=848 outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=1/rs=848/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=1/rs=848/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/banking77/size=1/rs=848/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/banking77/size=1/rs=848/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/banking77/size=1/rs=848/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/banking77/size=1/rs=848/test_label.csv --val_prop 0 --random_state 848 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=1/rs=848 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=1/rs=848/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=1/rs=848/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=1/rs=848/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 78     | train
--------------------------------------------------------
78        Trainable params
0         Non-trainable params
78        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=77
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=1/rs=848
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=1/rs=848/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=1/rs=848 outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=1/rs=848/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=1/rs=848/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/banking77/size=1/rs=848/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/banking77/size=1/rs=848/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/banking77/size=1/rs=848/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/banking77/size=1/rs=848/test_label.csv --val_prop 0 --random_state 848 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=1/rs=848 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=1/rs=848/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=1/rs=848/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=1/rs=848/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 78     | train
--------------------------------------------------------
1         Trainable params
77        Non-trainable params
78        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=77
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=1/rs=848
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=1/rs=848/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=1/rs=848 outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=1/rs=848/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=1/rs=848/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/banking77/size=1/rs=848/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/banking77/size=1/rs=848/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/banking77/size=1/rs=848/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/banking77/size=1/rs=848/test_label.csv --val_prop 0 --random_state 848 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=1/rs=848 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=1/rs=848/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=1/rs=848/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=1/rs=848/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 78     | train
--------------------------------------------------------
77        Trainable params
1         Non-trainable params
78        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=77
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=1/rs=858
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=1/rs=858/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=1/rs=858 outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=1/rs=858/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=1/rs=858/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/banking77/size=1/rs=858/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/banking77/size=1/rs=858/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/banking77/size=1/rs=858/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/banking77/size=1/rs=858/test_label.csv --val_prop 0 --random_state 858 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=1/rs=858 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=1/rs=858/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=1/rs=858/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=1/rs=858/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 78     | train
--------------------------------------------------------
78        Trainable params
0         Non-trainable params
78        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=77
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=1/rs=858
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=1/rs=858/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=1/rs=858 outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=1/rs=858/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=1/rs=858/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/banking77/size=1/rs=858/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/banking77/size=1/rs=858/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/banking77/size=1/rs=858/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/banking77/size=1/rs=858/test_label.csv --val_prop 0 --random_state 858 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=1/rs=858 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=1/rs=858/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=1/rs=858/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=1/rs=858/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 78     | train
--------------------------------------------------------
1         Trainable params
77        Non-trainable params
78        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=77
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=1/rs=858
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=1/rs=858/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=1/rs=858 outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=1/rs=858/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=1/rs=858/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/banking77/size=1/rs=858/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/banking77/size=1/rs=858/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/banking77/size=1/rs=858/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/banking77/size=1/rs=858/test_label.csv --val_prop 0 --random_state 858 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=1/rs=858 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=1/rs=858/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=1/rs=858/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=1/rs=858/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 78     | train
--------------------------------------------------------
77        Trainable params
1         Non-trainable params
78        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=77
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=1/rs=868
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=1/rs=868/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=1/rs=868 outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=1/rs=868/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=1/rs=868/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/banking77/size=1/rs=868/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/banking77/size=1/rs=868/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/banking77/size=1/rs=868/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/banking77/size=1/rs=868/test_label.csv --val_prop 0 --random_state 868 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=1/rs=868 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=1/rs=868/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=1/rs=868/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=1/rs=868/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 78     | train
--------------------------------------------------------
78        Trainable params
0         Non-trainable params
78        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=77
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=1/rs=868
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=1/rs=868/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=1/rs=868 outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=1/rs=868/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=1/rs=868/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/banking77/size=1/rs=868/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/banking77/size=1/rs=868/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/banking77/size=1/rs=868/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/banking77/size=1/rs=868/test_label.csv --val_prop 0 --random_state 868 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=1/rs=868 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=1/rs=868/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=1/rs=868/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=1/rs=868/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 78     | train
--------------------------------------------------------
1         Trainable params
77        Non-trainable params
78        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=77
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=1/rs=868
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=1/rs=868/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=1/rs=868 outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=1/rs=868/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=1/rs=868/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/banking77/size=1/rs=868/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/banking77/size=1/rs=868/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/banking77/size=1/rs=868/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/banking77/size=1/rs=868/test_label.csv --val_prop 0 --random_state 868 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=1/rs=868 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=1/rs=868/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=1/rs=868/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=1/rs=868/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 78     | train
--------------------------------------------------------
77        Trainable params
1         Non-trainable params
78        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for size in ${dataset2samples[$dataset]}
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=308
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=4/rs=295
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=4/rs=295/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=4/rs=295 outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=4/rs=295/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=4/rs=295/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/banking77/size=4/rs=295/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/banking77/size=4/rs=295/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/banking77/size=4/rs=295/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/banking77/size=4/rs=295/test_label.csv --val_prop 0 --random_state 295 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=4/rs=295 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=4/rs=295/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=4/rs=295/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=4/rs=295/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 78     | train
--------------------------------------------------------
78        Trainable params
0         Non-trainable params
78        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=308
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=4/rs=295
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=4/rs=295/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=4/rs=295 outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=4/rs=295/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=4/rs=295/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/banking77/size=4/rs=295/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/banking77/size=4/rs=295/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/banking77/size=4/rs=295/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/banking77/size=4/rs=295/test_label.csv --val_prop 0 --random_state 295 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=4/rs=295 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=4/rs=295/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=4/rs=295/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=4/rs=295/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 78     | train
--------------------------------------------------------
1         Trainable params
77        Non-trainable params
78        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=308
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=4/rs=295
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=4/rs=295/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=4/rs=295 outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=4/rs=295/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=4/rs=295/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/banking77/size=4/rs=295/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/banking77/size=4/rs=295/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/banking77/size=4/rs=295/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/banking77/size=4/rs=295/test_label.csv --val_prop 0 --random_state 295 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=4/rs=295 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=4/rs=295/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=4/rs=295/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=4/rs=295/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 78     | train
--------------------------------------------------------
77        Trainable params
1         Non-trainable params
78        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=308
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=4/rs=926
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=4/rs=926/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=4/rs=926 outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=4/rs=926/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=4/rs=926/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/banking77/size=4/rs=926/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/banking77/size=4/rs=926/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/banking77/size=4/rs=926/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/banking77/size=4/rs=926/test_label.csv --val_prop 0 --random_state 926 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=4/rs=926 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=4/rs=926/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=4/rs=926/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=4/rs=926/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 78     | train
--------------------------------------------------------
78        Trainable params
0         Non-trainable params
78        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=308
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=4/rs=926
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=4/rs=926/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=4/rs=926 outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=4/rs=926/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=4/rs=926/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/banking77/size=4/rs=926/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/banking77/size=4/rs=926/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/banking77/size=4/rs=926/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/banking77/size=4/rs=926/test_label.csv --val_prop 0 --random_state 926 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=4/rs=926 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=4/rs=926/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=4/rs=926/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=4/rs=926/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 78     | train
--------------------------------------------------------
1         Trainable params
77        Non-trainable params
78        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=308
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=4/rs=926
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=4/rs=926/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=4/rs=926 outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=4/rs=926/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=4/rs=926/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/banking77/size=4/rs=926/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/banking77/size=4/rs=926/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/banking77/size=4/rs=926/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/banking77/size=4/rs=926/test_label.csv --val_prop 0 --random_state 926 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=4/rs=926 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=4/rs=926/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=4/rs=926/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=4/rs=926/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 78     | train
--------------------------------------------------------
77        Trainable params
1         Non-trainable params
78        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=308
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=4/rs=962
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=4/rs=962/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=4/rs=962 outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=4/rs=962/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=4/rs=962/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/banking77/size=4/rs=962/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/banking77/size=4/rs=962/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/banking77/size=4/rs=962/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/banking77/size=4/rs=962/test_label.csv --val_prop 0 --random_state 962 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=4/rs=962 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=4/rs=962/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=4/rs=962/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=4/rs=962/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 78     | train
--------------------------------------------------------
78        Trainable params
0         Non-trainable params
78        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=308
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=4/rs=962
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=4/rs=962/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=4/rs=962 outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=4/rs=962/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=4/rs=962/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/banking77/size=4/rs=962/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/banking77/size=4/rs=962/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/banking77/size=4/rs=962/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/banking77/size=4/rs=962/test_label.csv --val_prop 0 --random_state 962 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=4/rs=962 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=4/rs=962/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=4/rs=962/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=4/rs=962/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 78     | train
--------------------------------------------------------
1         Trainable params
77        Non-trainable params
78        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=308
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=4/rs=962
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=4/rs=962/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=4/rs=962 outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=4/rs=962/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=4/rs=962/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/banking77/size=4/rs=962/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/banking77/size=4/rs=962/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/banking77/size=4/rs=962/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/banking77/size=4/rs=962/test_label.csv --val_prop 0 --random_state 962 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=4/rs=962 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=4/rs=962/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=4/rs=962/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=4/rs=962/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 78     | train
--------------------------------------------------------
77        Trainable params
1         Non-trainable params
78        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=308
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=4/rs=2951
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=4/rs=2951/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=4/rs=2951 outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=4/rs=2951/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=4/rs=2951/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/banking77/size=4/rs=2951/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/banking77/size=4/rs=2951/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/banking77/size=4/rs=2951/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/banking77/size=4/rs=2951/test_label.csv --val_prop 0 --random_state 2951 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=4/rs=2951 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=4/rs=2951/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=4/rs=2951/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=4/rs=2951/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 78     | train
--------------------------------------------------------
78        Trainable params
0         Non-trainable params
78        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=308
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=4/rs=2951
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=4/rs=2951/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=4/rs=2951 outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=4/rs=2951/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=4/rs=2951/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/banking77/size=4/rs=2951/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/banking77/size=4/rs=2951/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/banking77/size=4/rs=2951/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/banking77/size=4/rs=2951/test_label.csv --val_prop 0 --random_state 2951 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=4/rs=2951 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=4/rs=2951/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=4/rs=2951/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=4/rs=2951/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 78     | train
--------------------------------------------------------
1         Trainable params
77        Non-trainable params
78        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=308
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=4/rs=2951
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=4/rs=2951/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=4/rs=2951 outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=4/rs=2951/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=4/rs=2951/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/banking77/size=4/rs=2951/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/banking77/size=4/rs=2951/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/banking77/size=4/rs=2951/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/banking77/size=4/rs=2951/test_label.csv --val_prop 0 --random_state 2951 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=4/rs=2951 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=4/rs=2951/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=4/rs=2951/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=4/rs=2951/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 78     | train
--------------------------------------------------------
77        Trainable params
1         Non-trainable params
78        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=308
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=4/rs=9622
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=4/rs=9622/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=4/rs=9622 outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=4/rs=9622/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=4/rs=9622/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/banking77/size=4/rs=9622/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/banking77/size=4/rs=9622/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/banking77/size=4/rs=9622/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/banking77/size=4/rs=9622/test_label.csv --val_prop 0 --random_state 9622 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=4/rs=9622 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=4/rs=9622/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=4/rs=9622/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=4/rs=9622/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 78     | train
--------------------------------------------------------
78        Trainable params
0         Non-trainable params
78        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=308
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=4/rs=9622
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=4/rs=9622/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=4/rs=9622 outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=4/rs=9622/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=4/rs=9622/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/banking77/size=4/rs=9622/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/banking77/size=4/rs=9622/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/banking77/size=4/rs=9622/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/banking77/size=4/rs=9622/test_label.csv --val_prop 0 --random_state 9622 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=4/rs=9622 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=4/rs=9622/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=4/rs=9622/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=4/rs=9622/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 78     | train
--------------------------------------------------------
1         Trainable params
77        Non-trainable params
78        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=308
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=4/rs=9622
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=4/rs=9622/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=4/rs=9622 outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=4/rs=9622/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=4/rs=9622/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/banking77/size=4/rs=9622/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/banking77/size=4/rs=9622/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/banking77/size=4/rs=9622/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/banking77/size=4/rs=9622/test_label.csv --val_prop 0 --random_state 9622 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=4/rs=9622 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=4/rs=9622/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=4/rs=9622/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=4/rs=9622/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 78     | train
--------------------------------------------------------
77        Trainable params
1         Non-trainable params
78        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for size in ${dataset2samples[$dataset]}
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=4928
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=64/rs=131
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=64/rs=131/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=64/rs=131 outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=64/rs=131/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=64/rs=131/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/banking77/size=64/rs=131/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/banking77/size=64/rs=131/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/banking77/size=64/rs=131/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/banking77/size=64/rs=131/test_label.csv --val_prop 0 --random_state 131 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=64/rs=131 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=64/rs=131/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=64/rs=131/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=64/rs=131/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 78     | train
--------------------------------------------------------
78        Trainable params
0         Non-trainable params
78        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=4928
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=64/rs=131
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=64/rs=131/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=64/rs=131 outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=64/rs=131/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=64/rs=131/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/banking77/size=64/rs=131/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/banking77/size=64/rs=131/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/banking77/size=64/rs=131/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/banking77/size=64/rs=131/test_label.csv --val_prop 0 --random_state 131 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=64/rs=131 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=64/rs=131/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=64/rs=131/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=64/rs=131/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 78     | train
--------------------------------------------------------
1         Trainable params
77        Non-trainable params
78        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=4928
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=64/rs=131
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=64/rs=131/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=64/rs=131 outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=64/rs=131/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=64/rs=131/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/banking77/size=64/rs=131/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/banking77/size=64/rs=131/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/banking77/size=64/rs=131/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/banking77/size=64/rs=131/test_label.csv --val_prop 0 --random_state 131 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=64/rs=131 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=64/rs=131/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=64/rs=131/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=64/rs=131/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 78     | train
--------------------------------------------------------
77        Trainable params
1         Non-trainable params
78        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=4928
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=64/rs=888
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=64/rs=888/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=64/rs=888 outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=64/rs=888/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=64/rs=888/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/banking77/size=64/rs=888/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/banking77/size=64/rs=888/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/banking77/size=64/rs=888/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/banking77/size=64/rs=888/test_label.csv --val_prop 0 --random_state 888 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=64/rs=888 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=64/rs=888/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=64/rs=888/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=64/rs=888/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 78     | train
--------------------------------------------------------
78        Trainable params
0         Non-trainable params
78        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=4928
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=64/rs=888
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=64/rs=888/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=64/rs=888 outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=64/rs=888/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=64/rs=888/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/banking77/size=64/rs=888/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/banking77/size=64/rs=888/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/banking77/size=64/rs=888/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/banking77/size=64/rs=888/test_label.csv --val_prop 0 --random_state 888 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=64/rs=888 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=64/rs=888/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=64/rs=888/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=64/rs=888/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 78     | train
--------------------------------------------------------
1         Trainable params
77        Non-trainable params
78        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=4928
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=64/rs=888
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=64/rs=888/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=64/rs=888 outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=64/rs=888/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=64/rs=888/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/banking77/size=64/rs=888/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/banking77/size=64/rs=888/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/banking77/size=64/rs=888/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/banking77/size=64/rs=888/test_label.csv --val_prop 0 --random_state 888 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=64/rs=888 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=64/rs=888/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=64/rs=888/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=64/rs=888/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 78     | train
--------------------------------------------------------
77        Trainable params
1         Non-trainable params
78        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=4928
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=64/rs=893
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=64/rs=893/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=64/rs=893 outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=64/rs=893/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=64/rs=893/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/banking77/size=64/rs=893/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/banking77/size=64/rs=893/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/banking77/size=64/rs=893/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/banking77/size=64/rs=893/test_label.csv --val_prop 0 --random_state 893 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=64/rs=893 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=64/rs=893/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=64/rs=893/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=64/rs=893/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 78     | train
--------------------------------------------------------
78        Trainable params
0         Non-trainable params
78        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=4928
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=64/rs=893
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=64/rs=893/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=64/rs=893 outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=64/rs=893/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=64/rs=893/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/banking77/size=64/rs=893/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/banking77/size=64/rs=893/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/banking77/size=64/rs=893/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/banking77/size=64/rs=893/test_label.csv --val_prop 0 --random_state 893 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=64/rs=893 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=64/rs=893/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=64/rs=893/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=64/rs=893/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 78     | train
--------------------------------------------------------
1         Trainable params
77        Non-trainable params
78        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=4928
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=64/rs=893
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=64/rs=893/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=64/rs=893 outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=64/rs=893/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=64/rs=893/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/banking77/size=64/rs=893/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/banking77/size=64/rs=893/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/banking77/size=64/rs=893/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/banking77/size=64/rs=893/test_label.csv --val_prop 0 --random_state 893 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=64/rs=893 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=64/rs=893/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=64/rs=893/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=64/rs=893/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 78     | train
--------------------------------------------------------
77        Trainable params
1         Non-trainable params
78        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=4928
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=64/rs=912
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=64/rs=912/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=64/rs=912 outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=64/rs=912/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=64/rs=912/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/banking77/size=64/rs=912/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/banking77/size=64/rs=912/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/banking77/size=64/rs=912/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/banking77/size=64/rs=912/test_label.csv --val_prop 0 --random_state 912 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=64/rs=912 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=64/rs=912/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=64/rs=912/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=64/rs=912/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 78     | train
--------------------------------------------------------
78        Trainable params
0         Non-trainable params
78        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=4928
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=64/rs=912
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=64/rs=912/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=64/rs=912 outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=64/rs=912/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=64/rs=912/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/banking77/size=64/rs=912/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/banking77/size=64/rs=912/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/banking77/size=64/rs=912/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/banking77/size=64/rs=912/test_label.csv --val_prop 0 --random_state 912 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=64/rs=912 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=64/rs=912/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=64/rs=912/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=64/rs=912/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 78     | train
--------------------------------------------------------
1         Trainable params
77        Non-trainable params
78        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=4928
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=64/rs=912
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=64/rs=912/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=64/rs=912 outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=64/rs=912/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=64/rs=912/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/banking77/size=64/rs=912/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/banking77/size=64/rs=912/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/banking77/size=64/rs=912/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/banking77/size=64/rs=912/test_label.csv --val_prop 0 --random_state 912 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=64/rs=912 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=64/rs=912/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=64/rs=912/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=64/rs=912/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 78     | train
--------------------------------------------------------
77        Trainable params
1         Non-trainable params
78        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ for method in $methods
+ total_train_samples=4928
+ output_dir=outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=64/rs=933
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=64/rs=933/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=64/rs=933 outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=64/rs=933/checkpoint outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=64/rs=933/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/banking77/size=64/rs=933/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/banking77/size=64/rs=933/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/banking77/size=64/rs=933/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/banking77/size=64/rs=933/test_label.csv --val_prop 0 --random_state 933 --method dp_calibration --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=64/rs=933 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=64/rs=933/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=64/rs=933/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_dp_calibration_no_es/banking77/size=64/rs=933/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 78     | train
--------------------------------------------------------
78        Trainable params
0         Non-trainable params
78        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=4928
+ output_dir=outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=64/rs=933
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=64/rs=933/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=64/rs=933 outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=64/rs=933/checkpoint outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=64/rs=933/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/banking77/size=64/rs=933/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/banking77/size=64/rs=933/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/banking77/size=64/rs=933/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/banking77/size=64/rs=933/test_label.csv --val_prop 0 --random_state 933 --method temp_scaling --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=64/rs=933 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=64/rs=933/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=64/rs=933/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_temp_scaling_no_es/banking77/size=64/rs=933/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 78     | train
--------------------------------------------------------
1         Trainable params
77        Non-trainable params
78        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ for method in $methods
+ total_train_samples=4928
+ output_dir=outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=64/rs=933
+ '[' '!' -f outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=64/rs=933/train_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=64/rs=933 outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=64/rs=933/checkpoint outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=64/rs=933/logs
+ python -m llmcal2.scripts.affine_calibration --train_logits outputs/adaptation/tinyllama/lora/banking77/size=64/rs=933/val_logits.csv --train_labels outputs/adaptation/tinyllama/lora/banking77/size=64/rs=933/val_label.csv --test_logits outputs/adaptation/tinyllama/lora/banking77/size=64/rs=933/test_logits.csv --test_labels outputs/adaptation/tinyllama/lora/banking77/size=64/rs=933/test_label.csv --val_prop 0 --random_state 933 --method bias_only --max_ls 40 --learning_rate 1e-2 --max_epochs 30 --accelerator cpu --output_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=64/rs=933 --checkpoint_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=64/rs=933/checkpoint --log_dir outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=64/rs=933/logs
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_plus_bias_only_no_es/banking77/size=64/rs=933/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!

  | Name       | Type             | Params | Mode 
--------------------------------------------------------
0 | calibrator | AffineCalibrator | 78     | train
--------------------------------------------------------
77        Trainable params
1         Non-trainable params
78        Total params
0.000     Total estimated model params size (MB)
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=30` reached.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
+ ./scripts/lora_no_es.sh
+ CUDA_VISIBLE_DEVICES=0
+ accelerator=gpu
+ precision=bf16-true
+ strategy=auto
+ devices=1
+ num_nodes=1
+ batch_size=1
+ lora_args='--lora_r=8 --lora_alpha=16 --lora_dropout=0.05 --lora_query --lora_key --lora_value --lora_projection --lora_mlp --lora_head'
+ optimizer=adamw
+ weight_decay=0.0
+ max_steps=-1
+ model2checkpoint=(['tinyllama']='/mnt/extra/lestienne/lit-checkpoints/TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T')
+ declare -A model2checkpoint
+ DATASETS='sst2 agnews dbpedia 20newsgroups banking77'
+ dataset2numclasses=(['sst2']='2' ['agnews']='4' ['dbpedia']='14' ['20newsgroups']='20' ['banking77']='77')
+ declare -A dataset2numclasses
+ dataset2samples=(['sst2']='8 32 512' ['agnews']='4 16 256' ['dbpedia']='2 8 128' ['20newsgroups']='2 8 128' ['banking77']='1 4 64')
+ declare -A dataset2samples
+ dataset2seed=(['sst2_8']='639 923 932 6391 9322' ['sst2_32']='1564 1738 1783 15641 17832' ['sst2_512']='111 121 767 890 999' ['agnews_4']='295 926 962 2951 9622' ['agnews_16']='738 564 783 5641 7832' ['agnews_256']='493 821 812 4931 8212' ['dbpedia_2']='435 927 972 4351 9722' ['dbpedia_8']='338 364 383 3641 3832' ['dbpedia_128']='129 131 543 878 909' ['20newsgroups_2']='435 927 972 4351 9722' ['20newsgroups_8']='338 364 383 3641 3832' ['20newsgroups_128']='129 131 543 878 909' ['banking77_1']='322 444 848 858 868' ['banking77_4']='295 926 962 2951 9622' ['banking77_64']='131 888 912 933')
+ declare -A dataset2seed
+ dataset2hparams=(['sst2_8']='--learning_rate=0.00001 --max_epochs=400 --accumulate_grad_batches=8' ['sst2_32']='--learning_rate=0.0001 --max_epochs=250 --accumulate_grad_batches=16' ['sst2_512']='--learning_rate=0.0001 --max_epochs=20 --accumulate_grad_batches=32' ['agnews_4']='--learning_rate=0.00001 --max_epochs=400 --accumulate_grad_batches=8' ['agnews_16']='--learning_rate=0.0001 --max_epochs=100 --accumulate_grad_batches=16' ['agnews_256']='--learning_rate=0.0001 --max_epochs=20 --accumulate_grad_batches=32' ['dbpedia_2']='--learning_rate=0.0001 --max_epochs=200 --accumulate_grad_batches=16' ['dbpedia_8']='--learning_rate=0.0001 --max_epochs=100 --accumulate_grad_batches=16' ['dbpedia_128']='--learning_rate=0.0001 --max_epochs=20 --accumulate_grad_batches=32' ['20newsgroups_2']='--learning_rate=0.0001 --max_epochs=250 --accumulate_grad_batches=16' ['20newsgroups_8']='--learning_rate=0.0001 --max_epochs=75 --accumulate_grad_batches=32' ['20newsgroups_128']='--learning_rate=0.0001 --max_epochs=20 --accumulate_grad_batches=32' ['banking77_1']='--learning_rate=0.0001 --max_epochs=100 --accumulate_grad_batches=16' ['banking77_4']='--learning_rate=0.0001 --max_epochs=75 --accumulate_grad_batches=32' ['banking77_64']='--learning_rate=0.0001 --max_epochs=15 --accumulate_grad_batches=32')
+ declare -A dataset2hparams
+ for model in ${!model2checkpoint[@]}
+ checkpoint=/mnt/extra/lestienne/lit-checkpoints/TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T
+ for dataset in $DATASETS
+ for size in ${dataset2samples[$dataset]}
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ total_train_samples=16
+ output_dir=outputs/adaptation/tinyllama/lora_no_es/sst2/size=8/rs=639
+ '[' '!' -f outputs/adaptation/tinyllama/lora_no_es/sst2/size=8/rs=639/test_logits.csv ']'
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ total_train_samples=16
+ output_dir=outputs/adaptation/tinyllama/lora_no_es/sst2/size=8/rs=923
+ '[' '!' -f outputs/adaptation/tinyllama/lora_no_es/sst2/size=8/rs=923/test_logits.csv ']'
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ total_train_samples=16
+ output_dir=outputs/adaptation/tinyllama/lora_no_es/sst2/size=8/rs=932
+ '[' '!' -f outputs/adaptation/tinyllama/lora_no_es/sst2/size=8/rs=932/test_logits.csv ']'
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ total_train_samples=16
+ output_dir=outputs/adaptation/tinyllama/lora_no_es/sst2/size=8/rs=6391
+ '[' '!' -f outputs/adaptation/tinyllama/lora_no_es/sst2/size=8/rs=6391/test_logits.csv ']'
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ total_train_samples=16
+ output_dir=outputs/adaptation/tinyllama/lora_no_es/sst2/size=8/rs=9322
+ '[' '!' -f outputs/adaptation/tinyllama/lora_no_es/sst2/size=8/rs=9322/test_logits.csv ']'
+ for size in ${dataset2samples[$dataset]}
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ total_train_samples=64
+ output_dir=outputs/adaptation/tinyllama/lora_no_es/sst2/size=32/rs=1564
+ '[' '!' -f outputs/adaptation/tinyllama/lora_no_es/sst2/size=32/rs=1564/test_logits.csv ']'
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ total_train_samples=64
+ output_dir=outputs/adaptation/tinyllama/lora_no_es/sst2/size=32/rs=1738
+ '[' '!' -f outputs/adaptation/tinyllama/lora_no_es/sst2/size=32/rs=1738/test_logits.csv ']'
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ total_train_samples=64
+ output_dir=outputs/adaptation/tinyllama/lora_no_es/sst2/size=32/rs=1783
+ '[' '!' -f outputs/adaptation/tinyllama/lora_no_es/sst2/size=32/rs=1783/test_logits.csv ']'
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ total_train_samples=64
+ output_dir=outputs/adaptation/tinyllama/lora_no_es/sst2/size=32/rs=15641
+ '[' '!' -f outputs/adaptation/tinyllama/lora_no_es/sst2/size=32/rs=15641/test_logits.csv ']'
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ total_train_samples=64
+ output_dir=outputs/adaptation/tinyllama/lora_no_es/sst2/size=32/rs=17832
+ '[' '!' -f outputs/adaptation/tinyllama/lora_no_es/sst2/size=32/rs=17832/test_logits.csv ']'
+ for size in ${dataset2samples[$dataset]}
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ total_train_samples=1024
+ output_dir=outputs/adaptation/tinyllama/lora_no_es/sst2/size=512/rs=111
+ '[' '!' -f outputs/adaptation/tinyllama/lora_no_es/sst2/size=512/rs=111/test_logits.csv ']'
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ total_train_samples=1024
+ output_dir=outputs/adaptation/tinyllama/lora_no_es/sst2/size=512/rs=121
+ '[' '!' -f outputs/adaptation/tinyllama/lora_no_es/sst2/size=512/rs=121/test_logits.csv ']'
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ total_train_samples=1024
+ output_dir=outputs/adaptation/tinyllama/lora_no_es/sst2/size=512/rs=767
+ '[' '!' -f outputs/adaptation/tinyllama/lora_no_es/sst2/size=512/rs=767/test_logits.csv ']'
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ total_train_samples=1024
+ output_dir=outputs/adaptation/tinyllama/lora_no_es/sst2/size=512/rs=890
+ '[' '!' -f outputs/adaptation/tinyllama/lora_no_es/sst2/size=512/rs=890/test_logits.csv ']'
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ total_train_samples=1024
+ output_dir=outputs/adaptation/tinyllama/lora_no_es/sst2/size=512/rs=999
+ '[' '!' -f outputs/adaptation/tinyllama/lora_no_es/sst2/size=512/rs=999/test_logits.csv ']'
+ for dataset in $DATASETS
+ for size in ${dataset2samples[$dataset]}
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ total_train_samples=16
+ output_dir=outputs/adaptation/tinyllama/lora_no_es/agnews/size=4/rs=295
+ '[' '!' -f outputs/adaptation/tinyllama/lora_no_es/agnews/size=4/rs=295/test_logits.csv ']'
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ total_train_samples=16
+ output_dir=outputs/adaptation/tinyllama/lora_no_es/agnews/size=4/rs=926
+ '[' '!' -f outputs/adaptation/tinyllama/lora_no_es/agnews/size=4/rs=926/test_logits.csv ']'
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ total_train_samples=16
+ output_dir=outputs/adaptation/tinyllama/lora_no_es/agnews/size=4/rs=962
+ '[' '!' -f outputs/adaptation/tinyllama/lora_no_es/agnews/size=4/rs=962/test_logits.csv ']'
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ total_train_samples=16
+ output_dir=outputs/adaptation/tinyllama/lora_no_es/agnews/size=4/rs=2951
+ '[' '!' -f outputs/adaptation/tinyllama/lora_no_es/agnews/size=4/rs=2951/test_logits.csv ']'
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ total_train_samples=16
+ output_dir=outputs/adaptation/tinyllama/lora_no_es/agnews/size=4/rs=9622
+ '[' '!' -f outputs/adaptation/tinyllama/lora_no_es/agnews/size=4/rs=9622/test_logits.csv ']'
+ for size in ${dataset2samples[$dataset]}
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ total_train_samples=64
+ output_dir=outputs/adaptation/tinyllama/lora_no_es/agnews/size=16/rs=738
+ '[' '!' -f outputs/adaptation/tinyllama/lora_no_es/agnews/size=16/rs=738/test_logits.csv ']'
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ total_train_samples=64
+ output_dir=outputs/adaptation/tinyllama/lora_no_es/agnews/size=16/rs=564
+ '[' '!' -f outputs/adaptation/tinyllama/lora_no_es/agnews/size=16/rs=564/test_logits.csv ']'
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ total_train_samples=64
+ output_dir=outputs/adaptation/tinyllama/lora_no_es/agnews/size=16/rs=783
+ '[' '!' -f outputs/adaptation/tinyllama/lora_no_es/agnews/size=16/rs=783/test_logits.csv ']'
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ total_train_samples=64
+ output_dir=outputs/adaptation/tinyllama/lora_no_es/agnews/size=16/rs=5641
+ '[' '!' -f outputs/adaptation/tinyllama/lora_no_es/agnews/size=16/rs=5641/test_logits.csv ']'
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ total_train_samples=64
+ output_dir=outputs/adaptation/tinyllama/lora_no_es/agnews/size=16/rs=7832
+ '[' '!' -f outputs/adaptation/tinyllama/lora_no_es/agnews/size=16/rs=7832/test_logits.csv ']'
+ for size in ${dataset2samples[$dataset]}
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ total_train_samples=1024
+ output_dir=outputs/adaptation/tinyllama/lora_no_es/agnews/size=256/rs=493
+ '[' '!' -f outputs/adaptation/tinyllama/lora_no_es/agnews/size=256/rs=493/test_logits.csv ']'
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ total_train_samples=1024
+ output_dir=outputs/adaptation/tinyllama/lora_no_es/agnews/size=256/rs=821
+ '[' '!' -f outputs/adaptation/tinyllama/lora_no_es/agnews/size=256/rs=821/test_logits.csv ']'
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ total_train_samples=1024
+ output_dir=outputs/adaptation/tinyllama/lora_no_es/agnews/size=256/rs=812
+ '[' '!' -f outputs/adaptation/tinyllama/lora_no_es/agnews/size=256/rs=812/test_logits.csv ']'
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ total_train_samples=1024
+ output_dir=outputs/adaptation/tinyllama/lora_no_es/agnews/size=256/rs=4931
+ '[' '!' -f outputs/adaptation/tinyllama/lora_no_es/agnews/size=256/rs=4931/test_logits.csv ']'
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ total_train_samples=1024
+ output_dir=outputs/adaptation/tinyllama/lora_no_es/agnews/size=256/rs=8212
+ '[' '!' -f outputs/adaptation/tinyllama/lora_no_es/agnews/size=256/rs=8212/test_logits.csv ']'
+ for dataset in $DATASETS
+ for size in ${dataset2samples[$dataset]}
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ total_train_samples=28
+ output_dir=outputs/adaptation/tinyllama/lora_no_es/dbpedia/size=2/rs=435
+ '[' '!' -f outputs/adaptation/tinyllama/lora_no_es/dbpedia/size=2/rs=435/test_logits.csv ']'
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ total_train_samples=28
+ output_dir=outputs/adaptation/tinyllama/lora_no_es/dbpedia/size=2/rs=927
+ '[' '!' -f outputs/adaptation/tinyllama/lora_no_es/dbpedia/size=2/rs=927/test_logits.csv ']'
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ total_train_samples=28
+ output_dir=outputs/adaptation/tinyllama/lora_no_es/dbpedia/size=2/rs=972
+ '[' '!' -f outputs/adaptation/tinyllama/lora_no_es/dbpedia/size=2/rs=972/test_logits.csv ']'
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ total_train_samples=28
+ output_dir=outputs/adaptation/tinyllama/lora_no_es/dbpedia/size=2/rs=4351
+ '[' '!' -f outputs/adaptation/tinyllama/lora_no_es/dbpedia/size=2/rs=4351/test_logits.csv ']'
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ total_train_samples=28
+ output_dir=outputs/adaptation/tinyllama/lora_no_es/dbpedia/size=2/rs=9722
+ '[' '!' -f outputs/adaptation/tinyllama/lora_no_es/dbpedia/size=2/rs=9722/test_logits.csv ']'
+ for size in ${dataset2samples[$dataset]}
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ total_train_samples=112
+ output_dir=outputs/adaptation/tinyllama/lora_no_es/dbpedia/size=8/rs=338
+ '[' '!' -f outputs/adaptation/tinyllama/lora_no_es/dbpedia/size=8/rs=338/test_logits.csv ']'
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ total_train_samples=112
+ output_dir=outputs/adaptation/tinyllama/lora_no_es/dbpedia/size=8/rs=364
+ '[' '!' -f outputs/adaptation/tinyllama/lora_no_es/dbpedia/size=8/rs=364/test_logits.csv ']'
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ total_train_samples=112
+ output_dir=outputs/adaptation/tinyllama/lora_no_es/dbpedia/size=8/rs=383
+ '[' '!' -f outputs/adaptation/tinyllama/lora_no_es/dbpedia/size=8/rs=383/test_logits.csv ']'
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ total_train_samples=112
+ output_dir=outputs/adaptation/tinyllama/lora_no_es/dbpedia/size=8/rs=3641
+ '[' '!' -f outputs/adaptation/tinyllama/lora_no_es/dbpedia/size=8/rs=3641/test_logits.csv ']'
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ total_train_samples=112
+ output_dir=outputs/adaptation/tinyllama/lora_no_es/dbpedia/size=8/rs=3832
+ '[' '!' -f outputs/adaptation/tinyllama/lora_no_es/dbpedia/size=8/rs=3832/test_logits.csv ']'
+ for size in ${dataset2samples[$dataset]}
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ total_train_samples=1792
+ output_dir=outputs/adaptation/tinyllama/lora_no_es/dbpedia/size=128/rs=129
+ '[' '!' -f outputs/adaptation/tinyllama/lora_no_es/dbpedia/size=128/rs=129/test_logits.csv ']'
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ total_train_samples=1792
+ output_dir=outputs/adaptation/tinyllama/lora_no_es/dbpedia/size=128/rs=131
+ '[' '!' -f outputs/adaptation/tinyllama/lora_no_es/dbpedia/size=128/rs=131/test_logits.csv ']'
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ total_train_samples=1792
+ output_dir=outputs/adaptation/tinyllama/lora_no_es/dbpedia/size=128/rs=543
+ '[' '!' -f outputs/adaptation/tinyllama/lora_no_es/dbpedia/size=128/rs=543/test_logits.csv ']'
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ total_train_samples=1792
+ output_dir=outputs/adaptation/tinyllama/lora_no_es/dbpedia/size=128/rs=878
+ '[' '!' -f outputs/adaptation/tinyllama/lora_no_es/dbpedia/size=128/rs=878/test_logits.csv ']'
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ total_train_samples=1792
+ output_dir=outputs/adaptation/tinyllama/lora_no_es/dbpedia/size=128/rs=909
+ '[' '!' -f outputs/adaptation/tinyllama/lora_no_es/dbpedia/size=128/rs=909/test_logits.csv ']'
+ for dataset in $DATASETS
+ for size in ${dataset2samples[$dataset]}
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ total_train_samples=40
+ output_dir=outputs/adaptation/tinyllama/lora_no_es/20newsgroups/size=2/rs=435
+ '[' '!' -f outputs/adaptation/tinyllama/lora_no_es/20newsgroups/size=2/rs=435/test_logits.csv ']'
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ total_train_samples=40
+ output_dir=outputs/adaptation/tinyllama/lora_no_es/20newsgroups/size=2/rs=927
+ '[' '!' -f outputs/adaptation/tinyllama/lora_no_es/20newsgroups/size=2/rs=927/test_logits.csv ']'
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ total_train_samples=40
+ output_dir=outputs/adaptation/tinyllama/lora_no_es/20newsgroups/size=2/rs=972
+ '[' '!' -f outputs/adaptation/tinyllama/lora_no_es/20newsgroups/size=2/rs=972/test_logits.csv ']'
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ total_train_samples=40
+ output_dir=outputs/adaptation/tinyllama/lora_no_es/20newsgroups/size=2/rs=4351
+ '[' '!' -f outputs/adaptation/tinyllama/lora_no_es/20newsgroups/size=2/rs=4351/test_logits.csv ']'
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ total_train_samples=40
+ output_dir=outputs/adaptation/tinyllama/lora_no_es/20newsgroups/size=2/rs=9722
+ '[' '!' -f outputs/adaptation/tinyllama/lora_no_es/20newsgroups/size=2/rs=9722/test_logits.csv ']'
+ for size in ${dataset2samples[$dataset]}
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ total_train_samples=160
+ output_dir=outputs/adaptation/tinyllama/lora_no_es/20newsgroups/size=8/rs=338
+ '[' '!' -f outputs/adaptation/tinyllama/lora_no_es/20newsgroups/size=8/rs=338/test_logits.csv ']'
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ total_train_samples=160
+ output_dir=outputs/adaptation/tinyllama/lora_no_es/20newsgroups/size=8/rs=364
+ '[' '!' -f outputs/adaptation/tinyllama/lora_no_es/20newsgroups/size=8/rs=364/test_logits.csv ']'
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ total_train_samples=160
+ output_dir=outputs/adaptation/tinyllama/lora_no_es/20newsgroups/size=8/rs=383
+ '[' '!' -f outputs/adaptation/tinyllama/lora_no_es/20newsgroups/size=8/rs=383/test_logits.csv ']'
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ total_train_samples=160
+ output_dir=outputs/adaptation/tinyllama/lora_no_es/20newsgroups/size=8/rs=3641
+ '[' '!' -f outputs/adaptation/tinyllama/lora_no_es/20newsgroups/size=8/rs=3641/test_logits.csv ']'
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ total_train_samples=160
+ output_dir=outputs/adaptation/tinyllama/lora_no_es/20newsgroups/size=8/rs=3832
+ '[' '!' -f outputs/adaptation/tinyllama/lora_no_es/20newsgroups/size=8/rs=3832/test_logits.csv ']'
+ for size in ${dataset2samples[$dataset]}
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ total_train_samples=2560
+ output_dir=outputs/adaptation/tinyllama/lora_no_es/20newsgroups/size=128/rs=129
+ '[' '!' -f outputs/adaptation/tinyllama/lora_no_es/20newsgroups/size=128/rs=129/test_logits.csv ']'
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ total_train_samples=2560
+ output_dir=outputs/adaptation/tinyllama/lora_no_es/20newsgroups/size=128/rs=131
+ '[' '!' -f outputs/adaptation/tinyllama/lora_no_es/20newsgroups/size=128/rs=131/test_logits.csv ']'
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ total_train_samples=2560
+ output_dir=outputs/adaptation/tinyllama/lora_no_es/20newsgroups/size=128/rs=543
+ '[' '!' -f outputs/adaptation/tinyllama/lora_no_es/20newsgroups/size=128/rs=543/test_logits.csv ']'
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ total_train_samples=2560
+ output_dir=outputs/adaptation/tinyllama/lora_no_es/20newsgroups/size=128/rs=878
+ '[' '!' -f outputs/adaptation/tinyllama/lora_no_es/20newsgroups/size=128/rs=878/test_logits.csv ']'
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ total_train_samples=2560
+ output_dir=outputs/adaptation/tinyllama/lora_no_es/20newsgroups/size=128/rs=909
+ '[' '!' -f outputs/adaptation/tinyllama/lora_no_es/20newsgroups/size=128/rs=909/test_logits.csv ']'
+ for dataset in $DATASETS
+ for size in ${dataset2samples[$dataset]}
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ total_train_samples=77
+ output_dir=outputs/adaptation/tinyllama/lora_no_es/banking77/size=1/rs=322
+ '[' '!' -f outputs/adaptation/tinyllama/lora_no_es/banking77/size=1/rs=322/test_logits.csv ']'
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ total_train_samples=77
+ output_dir=outputs/adaptation/tinyllama/lora_no_es/banking77/size=1/rs=444
+ '[' '!' -f outputs/adaptation/tinyllama/lora_no_es/banking77/size=1/rs=444/test_logits.csv ']'
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ total_train_samples=77
+ output_dir=outputs/adaptation/tinyllama/lora_no_es/banking77/size=1/rs=848
+ '[' '!' -f outputs/adaptation/tinyllama/lora_no_es/banking77/size=1/rs=848/test_logits.csv ']'
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ total_train_samples=77
+ output_dir=outputs/adaptation/tinyllama/lora_no_es/banking77/size=1/rs=858
+ '[' '!' -f outputs/adaptation/tinyllama/lora_no_es/banking77/size=1/rs=858/test_logits.csv ']'
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ total_train_samples=77
+ output_dir=outputs/adaptation/tinyllama/lora_no_es/banking77/size=1/rs=868
+ '[' '!' -f outputs/adaptation/tinyllama/lora_no_es/banking77/size=1/rs=868/test_logits.csv ']'
+ for size in ${dataset2samples[$dataset]}
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ total_train_samples=308
+ output_dir=outputs/adaptation/tinyllama/lora_no_es/banking77/size=4/rs=295
+ '[' '!' -f outputs/adaptation/tinyllama/lora_no_es/banking77/size=4/rs=295/test_logits.csv ']'
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ total_train_samples=308
+ output_dir=outputs/adaptation/tinyllama/lora_no_es/banking77/size=4/rs=926
+ '[' '!' -f outputs/adaptation/tinyllama/lora_no_es/banking77/size=4/rs=926/test_logits.csv ']'
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ total_train_samples=308
+ output_dir=outputs/adaptation/tinyllama/lora_no_es/banking77/size=4/rs=962
+ '[' '!' -f outputs/adaptation/tinyllama/lora_no_es/banking77/size=4/rs=962/test_logits.csv ']'
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ total_train_samples=308
+ output_dir=outputs/adaptation/tinyllama/lora_no_es/banking77/size=4/rs=2951
+ '[' '!' -f outputs/adaptation/tinyllama/lora_no_es/banking77/size=4/rs=2951/test_logits.csv ']'
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ total_train_samples=308
+ output_dir=outputs/adaptation/tinyllama/lora_no_es/banking77/size=4/rs=9622
+ '[' '!' -f outputs/adaptation/tinyllama/lora_no_es/banking77/size=4/rs=9622/test_logits.csv ']'
+ for size in ${dataset2samples[$dataset]}
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ total_train_samples=4928
+ output_dir=outputs/adaptation/tinyllama/lora_no_es/banking77/size=64/rs=131
+ '[' '!' -f outputs/adaptation/tinyllama/lora_no_es/banking77/size=64/rs=131/test_logits.csv ']'
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ total_train_samples=4928
+ output_dir=outputs/adaptation/tinyllama/lora_no_es/banking77/size=64/rs=888
+ '[' '!' -f outputs/adaptation/tinyllama/lora_no_es/banking77/size=64/rs=888/test_logits.csv ']'
+ for random_state in ${dataset2seed[$dataset"_"$size]}
+ total_train_samples=4928
+ output_dir=outputs/adaptation/tinyllama/lora_no_es/banking77/size=64/rs=912
+ '[' '!' -f outputs/adaptation/tinyllama/lora_no_es/banking77/size=64/rs=912/test_logits.csv ']'
+ mkdir -p outputs/adaptation/tinyllama/lora_no_es/banking77/size=64/rs=912 outputs/adaptation/tinyllama/lora_no_es/banking77/size=64/rs=912/logs outputs/adaptation/tinyllama/lora_no_es/banking77/size=64/rs=912/checkpoints
+ python -m llmcal2.scripts.lora --data_dir outputs/prompts/generative/banking77 --total_train_samples 4928 --val_prop 0.0 --random_state 912 --checkpoint_dir /mnt/extra/lestienne/lit-checkpoints/TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T --batch_size 1 --accelerator gpu --strategy auto --devices 1 --num_nodes 1 --precision bf16-true --lora_r=8 --lora_alpha=16 --lora_dropout=0.05 --lora_query --lora_key --lora_value --lora_projection --lora_mlp --lora_head --optimizer=adamw --weight_decay=0.0 --max_steps=-1 --learning_rate=0.0001 --max_epochs=15 --accumulate_grad_batches=32 --output_dir outputs/adaptation/tinyllama/lora_no_es/banking77/size=64/rs=912 --log_dir outputs/adaptation/tinyllama/lora_no_es/banking77/size=64/rs=912/logs --output_checkpoint_dir outputs/adaptation/tinyllama/lora_no_es/banking77/size=64/rs=912/checkpoints
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:269: Experiment logs directory outputs/adaptation/tinyllama/lora_no_es/banking77/size=64/rs=912/logs exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name | Type           | Params | Mode 
------------------------------------------------
0 | gpt  | GenerativeLoRA | 1.1 B  | train
------------------------------------------------
6.6 M     Trainable params
1.1 B     Non-trainable params
1.1 B     Total params
4,426.514 Total estimated model params size (MB)
`Trainer.fit` stopped: `max_epochs=15` reached.
Traceback (most recent call last):
  File "/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/mnt/extra/lestienne/Documents/llmcal2/src/llmcal2/scripts/lora.py", line 258, in <module>
    Fire(main)
  File "/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/fire/core.py", line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File "/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/fire/core.py", line 691, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "/mnt/extra/lestienne/Documents/llmcal2/src/llmcal2/scripts/lora.py", line 215, in main
    checkpoint = lazy_load(output_dir / "best.ckpt")
  File "/mnt/extra/lestienne/anaconda3/envs/llmcal/lib/python3.10/site-packages/lightning/fabric/utilities/load.py", line 203, in _lazy_load
    raise FileNotFoundError(f"Path {str(filename)!r} does not exist or is not a file.")
FileNotFoundError: Path 'outputs/adaptation/tinyllama/lora_no_es/banking77/size=64/rs=912/best.ckpt' does not exist or is not a file.
